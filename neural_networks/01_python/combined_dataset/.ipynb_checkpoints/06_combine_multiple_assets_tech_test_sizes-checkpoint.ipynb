{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64e5407f-c4e7-47d4-9958-8edea3392661",
   "metadata": {},
   "source": [
    "## **This notebook aims to combine multiple assets databases**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e46c228-21f4-4e80-a4e8-b62981ab4799",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.metrics import accuracy_score\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf032534-2a6b-4f78-b6db-06262620ad55",
   "metadata": {},
   "outputs": [],
   "source": [
    "project_dir = \"/home/jupyter-tfg2425paula/prediction_project_v3\"\n",
    "os.chdir(project_dir)\n",
    "\n",
    "clean_data_dir = os.path.join(project_dir, \"00_data/clean\")\n",
    "horizontal_data_dir = os.path.join(project_dir, \"00_data/horizontal_structure\")\n",
    "results_dir = os.path.join(project_dir, \"02_results\")\n",
    "plots_dir = os.path.join(project_dir, \"03_plots\")\n",
    "pca_data_dir = os.path.join(project_dir, \"00_data/pca\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a34d7d2f-94f1-4a7e-a36a-65b386cd2711",
   "metadata": {},
   "source": [
    "### **GRU Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6be503cf-ab00-40f9-8287-6e871eb53021",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRU3DClassifier(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers, dropout):\n",
    "        super(GRU3DClassifier, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "\n",
    "        out, _ = self.gru(x, h0)\n",
    "        out = self.fc(out[:, -1, :]) \n",
    "        # return self.sigmoid(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e169a1d-ac5f-4796-8b5a-4447f1612a1c",
   "metadata": {},
   "source": [
    "### **LSTM Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c420d354-a19b-453e-9de3-78a238bf5920",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StockPriceLSTM(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers=1, dropout=0.0):\n",
    "        super(StockPriceLSTM, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers=num_layers, \n",
    "                            batch_first=True, dropout=dropout)\n",
    "    \n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)  # Get the batch size dynamically\n",
    "\n",
    "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_dim).to(x.device)  # (num_layers, batch_size, hidden_dim)\n",
    "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_dim).to(x.device)  # (num_layers, batch_size, hidden_dim)\n",
    "        \n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "\n",
    "        out = self.fc(out[:, -1, :]) \n",
    "        # out = self.sigmoid(out)\n",
    "        return out\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a8ade47-ae99-4802-a9be-59473a8a6598",
   "metadata": {},
   "source": [
    "### **Set folders**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fb30d8df-ff8b-48ef-a786-4385d72eed1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds = [0.3, 0.35, 0.4, 0.45, 0.5]\n",
    "thresholds = [0.5]\n",
    "learning_rates = [0.005, 0.008, 0.009, 0.01]\n",
    "learning_rates = [0.01]\n",
    "num_epochs_list = [100, 200]\n",
    "num_epochs_list = [100]\n",
    "batch_sizes = [16, 32]\n",
    "batch_sizes = [32]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7176635e-f447-4149-bab2-e997a4728295",
   "metadata": {},
   "source": [
    "#### **Model and Hyperparameters**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "273f20be-b345-4786-a715-dcf8fbcde47a",
   "metadata": {},
   "source": [
    "#### **Last data modifications**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e43ae2c5-e4c6-4be8-b82a-73c02d66705b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_remove_characters(df):\n",
    "\n",
    "    X = np.array([np.stack(row) for row in df.drop(columns=['Target']).values])\n",
    "    y = df['Target'].values\n",
    "\n",
    "    smote = SMOTE(random_state=42)\n",
    "    n_samples, timesteps, n_features = X.shape\n",
    "    X_flat = X.reshape((n_samples, timesteps * n_features))\n",
    "    X_flat = np.where(X_flat == 'ç', 0, X_flat)\n",
    "\n",
    "    X_resampled = X_flat.reshape((-1, timesteps, n_features))\n",
    "    \n",
    "    return X_resampled, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ddf708b-f9db-4ffc-88dd-06723ad78d3c",
   "metadata": {},
   "source": [
    "### **Evaluation function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bcba527d-6d55-4e90-9c55-fc058f338f90",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def evaluate_model_plot(\n",
    "    model, \n",
    "    X, \n",
    "    y, \n",
    "    criterion, \n",
    "    optimizer, \n",
    "    device, \n",
    "    train_size, \n",
    "    batch_size, \n",
    "    num_epochs, \n",
    "    lower_threshold,\n",
    "    plots_dir=None,\n",
    "    plot_filename=None\n",
    "):\n",
    "\n",
    "    # -------------------------------\n",
    "    # 0) Prepare Tensors & Splits\n",
    "    # -------------------------------\n",
    "    X = torch.tensor(X, dtype=torch.float32)\n",
    "    y = torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "    total_size = len(X)\n",
    "    # Determine actual train_size index\n",
    "    if train_size < 1.0:\n",
    "        lower_bound = int(train_size * total_size)\n",
    "    else:\n",
    "        lower_bound = train_size\n",
    "\n",
    "    # Training portion\n",
    "    X_train = X[:lower_bound].to(device)\n",
    "    y_train = y[:lower_bound].to(device)\n",
    "\n",
    "    train_dataset = TensorDataset(X_train, y_train)\n",
    "    trainloader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,   # Set True if you prefer shuffling\n",
    "        drop_last=False\n",
    "    )\n",
    "\n",
    "    if lower_bound < total_size:\n",
    "        X_val = X[lower_bound:].to(device)\n",
    "        y_val = y[lower_bound:].to(device)\n",
    "        val_dataset = TensorDataset(X_val, y_val)\n",
    "        valloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    else:\n",
    "        # If there's no leftover data for \"test\", handle gracefully\n",
    "        X_val = None\n",
    "        y_val = None\n",
    "        valloader = None\n",
    "\n",
    "    # ---------------------------------------------\n",
    "    # 1) SINGLE TRAINING PHASE + Track Loss Curves\n",
    "    # ---------------------------------------------\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "\n",
    "    epoch_train_losses = []\n",
    "    epoch_test_losses = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # TRAINING PASS\n",
    "        epoch_loss = 0.0\n",
    "        for X_batch, y_batch in trainloader:\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward\n",
    "            pred_y = model(X_batch)\n",
    "            loss = criterion(pred_y, y_batch)\n",
    "\n",
    "            # Backprop & update\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  # optional\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = epoch_loss / len(trainloader)\n",
    "        epoch_train_losses.append(avg_train_loss)\n",
    "\n",
    "        \n",
    "        # VALIDATION PASS (Optional but needed to get test_loss_curve)\n",
    "        if valloader is not None:\n",
    "            model.eval()\n",
    "            val_loss = 0.0\n",
    "            with torch.no_grad():\n",
    "                for Xb, yb in valloader:\n",
    "                    pred_yb = model(Xb)\n",
    "                    loss_b = criterion(pred_yb, yb)\n",
    "                    val_loss += loss_b.item()\n",
    "            avg_val_loss = val_loss / len(valloader)\n",
    "            epoch_test_losses.append(avg_val_loss)\n",
    "\n",
    "            model.train()  # Switch back to train mode\n",
    "\n",
    "        else:\n",
    "            # If no validation set, just store None or 0\n",
    "            epoch_test_losses.append(None)\n",
    "\n",
    "        # Print progress every 5 epochs or last epoch\n",
    "        if (epoch + 1) % 5 == 0 or (epoch == num_epochs - 1):\n",
    "            if epoch_test_losses[-1] is not None:\n",
    "                print(f\"[Epoch {epoch+1}/{num_epochs}] \",\n",
    "                      f\"Train Loss: {avg_train_loss:.4f}\")\n",
    "            else:\n",
    "                print(f\"[Epoch {epoch+1}/{num_epochs}] \"\n",
    "                      f\"Train Loss: {avg_train_loss:.4f}\")\n",
    "\n",
    "    # % decrease from first to last train loss\n",
    "    if len(epoch_train_losses) > 1:\n",
    "        loss_decrease_percentage = ((epoch_train_losses[-1] - epoch_train_losses[0])\n",
    "                                    / epoch_train_losses[0]) * 100\n",
    "    else:\n",
    "        loss_decrease_percentage = 0.0\n",
    "\n",
    "    final_train_loss = epoch_train_losses[-1]\n",
    "\n",
    "    # -------------------------------\n",
    "    # 2) ROLLING PREDICTIONS, NO UPDATE\n",
    "    # -------------------------------\n",
    "    model.eval()\n",
    "    rolling_predictions = []\n",
    "    rolling_targets = []\n",
    "\n",
    "    for i in range(lower_bound, total_size):\n",
    "        X_test = X[i:i+1].to(device)\n",
    "        y_test = y[i:i+1].to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            pred_y = model(X_test)\n",
    "            probabilities = torch.softmax(pred_y, dim=1).cpu().numpy()\n",
    "            prob_class_1 = probabilities[:, 1]  # shape: (1,)\n",
    "\n",
    "            # Threshold-based logic\n",
    "            pred_classes = np.zeros_like(prob_class_1)\n",
    "            # Predict -1 if prob < lower_threshold\n",
    "            pred_classes[prob_class_1 < lower_threshold] = -1\n",
    "            # Predict +1 if prob > (1 - lower_threshold)\n",
    "            pred_classes[prob_class_1 > (1 - lower_threshold)] = 1\n",
    "\n",
    "        rolling_predictions.append(pred_classes[0])\n",
    "        rolling_targets.append(y_test.item())\n",
    "\n",
    "    rolling_predictions = np.array(rolling_predictions)\n",
    "    rolling_targets = np.array(rolling_targets).astype(int)\n",
    "\n",
    "    # If original labels might be {0,1}, adapt as needed\n",
    "    rolling_targets[rolling_targets == 0] = -1\n",
    "\n",
    "    # Filter out zero predictions\n",
    "    nonzero_mask = (rolling_predictions != 0)\n",
    "    filtered_preds = rolling_predictions[nonzero_mask]\n",
    "    filtered_targets = rolling_targets[nonzero_mask]\n",
    "\n",
    "    if len(filtered_preds) == 0:\n",
    "        accuracy_nonzero = None\n",
    "        print(\"No nonzero predictions, cannot compute thresholded accuracy.\")\n",
    "    else:\n",
    "        accuracy_nonzero = accuracy_score(filtered_targets, filtered_preds)\n",
    "        print(f\"Accuracy on Nonzero Predictions: {accuracy_nonzero:.4f}\")\n",
    "\n",
    "    # -------------------------------------------------\n",
    "    # 3) PLOT (if plots_dir is set and there's test data)\n",
    "    # -------------------------------------------------\n",
    "    if plots_dir is not None:\n",
    "        os.makedirs(plots_dir, exist_ok=True)\n",
    "\n",
    "        # If user didn't provide a filename, create a default\n",
    "        if plot_filename is None:\n",
    "            plot_filename = \"train_test_loss_curve.png\"\n",
    "        plot_path = os.path.join(plots_dir, plot_filename)\n",
    "\n",
    "        # Plot the training and validation (test) loss curves\n",
    "        plt.figure(figsize=(8, 5))\n",
    "        plt.plot(epoch_train_losses, label=\"Train Loss\")\n",
    "        # Only plot test loss if it isn't None\n",
    "        if any(x is not None for x in epoch_test_losses):\n",
    "            plt.plot(epoch_test_losses, label=\"Test Loss\")\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.title(\"Train vs. Test Loss per Epoch\")\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "\n",
    "        # plt.savefig(plot_path)\n",
    "        plt.show()\n",
    "        print(f\"Loss curves saved to: {plot_path}\")\n",
    "\n",
    "    # ----------------\n",
    "    # 4) Return results\n",
    "    # ----------------\n",
    "    return {\n",
    "        \"rolling_predictions\": rolling_predictions,\n",
    "        \"rolling_targets\": rolling_targets,\n",
    "        \"filtered_predictions\": filtered_preds,\n",
    "        \"filtered_targets\": filtered_targets,\n",
    "        \"accuracy_nonzero\": accuracy_nonzero,\n",
    "        \"loss_decrease_percentage\": loss_decrease_percentage,\n",
    "        \"final_train_loss\": final_train_loss,\n",
    "        \"train_loss_curve\": epoch_train_losses,\n",
    "        \"test_loss_curve\": epoch_test_losses\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e5e03d-d5ce-406e-a4a2-e91cab13cd87",
   "metadata": {},
   "source": [
    "### **Execute evaluation funcion**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2397b7ed-82c7-4255-ab1f-848f0b531098",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_stocks_pkl_df(processing, security_type, period, window_size, project_dir, stocks):\n",
    "\n",
    "    X_list = []\n",
    "    y_list = []\n",
    "\n",
    "    for stock in stocks:\n",
    "        initial_data_dir = os.path.join(project_dir, f\"00_data/{processing}\") \n",
    "\n",
    "        # 1) Load original data (info only)\n",
    "        filename = f\"{security_type}/{stock}/{period}_data.csv\"\n",
    "        original_input_filepath = os.path.join(initial_data_dir, filename)\n",
    "        original_data = pd.read_csv(original_input_filepath)\n",
    "\n",
    "        print(f\"----- Appending stock: {stock}, period: {period}, data_type: {security_type} -----\")\n",
    "\n",
    "        # 2) Load the preprocessed data\n",
    "        pkl_filename = f\"{processing}/{security_type}/{stock}/{period}_{window_size}_data.pkl\"\n",
    "        input_filepath = os.path.join(horizontal_data_dir, pkl_filename)\n",
    "        input_df = pd.read_pickle(input_filepath)\n",
    "        \n",
    "        print(input_df.shape)\n",
    "\n",
    "        # 3) Reshape\n",
    "        X_resampled, y_resampled = reshape_remove_characters(input_df)\n",
    "\n",
    "        # Store in lists\n",
    "        X_list.append(X_resampled)\n",
    "        y_list.append(y_resampled)\n",
    "\n",
    "    # Concatenate all stocks along the sample dimension\n",
    "    X_combined = np.concatenate(X_list, axis=0)  # Shape: (total_samples, num_features, window_size)\n",
    "    y_combined = np.concatenate(y_list, axis=0)  # Shape: (total_samples,)\n",
    "\n",
    "    # Print shapes to verify\n",
    "    print(\"Final X shape:\", X_combined.shape)\n",
    "    print(\"Final y shape:\", y_combined.shape)\n",
    "    \n",
    "    return X_combined, y_combined\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf981ac3-95b4-41e6-b2f2-c44849b4280e",
   "metadata": {},
   "source": [
    "## **Apply concatenation to specified datasets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4e9684f3-3f8d-480e-9630-246d35135098",
   "metadata": {},
   "outputs": [],
   "source": [
    "processing = \"clean\"\n",
    "security_type = \"technical\"\n",
    "period = \"10y\"\n",
    "stocks = ['AAPL', 'MSFT', 'AMZN', 'NVDA', 'SPX']\n",
    "# window_size = 50\n",
    "\n",
    "# X_combined, y_combined = combine_stocks_pkl_df(processing, security_type, period, window_size, project_dir, stocks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb390b9-4ec7-4591-a161-727b96bd36b9",
   "metadata": {},
   "source": [
    "## **Run LSTM and GRU Models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "47587092-2c27-4ea0-b488-436522249985",
   "metadata": {},
   "outputs": [],
   "source": [
    "possible_train_size = 95\n",
    "\n",
    "num_epochs = 100\n",
    "learning_rate = 0.01\n",
    "prediction_threshold = 0.5\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "batch_size = 64\n",
    "hidden_size = 64  \n",
    "output_size = 2  \n",
    "num_layers = 2\n",
    "dropout = 0.2\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9a18c2d3-6e23-4c1e-a3b0-3cc638c384ae",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Appending stock: AAPL, period: 10y, data_type: technical -----\n",
      "(2513, 93)\n",
      "----- Appending stock: MSFT, period: 10y, data_type: technical -----\n",
      "(2513, 93)\n",
      "----- Appending stock: AMZN, period: 10y, data_type: technical -----\n",
      "(2513, 93)\n",
      "----- Appending stock: NVDA, period: 10y, data_type: technical -----\n",
      "(2513, 93)\n",
      "----- Appending stock: SPX, period: 10y, data_type: technical -----\n",
      "(2075, 93)\n",
      "Final X shape: (12127, 92, 5)\n",
      "Final y shape: (12127,)\n",
      "Training SPX | LR: 0.01 | Epochs: 100 | Batch: 64 | Security type: technical\n",
      "[Epoch 5/100]  Train Loss: 0.6843\n",
      "[Epoch 10/100]  Train Loss: 0.6857\n",
      "[Epoch 15/100]  Train Loss: 0.6890\n",
      "[Epoch 20/100]  Train Loss: 0.6882\n",
      "[Epoch 25/100]  Train Loss: 0.6859\n",
      "[Epoch 30/100]  Train Loss: 0.6864\n",
      "[Epoch 35/100]  Train Loss: 0.6860\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [17], line 30\u001b[0m\n\u001b[1;32m     25\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mlearning_rate)\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstocks[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | LR: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlearning_rate\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | Epochs: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     27\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m| Batch: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbatch_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | Security type: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msecurity_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 30\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_model_plot\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_combined\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_combined\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlower_threshold\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[43m    \u001b[49m\u001b[43mplots_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mplots_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[43m    \u001b[49m\u001b[43mplot_filename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[1;32m     43\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;66;03m# Store results in a list\u001b[39;00m\n\u001b[1;32m     46\u001b[0m results_list\u001b[38;5;241m.\u001b[39mappend({\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msecurity_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: security_type,\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwindow_size\u001b[39m\u001b[38;5;124m\"\u001b[39m: window_size,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfinal_train_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m: result[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfinal_train_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     54\u001b[0m })\n",
      "Cell \u001b[0;32mIn [7], line 79\u001b[0m, in \u001b[0;36mevaluate_model_plot\u001b[0;34m(model, X, y, criterion, optimizer, device, train_size, batch_size, num_epochs, lower_threshold, plots_dir, plot_filename)\u001b[0m\n\u001b[1;32m     76\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(pred_y, y_batch)\n\u001b[1;32m     78\u001b[0m \u001b[38;5;66;03m# Backprop & update\u001b[39;00m\n\u001b[0;32m---> 79\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     80\u001b[0m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(model\u001b[38;5;241m.\u001b[39mparameters(), max_norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m)  \u001b[38;5;66;03m# optional\u001b[39;00m\n\u001b[1;32m     81\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    580\u001b[0m     )\n\u001b[0;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/autograd/graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model_types = [\"lstm\", \"gru\"]   \n",
    "model_types = [\"gru\"] \n",
    "sample_size = 5\n",
    "window_sizes = [5, 10, 50, 100]\n",
    "window_sizes = [5]\n",
    "possible_train_sizes = [80, 90, 95]\n",
    "\n",
    "results_list = []\n",
    "\n",
    "for possible_train_size in possible_train_sizes:\n",
    "    X_combined, y_combined = combine_stocks_pkl_df(processing, security_type, period, window_size, project_dir, stocks)\n",
    "    input_size = X_combined.shape[2]\n",
    "    train_size = int(X_combined.shape[0] * possible_train_size / 100)\n",
    "    # train_size = X_combined.shape[0]-30\n",
    "    test_size = X_combined.shape[0] - train_size\n",
    "\n",
    "    for model_type in model_types:\n",
    "        for i in range(sample_size):\n",
    "            if model_type == \"gru\":\n",
    "                model = GRU3DClassifier(input_size, hidden_size, output_size, num_layers, dropout)\n",
    "            elif model_type == \"lstm\":\n",
    "                model = StockPriceLSTM(input_size, hidden_size, output_size)\n",
    "\n",
    "            model = model.to(device)\n",
    "\n",
    "            optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "            print(f\"Training {stocks[-1]} | LR: {learning_rate} | Epochs: {num_epochs} \"\n",
    "                  f\"| Batch: {batch_size} | Security type: {security_type}\")\n",
    "\n",
    "\n",
    "            result = evaluate_model_plot(\n",
    "                model, \n",
    "                X_combined, \n",
    "                y_combined, \n",
    "                criterion, \n",
    "                optimizer, \n",
    "                device, \n",
    "                train_size, \n",
    "                batch_size, \n",
    "                num_epochs, \n",
    "                lower_threshold = 0.5,\n",
    "                plots_dir=plots_dir,\n",
    "                plot_filename=None\n",
    "            )\n",
    "\n",
    "            # Store results in a list\n",
    "            results_list.append({\n",
    "                \"security_type\": security_type,\n",
    "                \"window_size\": window_size,\n",
    "                \"model_type\": model_type,\n",
    "                \"iteration\": i + 1,\n",
    "                \"possible_train_size\": possible_train_size,\n",
    "                \"accuracy_nonzero\": result[\"accuracy_nonzero\"],\n",
    "                \"loss_decrease_percentage\": result[\"loss_decrease_percentage\"],\n",
    "                \"final_train_loss\": result[\"final_train_loss\"]\n",
    "            })\n",
    "\n",
    "results_df = pd.DataFrame(results_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6684d41d-3665-41e0-b20c-15716fb40ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_folder = os.path.join(results_dir, 'combined_vs_individual')\n",
    "\n",
    "results_df.to_csv(os.path.join(output_folder, 'combined_technical.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b27199d-23d3-4384-8a00-68ac9d0a814c",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8068b712-a936-48e0-a23a-d0693afd67d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Appending stock: AAPL, period: 10y, data_type: technical -----\n",
      "(2513, 93)\n",
      "----- Appending stock: MSFT, period: 10y, data_type: technical -----\n",
      "(2513, 93)\n",
      "----- Appending stock: AMZN, period: 10y, data_type: technical -----\n",
      "(2513, 93)\n",
      "----- Appending stock: NVDA, period: 10y, data_type: technical -----\n",
      "(2513, 93)\n",
      "----- Appending stock: SPX, period: 10y, data_type: technical -----\n",
      "(2075, 93)\n",
      "Final X shape: (12127, 92, 5)\n",
      "Final y shape: (12127,)\n"
     ]
    }
   ],
   "source": [
    "X_combined, y_combined = combine_stocks_pkl_df(processing, security_type, period, window_size, project_dir, stocks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c3e5cd-f904-4d94-aaab-3f242b02af00",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
