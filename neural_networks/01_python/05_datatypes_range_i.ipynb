{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "940889cf-06e0-4e8c-8efa-1eeacc97ca57",
   "metadata": {},
   "source": [
    "## **This notebook aims to compare some models and store them in a reasonable fashion**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2e46c228-21f4-4e80-a4e8-b62981ab4799",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.metrics import accuracy_score\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a11a538-40d5-49c3-92e9-e26fdcefe083",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"  # Use GPU 0 in first notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7b259658-5fed-4231-9f06-f9db567e1d0f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "project_dir = \"/home/jupyter-tfg2425paula/prediction_project_v3\"\n",
    "os.chdir(project_dir)\n",
    "\n",
    "clean_data_dir = os.path.join(project_dir, \"00_data/clean\")\n",
    "horizontal_data_dir = os.path.join(project_dir, \"00_data/horizontal_structure\")\n",
    "results_dir = os.path.join(project_dir, \"02_results\")\n",
    "plots_dir = os.path.join(project_dir, \"03_plots\")\n",
    "pca_data_dir = os.path.join(project_dir, \"00_data/pca\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7319029-ba4c-4c90-91b9-a0a02b5be9af",
   "metadata": {},
   "source": [
    "### **GRU Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a7a1485d-99bb-4358-a216-1c1c75e50ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRU3DClassifier(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers, dropout):\n",
    "        super(GRU3DClassifier, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "\n",
    "        out, _ = self.gru(x, h0)\n",
    "        out = self.fc(out[:, -1, :]) \n",
    "        # return self.sigmoid(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3061a6b4-dad3-4129-a2ed-0098e04440e4",
   "metadata": {},
   "source": [
    "### **LSTM Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "95ffa6e2-2c35-48de-8c95-b7ce86fe9dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StockPriceLSTM(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers=1, dropout=0.0):\n",
    "        super(StockPriceLSTM, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers=num_layers, \n",
    "                            batch_first=True, dropout=dropout)\n",
    "    \n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)  # Get the batch size dynamically\n",
    "\n",
    "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_dim).to(x.device)  # (num_layers, batch_size, hidden_dim)\n",
    "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_dim).to(x.device)  # (num_layers, batch_size, hidden_dim)\n",
    "        \n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "\n",
    "        out = self.fc(out[:, -1, :]) \n",
    "        # out = self.sigmoid(out)\n",
    "        return out\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df6d4b78-1492-45ea-a9af-854d8d69b8de",
   "metadata": {},
   "source": [
    "### **Set folders**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e67b679f-204f-4e15-89fe-09879ed7246c",
   "metadata": {},
   "source": [
    "Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2b80ed82-98cd-453d-94e9-b7f42c61040e",
   "metadata": {},
   "outputs": [],
   "source": [
    "processing_types = [\"clean\", \"pca\"]\n",
    "processing_types= [\"clean\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da3781cf-0f0b-4858-bff8-09f839e2da04",
   "metadata": {},
   "source": [
    "Folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a2958856-f0de-4f93-85b5-9fdd169a2a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stocks = ['AAPL', 'MSFT', 'AMZN', 'NVDA', 'SPX']\n",
    "stocks = ['AAPL']\n",
    "# types_securities = [\"single_name\", \"options\", \"technical\"]\n",
    "types_securities = [\"options\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2e929df-0399-4e41-9b2b-1863f8c215b5",
   "metadata": {},
   "source": [
    "Different files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9b853df6-38b4-4bd7-85e5-6a232f17905e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# years = [\"15y\", \"10y\", \"5y\", \"2y\"]\n",
    "years = [\"10y\"]\n",
    "# window_sizes = [5, 10, 50, 100]\n",
    "window_sizes = [5]\n",
    "# train_sizes = [80, 90, 95]\n",
    "train_sizes = [95]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f9fb2cc-d8de-4e4f-be9d-f8cb7aac8e16",
   "metadata": {},
   "source": [
    "Same file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "145e5ba2-35c7-4a71-b100-624ca6ed1ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds = [0.3, 0.35, 0.4, 0.45, 0.5]\n",
    "thresholds = [0.5]\n",
    "learning_rates = [0.005, 0.008, 0.009, 0.01]\n",
    "learning_rates = [0.01]\n",
    "num_epochs_list = [100, 200]\n",
    "num_epochs_list = [100]\n",
    "batch_sizes = [16, 32]\n",
    "batch_sizes = [16]\n",
    "prediction_thresholds = [0.35, 0.4, 0.45, 0.5]\n",
    "prediction_thresholds = [0.5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbbee35f-0147-48b0-93ef-d9687984bbd0",
   "metadata": {},
   "source": [
    "#### **Model and Hyperparameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "71861bc1-7067-47f8-b3dd-a5cef2bc0f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "hidden_size = 64  \n",
    "output_size = 2  \n",
    "num_layers = 2\n",
    "dropout = 0.2\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "60900dea-a72e-4b53-979b-aded1e725489",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_types = [\"lstm\", \"gru\"]   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d873f70-b583-4e84-9289-bc303ef0446f",
   "metadata": {},
   "source": [
    "#### **Last data modifications**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "85729e3c-f060-4dc2-a4fc-a80573fefd73",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def reshape_remove_characters(df):\n",
    "\n",
    "    X = np.array([np.stack(row) for row in df.drop(columns=['Target']).values])\n",
    "    y = df['Target'].values\n",
    "\n",
    "    smote = SMOTE(random_state=42)\n",
    "    n_samples, timesteps, n_features = X.shape\n",
    "    X_flat = X.reshape((n_samples, timesteps * n_features))\n",
    "    X_flat = np.where(X_flat == 'ç', 0, X_flat)\n",
    "\n",
    "    X_resampled = X_flat.reshape((-1, timesteps, n_features))\n",
    "    \n",
    "    return X_resampled, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88656719-d0c4-4bf2-aef1-31f00b167d2a",
   "metadata": {},
   "source": [
    "### **Evaluation function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "74c49327-1c0d-4706-9943-54b795cb0687",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evaluate_rolling_unchanged_model_threshold(\n",
    "    model, \n",
    "    X, \n",
    "    y, \n",
    "    criterion, \n",
    "    optimizer, \n",
    "    device, \n",
    "    train_size, \n",
    "    batch_size, \n",
    "    num_epochs, \n",
    "    lower_threshold\n",
    "):\n",
    "    \"\"\"\n",
    "    Evaluate a PyTorch model using a rolling prediction approach for time series,\n",
    "    training the model only once on the initial training set. For each time step\n",
    "    after train_size, the model makes a prediction without further parameter updates.\n",
    "    Only predicts +1 or -1 if the probability of class 1 is above/below given thresholds;\n",
    "    otherwise, predicts 0. Accuracy is computed only on nonzero predictions.\n",
    "\n",
    "    Args:\n",
    "        model:          PyTorch model to evaluate.\n",
    "        X:              Feature data (numpy array).\n",
    "        y:              Target data (numpy array).\n",
    "        criterion:      Loss function (e.g., CrossEntropyLoss).\n",
    "        optimizer:      Optimizer (e.g., Adam).\n",
    "        device:         Device for computation (CPU or GPU).\n",
    "        train_size:     Initial size of the training data (int or float).\n",
    "                        If < 1, treated as fraction of total length.\n",
    "        batch_size:     Batch size for training.\n",
    "        num_epochs:     Number of epochs for initial training only.\n",
    "        lower_threshold: Probability threshold below which model predicts -1.\n",
    "        upper_threshold: Probability threshold above which model predicts +1.\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary with the following keys:\n",
    "            - \"rolling_predictions\": All predictions (-1, 0, +1) across the test period.\n",
    "            - \"rolling_targets\": Corresponding true targets in [-1, +1].\n",
    "            - \"filtered_predictions\": Nonzero predictions only.\n",
    "            - \"filtered_targets\": Targets corresponding to nonzero predictions.\n",
    "            - \"accuracy_nonzero\": Accuracy computed only on nonzero predictions.\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert X, y to tensors\n",
    "    X = torch.tensor(X, dtype=torch.float32)\n",
    "    y = torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "    # Determine initial training set size\n",
    "    if train_size < 1.0:\n",
    "        lower_bound = int(train_size * len(X))\n",
    "    else:\n",
    "        lower_bound = train_size\n",
    "\n",
    "    # -------------------------\n",
    "    # 1) SINGLE TRAINING PHASE\n",
    "    # -------------------------\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "    \n",
    "    X_train = X[:lower_bound].to(device)\n",
    "    y_train = y[:lower_bound].to(device)\n",
    "\n",
    "    train_dataset = TensorDataset(X_train, y_train)\n",
    "    trainloader = DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size=batch_size, \n",
    "        shuffle=False,         # Keep False if order matters; True for better generalization\n",
    "        # num_workers=4,         # Adjust based on your CPU cores\n",
    "        # pin_memory=True,       # Speeds up transfer if using GPUs\n",
    "        drop_last=False        # Ensure the last batch is included\n",
    "    )\n",
    "\n",
    "    epoch_train_losses = []\n",
    "    for epoch in range(num_epochs):\n",
    "        # torch.cuda.empty_cache()\n",
    "        epoch_loss = 0.0\n",
    "        for X_batch, y_batch in trainloader:\n",
    "            X_batch = X_batch.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            pred_y = model(X_batch)   # [batch_size, num_classes]\n",
    "            loss = criterion(pred_y, y_batch)\n",
    "            loss.backward()\n",
    "\n",
    "            # Gradient clipping (optional)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "               \n",
    "        if (epoch + 1) % 5 == 0 or epoch == num_epochs - 1:\n",
    "            print(f\"[Train] Epoch {epoch+1}/{num_epochs}, Loss={epoch_loss/len(trainloader):.4f}\")\n",
    "\n",
    "        epoch_train_losses.append(epoch_loss/len(trainloader))\n",
    "        \n",
    "    loss_decrease_percentage = ((epoch_train_losses[-1] - epoch_train_losses[0]) / epoch_train_losses[0]) * 100\n",
    "    # ---------------------------------\n",
    "    # 2) ROLLING PREDICTIONS, NO UPDATE\n",
    "    # ---------------------------------\n",
    "    model.eval()\n",
    "\n",
    "    rolling_predictions = []\n",
    "    rolling_targets     = []\n",
    "\n",
    "    for i in range(lower_bound, len(X)):\n",
    "        # Single-step \"test\" sample\n",
    "        X_test = X[i:i+1].to(device)  # shape: (1, num_features)\n",
    "        y_test = y[i:i+1].to(device)  # shape: (1, )\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # Forward pass\n",
    "            pred_y = model(X_test)  # [1, num_classes]\n",
    "            probabilities = torch.softmax(pred_y, dim=1).cpu().numpy()  # shape: (1, 2)\n",
    "            prob_class_1  = probabilities[:, 1]  # shape: (1,)\n",
    "\n",
    "            # Threshold-based logic\n",
    "            # Initialize all predictions to 0\n",
    "            pred_classes = np.zeros_like(prob_class_1)\n",
    "            # Predict -1 if prob < lower_threshold\n",
    "            pred_classes[prob_class_1 < lower_threshold] = -1\n",
    "            # Predict +1 if prob > upper_threshold\n",
    "            pred_classes[prob_class_1 > 1-lower_threshold] = 1\n",
    "\n",
    "        rolling_predictions.append(pred_classes[0])  # scalar\n",
    "        rolling_targets.append(y_test.item())\n",
    "\n",
    "    rolling_predictions = np.array(rolling_predictions)\n",
    "    rolling_targets = np.array(rolling_targets).astype(int)\n",
    "\n",
    "    # Convert any 0-labeled targets to -1 if your original data is in [-1, +1]\n",
    "    # (Sometimes y might be {0,1} or {-1, +1}; adapt as needed.)\n",
    "    rolling_targets[rolling_targets == 0] = -1\n",
    "\n",
    "    # Filter out zero predictions\n",
    "    nonzero_mask = rolling_predictions != 0\n",
    "    filtered_preds = rolling_predictions[nonzero_mask]\n",
    "    filtered_targets = rolling_targets[nonzero_mask]\n",
    "\n",
    "    if len(filtered_preds) == 0:\n",
    "        accuracy_nonzero = None\n",
    "        print(\"No nonzero predictions, cannot compute thresholded accuracy.\")\n",
    "    else:\n",
    "        accuracy_nonzero = accuracy_score(filtered_targets, filtered_preds)\n",
    "        print(f\"Accuracy on Nonzero Predictions: {accuracy_nonzero:.4f}\")\n",
    "\n",
    "    return {\n",
    "        \"rolling_predictions\": rolling_predictions,\n",
    "        \"rolling_targets\": rolling_targets,\n",
    "        \"filtered_predictions\": filtered_preds,\n",
    "        \"filtered_targets\": filtered_targets,\n",
    "        \"accuracy_nonzero\": accuracy_nonzero,\n",
    "        \"loss_decrease_percentage\": loss_decrease_percentage,\n",
    "        \"final_train_loss\": epoch_train_losses[-1] \n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b794e699-83fb-4ca2-8c93-318858018a2e",
   "metadata": {},
   "source": [
    "### **3nd Type of comparison:**\n",
    "\n",
    "Learning rates, for AAPL 10y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "935b7698-c75e-496b-8850-8053044ce7f9",
   "metadata": {},
   "source": [
    "### **Plot curves**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8953eb19-f0c8-4c66-bdfa-9bd8a8a92b1a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def evaluate_rolling_unchanged_model_threshold(\n",
    "    model, \n",
    "    X, \n",
    "    y, \n",
    "    criterion, \n",
    "    optimizer, \n",
    "    device, \n",
    "    train_size, \n",
    "    batch_size, \n",
    "    num_epochs, \n",
    "    lower_threshold,\n",
    "    plots_dir=None,\n",
    "    plot_filename=None\n",
    "):\n",
    "\n",
    "    # -------------------------------\n",
    "    # 0) Prepare Tensors & Splits\n",
    "    # -------------------------------\n",
    "    X = torch.tensor(X, dtype=torch.float32)\n",
    "    y = torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "    total_size = len(X)\n",
    "    # Determine actual train_size index\n",
    "    if train_size < 1.0:\n",
    "        lower_bound = int(train_size * total_size)\n",
    "    else:\n",
    "        lower_bound = train_size\n",
    "\n",
    "    # Training portion\n",
    "    X_train = X[:lower_bound].to(device)\n",
    "    y_train = y[:lower_bound].to(device)\n",
    "\n",
    "    train_dataset = TensorDataset(X_train, y_train)\n",
    "    trainloader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,   # Set True if you prefer shuffling\n",
    "        drop_last=False\n",
    "    )\n",
    "\n",
    "    if lower_bound < total_size:\n",
    "        X_val = X[lower_bound:].to(device)\n",
    "        y_val = y[lower_bound:].to(device)\n",
    "        val_dataset = TensorDataset(X_val, y_val)\n",
    "        valloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    else:\n",
    "        # If there's no leftover data for \"test\", handle gracefully\n",
    "        X_val = None\n",
    "        y_val = None\n",
    "        valloader = None\n",
    "\n",
    "    # ---------------------------------------------\n",
    "    # 1) SINGLE TRAINING PHASE + Track Loss Curves\n",
    "    # ---------------------------------------------\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "\n",
    "    epoch_train_losses = []\n",
    "    epoch_test_losses = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # TRAINING PASS\n",
    "        epoch_loss = 0.0\n",
    "        for X_batch, y_batch in trainloader:\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward\n",
    "            pred_y = model(X_batch)\n",
    "            loss = criterion(pred_y, y_batch)\n",
    "\n",
    "            # Backprop & update\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  # optional\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = epoch_loss / len(trainloader)\n",
    "        epoch_train_losses.append(avg_train_loss)\n",
    "\n",
    "        # VALIDATION PASS (Optional but needed to get test_loss_curve)\n",
    "        if valloader is not None:\n",
    "            model.eval()\n",
    "            val_loss = 0.0\n",
    "            with torch.no_grad():\n",
    "                for Xb, yb in valloader:\n",
    "                    pred_yb = model(Xb)\n",
    "                    loss_b = criterion(pred_yb, yb)\n",
    "                    val_loss += loss_b.item()\n",
    "            avg_val_loss = val_loss / len(valloader)\n",
    "            epoch_test_losses.append(avg_val_loss)\n",
    "\n",
    "            model.train()  # Switch back to train mode\n",
    "\n",
    "        else:\n",
    "            # If no validation set, just store None or 0\n",
    "            epoch_test_losses.append(None)\n",
    "\n",
    "        # Print progress every 5 epochs or last epoch\n",
    "        if (epoch + 1) % 5 == 0 or (epoch == num_epochs - 1):\n",
    "            if epoch_test_losses[-1] is not None:\n",
    "                print(f\"[Epoch {epoch+1}/{num_epochs}] \"\n",
    "                      f\"Train Loss: {avg_train_loss:.4f}, \"\n",
    "                      f\"Val Loss: {avg_val_loss:.4f}\")\n",
    "            else:\n",
    "                print(f\"[Epoch {epoch+1}/{num_epochs}] \"\n",
    "                      f\"Train Loss: {avg_train_loss:.4f}\")\n",
    "\n",
    "    # % decrease from first to last train loss\n",
    "    if len(epoch_train_losses) > 1:\n",
    "        loss_decrease_percentage = ((epoch_train_losses[-1] - epoch_train_losses[0])\n",
    "                                    / epoch_train_losses[0]) * 100\n",
    "    else:\n",
    "        loss_decrease_percentage = 0.0\n",
    "\n",
    "    final_train_loss = epoch_train_losses[-1]\n",
    "\n",
    "    # -------------------------------\n",
    "    # 2) ROLLING PREDICTIONS, NO UPDATE\n",
    "    # -------------------------------\n",
    "    model.eval()\n",
    "    rolling_predictions = []\n",
    "    rolling_targets = []\n",
    "\n",
    "    for i in range(lower_bound, total_size):\n",
    "        X_test = X[i:i+1].to(device)\n",
    "        y_test = y[i:i+1].to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            pred_y = model(X_test)\n",
    "            probabilities = torch.softmax(pred_y, dim=1).cpu().numpy()\n",
    "            prob_class_1 = probabilities[:, 1]  # shape: (1,)\n",
    "\n",
    "            # Threshold-based logic\n",
    "            pred_classes = np.zeros_like(prob_class_1)\n",
    "            # Predict -1 if prob < lower_threshold\n",
    "            pred_classes[prob_class_1 < lower_threshold] = -1\n",
    "            # Predict +1 if prob > (1 - lower_threshold)\n",
    "            pred_classes[prob_class_1 > (1 - lower_threshold)] = 1\n",
    "\n",
    "        rolling_predictions.append(pred_classes[0])\n",
    "        rolling_targets.append(y_test.item())\n",
    "\n",
    "    rolling_predictions = np.array(rolling_predictions)\n",
    "    rolling_targets = np.array(rolling_targets).astype(int)\n",
    "\n",
    "    # If original labels might be {0,1}, adapt as needed\n",
    "    rolling_targets[rolling_targets == 0] = -1\n",
    "\n",
    "    # Filter out zero predictions\n",
    "    nonzero_mask = (rolling_predictions != 0)\n",
    "    filtered_preds = rolling_predictions[nonzero_mask]\n",
    "    filtered_targets = rolling_targets[nonzero_mask]\n",
    "\n",
    "    if len(filtered_preds) == 0:\n",
    "        accuracy_nonzero = None\n",
    "        print(\"No nonzero predictions, cannot compute thresholded accuracy.\")\n",
    "    else:\n",
    "        accuracy_nonzero = accuracy_score(filtered_targets, filtered_preds)\n",
    "        print(f\"Accuracy on Nonzero Predictions: {accuracy_nonzero:.4f}\")\n",
    "\n",
    "    # -------------------------------------------------\n",
    "    # 3) PLOT (if plots_dir is set and there's test data)\n",
    "    # -------------------------------------------------\n",
    "    if plots_dir is not None:\n",
    "        os.makedirs(plots_dir, exist_ok=True)\n",
    "\n",
    "        # If user didn't provide a filename, create a default\n",
    "        if plot_filename is None:\n",
    "            plot_filename = \"train_test_loss_curve.png\"\n",
    "        plot_path = os.path.join(plots_dir, plot_filename)\n",
    "\n",
    "        # Plot the training and validation (test) loss curves\n",
    "        plt.figure(figsize=(8, 5))\n",
    "        plt.plot(epoch_train_losses, label=\"Train Loss\")\n",
    "        # Only plot test loss if it isn't None\n",
    "        if any(x is not None for x in epoch_test_losses):\n",
    "            plt.plot(epoch_test_losses, label=\"Test Loss\")\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.title(\"Train vs. Test Loss per Epoch\")\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "\n",
    "        plt.savefig(plot_path)\n",
    "        plt.close()\n",
    "        print(f\"Loss curves saved to: {plot_path}\")\n",
    "\n",
    "    # ----------------\n",
    "    # 4) Return results\n",
    "    # ----------------\n",
    "    return {\n",
    "        \"rolling_predictions\": rolling_predictions,\n",
    "        \"rolling_targets\": rolling_targets,\n",
    "        \"filtered_predictions\": filtered_preds,\n",
    "        \"filtered_targets\": filtered_targets,\n",
    "        \"accuracy_nonzero\": accuracy_nonzero,\n",
    "        \"loss_decrease_percentage\": loss_decrease_percentage,\n",
    "        \"final_train_loss\": final_train_loss,\n",
    "        \"train_loss_curve\": epoch_train_losses,\n",
    "        \"test_loss_curve\": epoch_test_losses\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "152fb8d5-715f-4409-8b9f-8b01040b777b",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----- LEARNING_RATE: 0.009883203468677031, SECURITY_TYPE: single_name, MODEL_TYPE: gru -----\n",
      "0\n",
      "Training AAPL | LR: 0.009883203468677031 | Epochs: 80 | Batch: 32 | Security type: single_name\n",
      "[Epoch 5/80] Train Loss: 0.6937, Val Loss: 0.6931\n",
      "[Epoch 10/80] Train Loss: 0.6937, Val Loss: 0.6927\n",
      "[Epoch 15/80] Train Loss: 0.6934, Val Loss: 0.6921\n",
      "[Epoch 20/80] Train Loss: 0.6928, Val Loss: 0.6912\n",
      "[Epoch 25/80] Train Loss: 0.6916, Val Loss: 0.6873\n",
      "[Epoch 30/80] Train Loss: 0.6910, Val Loss: 0.6877\n",
      "[Epoch 35/80] Train Loss: 0.6913, Val Loss: 0.6889\n",
      "[Epoch 40/80] Train Loss: 0.6898, Val Loss: 0.6874\n",
      "[Epoch 45/80] Train Loss: 0.6882, Val Loss: 0.6874\n",
      "[Epoch 50/80] Train Loss: 0.6892, Val Loss: 0.6883\n",
      "[Epoch 55/80] Train Loss: 0.6884, Val Loss: 0.6899\n",
      "[Epoch 60/80] Train Loss: 0.6878, Val Loss: 0.6840\n",
      "[Epoch 65/80] Train Loss: 0.6843, Val Loss: 0.6827\n",
      "[Epoch 70/80] Train Loss: 0.6842, Val Loss: 0.6835\n",
      "[Epoch 75/80] Train Loss: 0.6835, Val Loss: 0.6789\n",
      "[Epoch 80/80] Train Loss: 0.6807, Val Loss: 0.6825\n",
      "Accuracy on Nonzero Predictions: 0.5573\n",
      "1\n",
      "Training AAPL | LR: 0.009883203468677031 | Epochs: 80 | Batch: 32 | Security type: single_name\n",
      "[Epoch 5/80] Train Loss: 0.6939, Val Loss: 0.6927\n",
      "[Epoch 10/80] Train Loss: 0.6936, Val Loss: 0.6924\n",
      "[Epoch 15/80] Train Loss: 0.6936, Val Loss: 0.6921\n",
      "[Epoch 20/80] Train Loss: 0.6936, Val Loss: 0.6923\n",
      "[Epoch 25/80] Train Loss: 0.6938, Val Loss: 0.6928\n",
      "[Epoch 30/80] Train Loss: 0.6935, Val Loss: 0.6918\n",
      "[Epoch 35/80] Train Loss: 0.6935, Val Loss: 0.6916\n",
      "[Epoch 40/80] Train Loss: 0.6926, Val Loss: 0.6869\n",
      "[Epoch 45/80] Train Loss: 0.6918, Val Loss: 0.6883\n",
      "[Epoch 50/80] Train Loss: 0.6920, Val Loss: 0.6880\n",
      "[Epoch 55/80] Train Loss: 0.6905, Val Loss: 0.6861\n",
      "[Epoch 60/80] Train Loss: 0.6891, Val Loss: 0.6844\n",
      "[Epoch 65/80] Train Loss: 0.6877, Val Loss: 0.6791\n",
      "[Epoch 70/80] Train Loss: 0.6869, Val Loss: 0.6830\n",
      "[Epoch 75/80] Train Loss: 0.6848, Val Loss: 0.6819\n",
      "[Epoch 80/80] Train Loss: 0.6843, Val Loss: 0.6803\n",
      "Accuracy on Nonzero Predictions: 0.5878\n",
      "2\n",
      "Training AAPL | LR: 0.009883203468677031 | Epochs: 80 | Batch: 32 | Security type: single_name\n",
      "[Epoch 5/80] Train Loss: 0.6938, Val Loss: 0.6928\n",
      "[Epoch 10/80] Train Loss: 0.6936, Val Loss: 0.6925\n",
      "[Epoch 15/80] Train Loss: 0.6936, Val Loss: 0.6918\n",
      "[Epoch 20/80] Train Loss: 0.6931, Val Loss: 0.6927\n",
      "[Epoch 25/80] Train Loss: 0.6923, Val Loss: 0.6900\n",
      "[Epoch 30/80] Train Loss: 0.6919, Val Loss: 0.6846\n",
      "[Epoch 35/80] Train Loss: 0.6906, Val Loss: 0.6891\n",
      "[Epoch 40/80] Train Loss: 0.6893, Val Loss: 0.6878\n",
      "[Epoch 45/80] Train Loss: 0.6895, Val Loss: 0.6896\n",
      "[Epoch 50/80] Train Loss: 0.6868, Val Loss: 0.6899\n",
      "[Epoch 55/80] Train Loss: 0.6852, Val Loss: 0.6945\n",
      "[Epoch 60/80] Train Loss: 0.6854, Val Loss: 0.6934\n",
      "[Epoch 65/80] Train Loss: 0.6816, Val Loss: 0.6986\n",
      "[Epoch 70/80] Train Loss: 0.6818, Val Loss: 0.6952\n",
      "[Epoch 75/80] Train Loss: 0.6772, Val Loss: 0.7019\n",
      "[Epoch 80/80] Train Loss: 0.6746, Val Loss: 0.7050\n",
      "Accuracy on Nonzero Predictions: 0.5725\n",
      "3\n",
      "Training AAPL | LR: 0.009883203468677031 | Epochs: 80 | Batch: 32 | Security type: single_name\n",
      "[Epoch 5/80] Train Loss: 0.6940, Val Loss: 0.6934\n",
      "[Epoch 10/80] Train Loss: 0.6936, Val Loss: 0.6923\n",
      "[Epoch 15/80] Train Loss: 0.6936, Val Loss: 0.6923\n",
      "[Epoch 20/80] Train Loss: 0.6936, Val Loss: 0.6921\n",
      "[Epoch 25/80] Train Loss: 0.6936, Val Loss: 0.6920\n",
      "[Epoch 30/80] Train Loss: 0.6935, Val Loss: 0.6921\n",
      "[Epoch 35/80] Train Loss: 0.6929, Val Loss: 0.6908\n",
      "[Epoch 40/80] Train Loss: 0.6916, Val Loss: 0.6868\n",
      "[Epoch 45/80] Train Loss: 0.6906, Val Loss: 0.6833\n",
      "[Epoch 50/80] Train Loss: 0.6904, Val Loss: 0.6832\n",
      "[Epoch 55/80] Train Loss: 0.6894, Val Loss: 0.6859\n",
      "[Epoch 60/80] Train Loss: 0.6900, Val Loss: 0.6869\n",
      "[Epoch 65/80] Train Loss: 0.6874, Val Loss: 0.6862\n",
      "[Epoch 70/80] Train Loss: 0.6873, Val Loss: 0.6935\n",
      "[Epoch 75/80] Train Loss: 0.6851, Val Loss: 0.6856\n",
      "[Epoch 80/80] Train Loss: 0.6819, Val Loss: 0.6949\n",
      "Accuracy on Nonzero Predictions: 0.4122\n",
      "4\n",
      "Training AAPL | LR: 0.009883203468677031 | Epochs: 80 | Batch: 32 | Security type: single_name\n",
      "[Epoch 5/80] Train Loss: 0.6939, Val Loss: 0.6939\n",
      "[Epoch 10/80] Train Loss: 0.6937, Val Loss: 0.6928\n",
      "[Epoch 15/80] Train Loss: 0.6936, Val Loss: 0.6923\n",
      "[Epoch 20/80] Train Loss: 0.6935, Val Loss: 0.6923\n",
      "[Epoch 25/80] Train Loss: 0.6936, Val Loss: 0.6919\n",
      "[Epoch 30/80] Train Loss: 0.6934, Val Loss: 0.6910\n",
      "[Epoch 35/80] Train Loss: 0.6921, Val Loss: 0.6898\n",
      "[Epoch 40/80] Train Loss: 0.6916, Val Loss: 0.6910\n",
      "[Epoch 45/80] Train Loss: 0.6918, Val Loss: 0.6860\n",
      "[Epoch 50/80] Train Loss: 0.6892, Val Loss: 0.6901\n",
      "[Epoch 55/80] Train Loss: 0.6899, Val Loss: 0.6862\n",
      "[Epoch 60/80] Train Loss: 0.6893, Val Loss: 0.6908\n",
      "[Epoch 65/80] Train Loss: 0.6865, Val Loss: 0.6865\n",
      "[Epoch 70/80] Train Loss: 0.6847, Val Loss: 0.6872\n",
      "[Epoch 75/80] Train Loss: 0.6835, Val Loss: 0.6870\n",
      "[Epoch 80/80] Train Loss: 0.6830, Val Loss: 0.6895\n",
      "Accuracy on Nonzero Predictions: 0.5954\n",
      "5\n",
      "Training AAPL | LR: 0.009883203468677031 | Epochs: 80 | Batch: 32 | Security type: single_name\n",
      "[Epoch 5/80] Train Loss: 0.6940, Val Loss: 0.6938\n",
      "[Epoch 10/80] Train Loss: 0.6936, Val Loss: 0.6923\n",
      "[Epoch 15/80] Train Loss: 0.6937, Val Loss: 0.6921\n",
      "[Epoch 20/80] Train Loss: 0.6936, Val Loss: 0.6921\n",
      "[Epoch 25/80] Train Loss: 0.6936, Val Loss: 0.6923\n",
      "[Epoch 30/80] Train Loss: 0.6936, Val Loss: 0.6920\n",
      "[Epoch 35/80] Train Loss: 0.6936, Val Loss: 0.6922\n",
      "[Epoch 40/80] Train Loss: 0.6936, Val Loss: 0.6926\n",
      "[Epoch 45/80] Train Loss: 0.6936, Val Loss: 0.6917\n",
      "[Epoch 50/80] Train Loss: 0.6936, Val Loss: 0.6922\n",
      "[Epoch 55/80] Train Loss: 0.6935, Val Loss: 0.6920\n",
      "[Epoch 60/80] Train Loss: 0.6941, Val Loss: 0.6909\n",
      "[Epoch 65/80] Train Loss: 0.6936, Val Loss: 0.6922\n",
      "[Epoch 70/80] Train Loss: 0.6936, Val Loss: 0.6923\n",
      "[Epoch 75/80] Train Loss: 0.6937, Val Loss: 0.6927\n",
      "[Epoch 80/80] Train Loss: 0.6937, Val Loss: 0.6932\n",
      "Accuracy on Nonzero Predictions: 0.5573\n",
      "6\n",
      "Training AAPL | LR: 0.009883203468677031 | Epochs: 80 | Batch: 32 | Security type: single_name\n",
      "[Epoch 5/80] Train Loss: 0.6938, Val Loss: 0.6922\n",
      "[Epoch 10/80] Train Loss: 0.6936, Val Loss: 0.6924\n",
      "[Epoch 15/80] Train Loss: 0.6936, Val Loss: 0.6922\n",
      "[Epoch 20/80] Train Loss: 0.6936, Val Loss: 0.6926\n",
      "[Epoch 25/80] Train Loss: 0.6936, Val Loss: 0.6923\n",
      "[Epoch 30/80] Train Loss: 0.6936, Val Loss: 0.6923\n",
      "[Epoch 35/80] Train Loss: 0.6933, Val Loss: 0.6919\n",
      "[Epoch 40/80] Train Loss: 0.6934, Val Loss: 0.6928\n",
      "[Epoch 45/80] Train Loss: 0.6938, Val Loss: 0.6924\n",
      "[Epoch 50/80] Train Loss: 0.6935, Val Loss: 0.6923\n",
      "[Epoch 55/80] Train Loss: 0.6928, Val Loss: 0.6920\n",
      "[Epoch 60/80] Train Loss: 0.6922, Val Loss: 0.6921\n",
      "[Epoch 65/80] Train Loss: 0.6924, Val Loss: 0.6922\n",
      "[Epoch 70/80] Train Loss: 0.6918, Val Loss: 0.6901\n",
      "[Epoch 75/80] Train Loss: 0.6912, Val Loss: 0.6921\n",
      "[Epoch 80/80] Train Loss: 0.6899, Val Loss: 0.6869\n",
      "Accuracy on Nonzero Predictions: 0.5878\n",
      "7\n",
      "Training AAPL | LR: 0.009883203468677031 | Epochs: 80 | Batch: 32 | Security type: single_name\n",
      "[Epoch 5/80] Train Loss: 0.6939, Val Loss: 0.6934\n",
      "[Epoch 10/80] Train Loss: 0.6938, Val Loss: 0.6921\n",
      "[Epoch 15/80] Train Loss: 0.6936, Val Loss: 0.6923\n",
      "[Epoch 20/80] Train Loss: 0.6936, Val Loss: 0.6924\n",
      "[Epoch 25/80] Train Loss: 0.6927, Val Loss: 0.6932\n",
      "[Epoch 30/80] Train Loss: 0.6910, Val Loss: 0.6941\n",
      "[Epoch 35/80] Train Loss: 0.6918, Val Loss: 0.6926\n",
      "[Epoch 40/80] Train Loss: 0.6908, Val Loss: 0.6923\n",
      "[Epoch 45/80] Train Loss: 0.6906, Val Loss: 0.6922\n",
      "[Epoch 50/80] Train Loss: 0.6881, Val Loss: 0.6920\n",
      "[Epoch 55/80] Train Loss: 0.6877, Val Loss: 0.6917\n",
      "[Epoch 60/80] Train Loss: 0.6887, Val Loss: 0.6932\n",
      "[Epoch 65/80] Train Loss: 0.6872, Val Loss: 0.6916\n",
      "[Epoch 70/80] Train Loss: 0.6850, Val Loss: 0.6977\n",
      "[Epoch 75/80] Train Loss: 0.6834, Val Loss: 0.6932\n",
      "[Epoch 80/80] Train Loss: 0.6844, Val Loss: 0.6919\n",
      "Accuracy on Nonzero Predictions: 0.5344\n",
      "8\n",
      "Training AAPL | LR: 0.009883203468677031 | Epochs: 80 | Batch: 32 | Security type: single_name\n",
      "[Epoch 5/80] Train Loss: 0.6938, Val Loss: 0.6925\n",
      "[Epoch 10/80] Train Loss: 0.6936, Val Loss: 0.6923\n",
      "[Epoch 15/80] Train Loss: 0.6936, Val Loss: 0.6923\n",
      "[Epoch 20/80] Train Loss: 0.6936, Val Loss: 0.6924\n",
      "[Epoch 25/80] Train Loss: 0.6934, Val Loss: 0.6930\n",
      "[Epoch 30/80] Train Loss: 0.6931, Val Loss: 0.6900\n",
      "[Epoch 35/80] Train Loss: 0.6922, Val Loss: 0.6849\n",
      "[Epoch 40/80] Train Loss: 0.6918, Val Loss: 0.6875\n",
      "[Epoch 45/80] Train Loss: 0.6909, Val Loss: 0.6864\n",
      "[Epoch 50/80] Train Loss: 0.6894, Val Loss: 0.6852\n",
      "[Epoch 55/80] Train Loss: 0.6909, Val Loss: 0.6877\n",
      "[Epoch 60/80] Train Loss: 0.6891, Val Loss: 0.6838\n",
      "[Epoch 65/80] Train Loss: 0.6880, Val Loss: 0.6820\n",
      "[Epoch 70/80] Train Loss: 0.6877, Val Loss: 0.6796\n",
      "[Epoch 75/80] Train Loss: 0.6871, Val Loss: 0.6794\n",
      "[Epoch 80/80] Train Loss: 0.6868, Val Loss: 0.6793\n",
      "Accuracy on Nonzero Predictions: 0.4504\n",
      "9\n",
      "Training AAPL | LR: 0.009883203468677031 | Epochs: 80 | Batch: 32 | Security type: single_name\n",
      "[Epoch 5/80] Train Loss: 0.6941, Val Loss: 0.6926\n",
      "[Epoch 10/80] Train Loss: 0.6936, Val Loss: 0.6924\n",
      "[Epoch 15/80] Train Loss: 0.6936, Val Loss: 0.6923\n",
      "[Epoch 20/80] Train Loss: 0.6936, Val Loss: 0.6922\n",
      "[Epoch 25/80] Train Loss: 0.6936, Val Loss: 0.6922\n",
      "[Epoch 30/80] Train Loss: 0.6934, Val Loss: 0.6911\n",
      "[Epoch 35/80] Train Loss: 0.6929, Val Loss: 0.6908\n",
      "[Epoch 40/80] Train Loss: 0.6927, Val Loss: 0.6875\n",
      "[Epoch 45/80] Train Loss: 0.6917, Val Loss: 0.6842\n",
      "[Epoch 50/80] Train Loss: 0.6899, Val Loss: 0.6839\n",
      "[Epoch 55/80] Train Loss: 0.6899, Val Loss: 0.6860\n",
      "[Epoch 60/80] Train Loss: 0.6895, Val Loss: 0.6921\n",
      "[Epoch 65/80] Train Loss: 0.6885, Val Loss: 0.6901\n",
      "[Epoch 70/80] Train Loss: 0.6869, Val Loss: 0.6940\n",
      "[Epoch 75/80] Train Loss: 0.6843, Val Loss: 0.6986\n",
      "[Epoch 80/80] Train Loss: 0.6824, Val Loss: 0.7019\n",
      "Accuracy on Nonzero Predictions: 0.4504\n",
      "10\n",
      "Training AAPL | LR: 0.009883203468677031 | Epochs: 80 | Batch: 32 | Security type: single_name\n",
      "[Epoch 5/80] Train Loss: 0.6938, Val Loss: 0.6930\n",
      "[Epoch 10/80] Train Loss: 0.6936, Val Loss: 0.6924\n",
      "[Epoch 15/80] Train Loss: 0.6936, Val Loss: 0.6924\n",
      "[Epoch 20/80] Train Loss: 0.6936, Val Loss: 0.6924\n",
      "[Epoch 25/80] Train Loss: 0.6936, Val Loss: 0.6923\n",
      "[Epoch 30/80] Train Loss: 0.6936, Val Loss: 0.6923\n",
      "[Epoch 35/80] Train Loss: 0.6936, Val Loss: 0.6923\n",
      "[Epoch 40/80] Train Loss: 0.6936, Val Loss: 0.6923\n",
      "[Epoch 45/80] Train Loss: 0.6936, Val Loss: 0.6923\n",
      "[Epoch 50/80] Train Loss: 0.6942, Val Loss: 0.6920\n",
      "[Epoch 55/80] Train Loss: 0.6936, Val Loss: 0.6923\n",
      "[Epoch 60/80] Train Loss: 0.6936, Val Loss: 0.6923\n",
      "[Epoch 65/80] Train Loss: 0.6936, Val Loss: 0.6923\n",
      "[Epoch 70/80] Train Loss: 0.6936, Val Loss: 0.6923\n",
      "[Epoch 75/80] Train Loss: 0.6936, Val Loss: 0.6923\n",
      "[Epoch 80/80] Train Loss: 0.6936, Val Loss: 0.6923\n",
      "Accuracy on Nonzero Predictions: 0.5878\n",
      "11\n",
      "Training AAPL | LR: 0.009883203468677031 | Epochs: 80 | Batch: 32 | Security type: single_name\n",
      "[Epoch 5/80] Train Loss: 0.6939, Val Loss: 0.6929\n",
      "[Epoch 10/80] Train Loss: 0.6936, Val Loss: 0.6925\n",
      "[Epoch 15/80] Train Loss: 0.6936, Val Loss: 0.6924\n",
      "[Epoch 20/80] Train Loss: 0.6936, Val Loss: 0.6923\n",
      "[Epoch 25/80] Train Loss: 0.6936, Val Loss: 0.6922\n",
      "[Epoch 30/80] Train Loss: 0.6934, Val Loss: 0.6922\n",
      "[Epoch 35/80] Train Loss: 0.6925, Val Loss: 0.6896\n",
      "[Epoch 40/80] Train Loss: 0.6921, Val Loss: 0.6879\n",
      "[Epoch 45/80] Train Loss: 0.6911, Val Loss: 0.6845\n",
      "[Epoch 50/80] Train Loss: 0.6903, Val Loss: 0.6815\n",
      "[Epoch 55/80] Train Loss: 0.6897, Val Loss: 0.6830\n",
      "[Epoch 60/80] Train Loss: 0.6883, Val Loss: 0.6824\n",
      "[Epoch 65/80] Train Loss: 0.6866, Val Loss: 0.6874\n",
      "[Epoch 70/80] Train Loss: 0.6854, Val Loss: 0.6875\n",
      "[Epoch 75/80] Train Loss: 0.6828, Val Loss: 0.6864\n",
      "[Epoch 80/80] Train Loss: 0.6840, Val Loss: 0.6938\n",
      "Accuracy on Nonzero Predictions: 0.5573\n",
      "12\n",
      "Training AAPL | LR: 0.009883203468677031 | Epochs: 80 | Batch: 32 | Security type: single_name\n",
      "[Epoch 5/80] Train Loss: 0.6937, Val Loss: 0.6937\n",
      "[Epoch 10/80] Train Loss: 0.6935, Val Loss: 0.6926\n",
      "[Epoch 15/80] Train Loss: 0.6936, Val Loss: 0.6924\n",
      "[Epoch 20/80] Train Loss: 0.6934, Val Loss: 0.6923\n",
      "[Epoch 25/80] Train Loss: 0.6921, Val Loss: 0.6906\n",
      "[Epoch 30/80] Train Loss: 0.6919, Val Loss: 0.6885\n",
      "[Epoch 35/80] Train Loss: 0.6909, Val Loss: 0.6892\n",
      "[Epoch 40/80] Train Loss: 0.6908, Val Loss: 0.6851\n",
      "[Epoch 45/80] Train Loss: 0.6903, Val Loss: 0.6890\n",
      "[Epoch 50/80] Train Loss: 0.6887, Val Loss: 0.6897\n",
      "[Epoch 55/80] Train Loss: 0.6870, Val Loss: 0.6909\n",
      "[Epoch 60/80] Train Loss: 0.6863, Val Loss: 0.6940\n",
      "[Epoch 65/80] Train Loss: 0.6858, Val Loss: 0.6962\n",
      "[Epoch 70/80] Train Loss: 0.6837, Val Loss: 0.6996\n",
      "[Epoch 75/80] Train Loss: 0.6833, Val Loss: 0.7048\n",
      "[Epoch 80/80] Train Loss: 0.6832, Val Loss: 0.7091\n",
      "Accuracy on Nonzero Predictions: 0.5344\n",
      "13\n",
      "Training AAPL | LR: 0.009883203468677031 | Epochs: 80 | Batch: 32 | Security type: single_name\n",
      "[Epoch 5/80] Train Loss: 0.6938, Val Loss: 0.6940\n",
      "[Epoch 10/80] Train Loss: 0.6937, Val Loss: 0.6925\n",
      "[Epoch 15/80] Train Loss: 0.6935, Val Loss: 0.6918\n",
      "[Epoch 20/80] Train Loss: 0.6927, Val Loss: 0.6938\n",
      "[Epoch 25/80] Train Loss: 0.6925, Val Loss: 0.6945\n",
      "[Epoch 30/80] Train Loss: 0.6914, Val Loss: 0.6941\n",
      "[Epoch 35/80] Train Loss: 0.6917, Val Loss: 0.6937\n",
      "[Epoch 40/80] Train Loss: 0.6897, Val Loss: 0.6945\n",
      "[Epoch 45/80] Train Loss: 0.6904, Val Loss: 0.6886\n",
      "[Epoch 50/80] Train Loss: 0.6897, Val Loss: 0.6921\n",
      "[Epoch 55/80] Train Loss: 0.6865, Val Loss: 0.6906\n",
      "[Epoch 60/80] Train Loss: 0.6843, Val Loss: 0.6859\n",
      "[Epoch 65/80] Train Loss: 0.6847, Val Loss: 0.6909\n",
      "[Epoch 70/80] Train Loss: 0.6820, Val Loss: 0.6911\n",
      "[Epoch 75/80] Train Loss: 0.6772, Val Loss: 0.6938\n",
      "[Epoch 80/80] Train Loss: 0.6767, Val Loss: 0.6932\n",
      "Accuracy on Nonzero Predictions: 0.5802\n",
      "14\n",
      "Training AAPL | LR: 0.009883203468677031 | Epochs: 80 | Batch: 32 | Security type: single_name\n",
      "[Epoch 5/80] Train Loss: 0.6938, Val Loss: 0.6926\n",
      "[Epoch 10/80] Train Loss: 0.6937, Val Loss: 0.6928\n",
      "[Epoch 15/80] Train Loss: 0.6936, Val Loss: 0.6922\n",
      "[Epoch 20/80] Train Loss: 0.6936, Val Loss: 0.6919\n",
      "[Epoch 25/80] Train Loss: 0.6935, Val Loss: 0.6925\n",
      "[Epoch 30/80] Train Loss: 0.6935, Val Loss: 0.6938\n",
      "[Epoch 35/80] Train Loss: 0.6925, Val Loss: 0.6915\n",
      "[Epoch 40/80] Train Loss: 0.6918, Val Loss: 0.6943\n",
      "[Epoch 45/80] Train Loss: 0.6911, Val Loss: 0.6910\n",
      "[Epoch 50/80] Train Loss: 0.6908, Val Loss: 0.6891\n",
      "[Epoch 55/80] Train Loss: 0.6893, Val Loss: 0.6942\n",
      "[Epoch 60/80] Train Loss: 0.6864, Val Loss: 0.6945\n",
      "[Epoch 65/80] Train Loss: 0.6862, Val Loss: 0.6956\n",
      "[Epoch 70/80] Train Loss: 0.6851, Val Loss: 0.6942\n",
      "[Epoch 75/80] Train Loss: 0.6824, Val Loss: 0.6988\n",
      "[Epoch 80/80] Train Loss: 0.6817, Val Loss: 0.6937\n",
      "Accuracy on Nonzero Predictions: 0.5725\n",
      "15\n",
      "Training AAPL | LR: 0.009883203468677031 | Epochs: 80 | Batch: 32 | Security type: single_name\n",
      "[Epoch 5/80] Train Loss: 0.6939, Val Loss: 0.6932\n",
      "[Epoch 10/80] Train Loss: 0.6936, Val Loss: 0.6924\n",
      "[Epoch 15/80] Train Loss: 0.6936, Val Loss: 0.6921\n",
      "[Epoch 20/80] Train Loss: 0.6937, Val Loss: 0.6917\n",
      "[Epoch 25/80] Train Loss: 0.6934, Val Loss: 0.6925\n",
      "[Epoch 30/80] Train Loss: 0.6919, Val Loss: 0.6892\n",
      "[Epoch 35/80] Train Loss: 0.6926, Val Loss: 0.6911\n",
      "[Epoch 40/80] Train Loss: 0.6910, Val Loss: 0.6914\n",
      "[Epoch 45/80] Train Loss: 0.6910, Val Loss: 0.6920\n",
      "[Epoch 50/80] Train Loss: 0.6907, Val Loss: 0.6901\n",
      "[Epoch 55/80] Train Loss: 0.6892, Val Loss: 0.6926\n",
      "[Epoch 60/80] Train Loss: 0.6899, Val Loss: 0.6878\n",
      "[Epoch 65/80] Train Loss: 0.6876, Val Loss: 0.6924\n",
      "[Epoch 70/80] Train Loss: 0.6861, Val Loss: 0.6924\n",
      "[Epoch 75/80] Train Loss: 0.6852, Val Loss: 0.6934\n",
      "[Epoch 80/80] Train Loss: 0.6834, Val Loss: 0.6966\n",
      "Accuracy on Nonzero Predictions: 0.4046\n",
      "16\n",
      "Training AAPL | LR: 0.009883203468677031 | Epochs: 80 | Batch: 32 | Security type: single_name\n",
      "[Epoch 5/80] Train Loss: 0.6940, Val Loss: 0.6936\n",
      "[Epoch 10/80] Train Loss: 0.6936, Val Loss: 0.6924\n",
      "[Epoch 15/80] Train Loss: 0.6936, Val Loss: 0.6924\n",
      "[Epoch 20/80] Train Loss: 0.6936, Val Loss: 0.6923\n",
      "[Epoch 25/80] Train Loss: 0.6936, Val Loss: 0.6923\n",
      "[Epoch 30/80] Train Loss: 0.6936, Val Loss: 0.6922\n",
      "[Epoch 35/80] Train Loss: 0.6936, Val Loss: 0.6923\n",
      "[Epoch 40/80] Train Loss: 0.6935, Val Loss: 0.6913\n",
      "[Epoch 45/80] Train Loss: 0.6926, Val Loss: 0.6894\n",
      "[Epoch 50/80] Train Loss: 0.6932, Val Loss: 0.6894\n",
      "[Epoch 55/80] Train Loss: 0.6905, Val Loss: 0.6913\n",
      "[Epoch 60/80] Train Loss: 0.6894, Val Loss: 0.6882\n",
      "[Epoch 65/80] Train Loss: 0.6878, Val Loss: 0.6889\n",
      "[Epoch 70/80] Train Loss: 0.6872, Val Loss: 0.6886\n",
      "[Epoch 75/80] Train Loss: 0.6867, Val Loss: 0.6877\n",
      "[Epoch 80/80] Train Loss: 0.6853, Val Loss: 0.6835\n",
      "Accuracy on Nonzero Predictions: 0.5878\n",
      "17\n",
      "Training AAPL | LR: 0.009883203468677031 | Epochs: 80 | Batch: 32 | Security type: single_name\n",
      "[Epoch 5/80] Train Loss: 0.6940, Val Loss: 0.6937\n",
      "[Epoch 10/80] Train Loss: 0.6937, Val Loss: 0.6925\n",
      "[Epoch 15/80] Train Loss: 0.6936, Val Loss: 0.6923\n",
      "[Epoch 20/80] Train Loss: 0.6935, Val Loss: 0.6921\n",
      "[Epoch 25/80] Train Loss: 0.6936, Val Loss: 0.6922\n",
      "[Epoch 30/80] Train Loss: 0.6935, Val Loss: 0.6922\n",
      "[Epoch 35/80] Train Loss: 0.6936, Val Loss: 0.6922\n",
      "[Epoch 40/80] Train Loss: 0.6934, Val Loss: 0.6902\n",
      "[Epoch 45/80] Train Loss: 0.6921, Val Loss: 0.6827\n",
      "[Epoch 50/80] Train Loss: 0.6924, Val Loss: 0.6848\n",
      "[Epoch 55/80] Train Loss: 0.6905, Val Loss: 0.6763\n",
      "[Epoch 60/80] Train Loss: 0.6906, Val Loss: 0.6766\n",
      "[Epoch 65/80] Train Loss: 0.6893, Val Loss: 0.6720\n",
      "[Epoch 70/80] Train Loss: 0.6884, Val Loss: 0.6702\n",
      "[Epoch 75/80] Train Loss: 0.6876, Val Loss: 0.6690\n",
      "[Epoch 80/80] Train Loss: 0.6870, Val Loss: 0.6675\n",
      "Accuracy on Nonzero Predictions: 0.5802\n",
      "18\n",
      "Training AAPL | LR: 0.009883203468677031 | Epochs: 80 | Batch: 32 | Security type: single_name\n",
      "[Epoch 5/80] Train Loss: 0.6942, Val Loss: 0.6942\n",
      "[Epoch 10/80] Train Loss: 0.6936, Val Loss: 0.6923\n",
      "[Epoch 15/80] Train Loss: 0.6936, Val Loss: 0.6920\n",
      "[Epoch 20/80] Train Loss: 0.6935, Val Loss: 0.6925\n",
      "[Epoch 25/80] Train Loss: 0.6929, Val Loss: 0.6932\n",
      "[Epoch 30/80] Train Loss: 0.6916, Val Loss: 0.6909\n",
      "[Epoch 35/80] Train Loss: 0.6913, Val Loss: 0.6869\n",
      "[Epoch 40/80] Train Loss: 0.6910, Val Loss: 0.6887\n",
      "[Epoch 45/80] Train Loss: 0.6905, Val Loss: 0.6885\n",
      "[Epoch 50/80] Train Loss: 0.6887, Val Loss: 0.6952\n",
      "[Epoch 55/80] Train Loss: 0.6883, Val Loss: 0.6926\n",
      "[Epoch 60/80] Train Loss: 0.6884, Val Loss: 0.6869\n",
      "[Epoch 65/80] Train Loss: 0.6857, Val Loss: 0.6885\n",
      "[Epoch 70/80] Train Loss: 0.6828, Val Loss: 0.6902\n",
      "[Epoch 75/80] Train Loss: 0.6855, Val Loss: 0.6849\n",
      "[Epoch 80/80] Train Loss: 0.6807, Val Loss: 0.6868\n",
      "Accuracy on Nonzero Predictions: 0.5420\n",
      "19\n",
      "Training AAPL | LR: 0.009883203468677031 | Epochs: 80 | Batch: 32 | Security type: single_name\n",
      "[Epoch 5/80] Train Loss: 0.6939, Val Loss: 0.6924\n",
      "[Epoch 10/80] Train Loss: 0.6938, Val Loss: 0.6930\n",
      "[Epoch 15/80] Train Loss: 0.6936, Val Loss: 0.6924\n",
      "[Epoch 20/80] Train Loss: 0.6936, Val Loss: 0.6924\n",
      "[Epoch 25/80] Train Loss: 0.6937, Val Loss: 0.6920\n",
      "[Epoch 30/80] Train Loss: 0.6930, Val Loss: 0.6930\n",
      "[Epoch 35/80] Train Loss: 0.6921, Val Loss: 0.6912\n",
      "[Epoch 40/80] Train Loss: 0.6912, Val Loss: 0.6877\n",
      "[Epoch 45/80] Train Loss: 0.6906, Val Loss: 0.6889\n",
      "[Epoch 50/80] Train Loss: 0.6891, Val Loss: 0.6808\n",
      "[Epoch 55/80] Train Loss: 0.6867, Val Loss: 0.6804\n",
      "[Epoch 60/80] Train Loss: 0.6870, Val Loss: 0.6842\n",
      "[Epoch 65/80] Train Loss: 0.6853, Val Loss: 0.6829\n",
      "[Epoch 70/80] Train Loss: 0.6851, Val Loss: 0.6874\n",
      "[Epoch 75/80] Train Loss: 0.6844, Val Loss: 0.6861\n",
      "[Epoch 80/80] Train Loss: 0.6824, Val Loss: 0.6944\n",
      "Accuracy on Nonzero Predictions: 0.5649\n",
      "20\n",
      "Training AAPL | LR: 0.009883203468677031 | Epochs: 80 | Batch: 32 | Security type: single_name\n",
      "[Epoch 5/80] Train Loss: 0.6938, Val Loss: 0.6930\n",
      "[Epoch 10/80] Train Loss: 0.6936, Val Loss: 0.6926\n",
      "[Epoch 15/80] Train Loss: 0.6936, Val Loss: 0.6924\n",
      "[Epoch 20/80] Train Loss: 0.6936, Val Loss: 0.6924\n",
      "[Epoch 25/80] Train Loss: 0.6936, Val Loss: 0.6922\n",
      "[Epoch 30/80] Train Loss: 0.6934, Val Loss: 0.6927\n",
      "[Epoch 35/80] Train Loss: 0.6928, Val Loss: 0.6900\n",
      "[Epoch 40/80] Train Loss: 0.6908, Val Loss: 0.6831\n",
      "[Epoch 45/80] Train Loss: 0.6909, Val Loss: 0.6856\n",
      "[Epoch 50/80] Train Loss: 0.6898, Val Loss: 0.6826\n",
      "[Epoch 55/80] Train Loss: 0.6869, Val Loss: 0.6840\n",
      "[Epoch 60/80] Train Loss: 0.6888, Val Loss: 0.6836\n",
      "[Epoch 65/80] Train Loss: 0.6873, Val Loss: 0.6839\n",
      "[Epoch 70/80] Train Loss: 0.6861, Val Loss: 0.6847\n",
      "[Epoch 75/80] Train Loss: 0.6858, Val Loss: 0.6849\n",
      "[Epoch 80/80] Train Loss: 0.6836, Val Loss: 0.6869\n",
      "Accuracy on Nonzero Predictions: 0.4427\n",
      "21\n",
      "Training AAPL | LR: 0.009883203468677031 | Epochs: 80 | Batch: 32 | Security type: single_name\n",
      "[Epoch 5/80] Train Loss: 0.6940, Val Loss: 0.6928\n",
      "[Epoch 10/80] Train Loss: 0.6936, Val Loss: 0.6923\n",
      "[Epoch 15/80] Train Loss: 0.6936, Val Loss: 0.6923\n",
      "[Epoch 20/80] Train Loss: 0.6936, Val Loss: 0.6923\n",
      "[Epoch 25/80] Train Loss: 0.6936, Val Loss: 0.6923\n",
      "[Epoch 30/80] Train Loss: 0.6936, Val Loss: 0.6923\n",
      "[Epoch 35/80] Train Loss: 0.6936, Val Loss: 0.6923\n",
      "[Epoch 40/80] Train Loss: 0.6936, Val Loss: 0.6923\n",
      "[Epoch 45/80] Train Loss: 0.6936, Val Loss: 0.6923\n",
      "[Epoch 50/80] Train Loss: 0.6936, Val Loss: 0.6923\n",
      "[Epoch 55/80] Train Loss: 0.6936, Val Loss: 0.6923\n",
      "[Epoch 60/80] Train Loss: 0.6938, Val Loss: 0.6923\n",
      "[Epoch 65/80] Train Loss: 0.6936, Val Loss: 0.6923\n",
      "[Epoch 70/80] Train Loss: 0.6936, Val Loss: 0.6923\n",
      "[Epoch 75/80] Train Loss: 0.6937, Val Loss: 0.6922\n",
      "[Epoch 80/80] Train Loss: 0.6937, Val Loss: 0.6923\n",
      "Accuracy on Nonzero Predictions: 0.5878\n",
      "22\n",
      "Training AAPL | LR: 0.009883203468677031 | Epochs: 80 | Batch: 32 | Security type: single_name\n",
      "[Epoch 5/80] Train Loss: 0.6938, Val Loss: 0.6937\n",
      "[Epoch 10/80] Train Loss: 0.6936, Val Loss: 0.6924\n",
      "[Epoch 15/80] Train Loss: 0.6936, Val Loss: 0.6922\n",
      "[Epoch 20/80] Train Loss: 0.6935, Val Loss: 0.6921\n",
      "[Epoch 25/80] Train Loss: 0.6930, Val Loss: 0.6923\n",
      "[Epoch 30/80] Train Loss: 0.6916, Val Loss: 0.6916\n",
      "[Epoch 35/80] Train Loss: 0.6906, Val Loss: 0.6881\n",
      "[Epoch 40/80] Train Loss: 0.6913, Val Loss: 0.6903\n",
      "[Epoch 45/80] Train Loss: 0.6880, Val Loss: 0.6876\n",
      "[Epoch 50/80] Train Loss: 0.6877, Val Loss: 0.6860\n",
      "[Epoch 55/80] Train Loss: 0.6870, Val Loss: 0.6895\n",
      "[Epoch 60/80] Train Loss: 0.6859, Val Loss: 0.6879\n",
      "[Epoch 65/80] Train Loss: 0.6843, Val Loss: 0.6855\n",
      "[Epoch 70/80] Train Loss: 0.6823, Val Loss: 0.6881\n",
      "[Epoch 75/80] Train Loss: 0.6827, Val Loss: 0.6955\n",
      "[Epoch 80/80] Train Loss: 0.6794, Val Loss: 0.6915\n",
      "Accuracy on Nonzero Predictions: 0.5344\n",
      "23\n",
      "Training AAPL | LR: 0.009883203468677031 | Epochs: 80 | Batch: 32 | Security type: single_name\n",
      "[Epoch 5/80] Train Loss: 0.6941, Val Loss: 0.6937\n",
      "[Epoch 10/80] Train Loss: 0.6936, Val Loss: 0.6924\n",
      "[Epoch 15/80] Train Loss: 0.6936, Val Loss: 0.6923\n",
      "[Epoch 20/80] Train Loss: 0.6936, Val Loss: 0.6923\n",
      "[Epoch 25/80] Train Loss: 0.6936, Val Loss: 0.6921\n",
      "[Epoch 30/80] Train Loss: 0.6936, Val Loss: 0.6923\n",
      "[Epoch 35/80] Train Loss: 0.6936, Val Loss: 0.6923\n",
      "[Epoch 40/80] Train Loss: 0.6936, Val Loss: 0.6924\n",
      "[Epoch 45/80] Train Loss: 0.6935, Val Loss: 0.6931\n",
      "[Epoch 50/80] Train Loss: 0.6936, Val Loss: 0.6923\n",
      "[Epoch 55/80] Train Loss: 0.6936, Val Loss: 0.6923\n",
      "[Epoch 60/80] Train Loss: 0.6933, Val Loss: 0.6923\n",
      "[Epoch 65/80] Train Loss: 0.6928, Val Loss: 0.6920\n",
      "[Epoch 70/80] Train Loss: 0.6926, Val Loss: 0.6919\n",
      "[Epoch 75/80] Train Loss: 0.6919, Val Loss: 0.6919\n",
      "[Epoch 80/80] Train Loss: 0.6919, Val Loss: 0.6918\n",
      "Accuracy on Nonzero Predictions: 0.5878\n",
      "24\n",
      "Training AAPL | LR: 0.009883203468677031 | Epochs: 80 | Batch: 32 | Security type: single_name\n",
      "[Epoch 5/80] Train Loss: 0.6939, Val Loss: 0.6933\n",
      "[Epoch 10/80] Train Loss: 0.6936, Val Loss: 0.6923\n",
      "[Epoch 15/80] Train Loss: 0.6936, Val Loss: 0.6923\n",
      "[Epoch 20/80] Train Loss: 0.6936, Val Loss: 0.6923\n",
      "[Epoch 25/80] Train Loss: 0.6936, Val Loss: 0.6923\n",
      "[Epoch 30/80] Train Loss: 0.6936, Val Loss: 0.6922\n",
      "[Epoch 35/80] Train Loss: 0.6934, Val Loss: 0.6922\n",
      "[Epoch 40/80] Train Loss: 0.6931, Val Loss: 0.6910\n",
      "[Epoch 45/80] Train Loss: 0.6925, Val Loss: 0.6911\n",
      "[Epoch 50/80] Train Loss: 0.6919, Val Loss: 0.6916\n",
      "[Epoch 55/80] Train Loss: 0.6907, Val Loss: 0.6911\n",
      "[Epoch 60/80] Train Loss: 0.6898, Val Loss: 0.6946\n",
      "[Epoch 65/80] Train Loss: 0.6892, Val Loss: 0.6916\n",
      "[Epoch 70/80] Train Loss: 0.6876, Val Loss: 0.6914\n",
      "[Epoch 75/80] Train Loss: 0.6884, Val Loss: 0.6932\n",
      "[Epoch 80/80] Train Loss: 0.6850, Val Loss: 0.6963\n",
      "Accuracy on Nonzero Predictions: 0.5573\n",
      "25\n",
      "Training AAPL | LR: 0.009883203468677031 | Epochs: 80 | Batch: 32 | Security type: single_name\n",
      "[Epoch 5/80] Train Loss: 0.6941, Val Loss: 0.6940\n",
      "[Epoch 10/80] Train Loss: 0.6936, Val Loss: 0.6923\n",
      "[Epoch 15/80] Train Loss: 0.6936, Val Loss: 0.6923\n",
      "[Epoch 20/80] Train Loss: 0.6936, Val Loss: 0.6920\n",
      "[Epoch 25/80] Train Loss: 0.6936, Val Loss: 0.6921\n",
      "[Epoch 30/80] Train Loss: 0.6932, Val Loss: 0.6918\n",
      "[Epoch 35/80] Train Loss: 0.6930, Val Loss: 0.6889\n",
      "[Epoch 40/80] Train Loss: 0.6922, Val Loss: 0.6900\n",
      "[Epoch 45/80] Train Loss: 0.6912, Val Loss: 0.6855\n",
      "[Epoch 50/80] Train Loss: 0.6917, Val Loss: 0.6847\n",
      "[Epoch 55/80] Train Loss: 0.6902, Val Loss: 0.6880\n",
      "[Epoch 60/80] Train Loss: 0.6883, Val Loss: 0.6875\n",
      "[Epoch 65/80] Train Loss: 0.6894, Val Loss: 0.6846\n",
      "[Epoch 70/80] Train Loss: 0.6878, Val Loss: 0.6857\n",
      "[Epoch 75/80] Train Loss: 0.6865, Val Loss: 0.6886\n",
      "[Epoch 80/80] Train Loss: 0.6840, Val Loss: 0.6925\n",
      "Accuracy on Nonzero Predictions: 0.4656\n",
      "26\n",
      "Training AAPL | LR: 0.009883203468677031 | Epochs: 80 | Batch: 32 | Security type: single_name\n",
      "[Epoch 5/80] Train Loss: 0.6941, Val Loss: 0.6939\n",
      "[Epoch 10/80] Train Loss: 0.6936, Val Loss: 0.6924\n",
      "[Epoch 15/80] Train Loss: 0.6936, Val Loss: 0.6923\n",
      "[Epoch 20/80] Train Loss: 0.6936, Val Loss: 0.6922\n",
      "[Epoch 25/80] Train Loss: 0.6936, Val Loss: 0.6923\n",
      "[Epoch 30/80] Train Loss: 0.6936, Val Loss: 0.6923\n",
      "[Epoch 35/80] Train Loss: 0.6937, Val Loss: 0.6921\n",
      "[Epoch 40/80] Train Loss: 0.6936, Val Loss: 0.6923\n",
      "[Epoch 45/80] Train Loss: 0.6936, Val Loss: 0.6923\n",
      "[Epoch 50/80] Train Loss: 0.6936, Val Loss: 0.6923\n",
      "[Epoch 55/80] Train Loss: 0.6936, Val Loss: 0.6923\n",
      "[Epoch 60/80] Train Loss: 0.6936, Val Loss: 0.6923\n",
      "[Epoch 65/80] Train Loss: 0.6936, Val Loss: 0.6923\n",
      "[Epoch 70/80] Train Loss: 0.6936, Val Loss: 0.6923\n",
      "[Epoch 75/80] Train Loss: 0.6936, Val Loss: 0.6923\n",
      "[Epoch 80/80] Train Loss: 0.6936, Val Loss: 0.6923\n",
      "Accuracy on Nonzero Predictions: 0.5878\n",
      "27\n",
      "Training AAPL | LR: 0.009883203468677031 | Epochs: 80 | Batch: 32 | Security type: single_name\n",
      "[Epoch 5/80] Train Loss: 0.6939, Val Loss: 0.6935\n",
      "[Epoch 10/80] Train Loss: 0.6936, Val Loss: 0.6924\n",
      "[Epoch 15/80] Train Loss: 0.6935, Val Loss: 0.6929\n",
      "[Epoch 20/80] Train Loss: 0.6936, Val Loss: 0.6913\n",
      "[Epoch 25/80] Train Loss: 0.6931, Val Loss: 0.6905\n",
      "[Epoch 30/80] Train Loss: 0.6913, Val Loss: 0.6893\n",
      "[Epoch 35/80] Train Loss: 0.6910, Val Loss: 0.6915\n",
      "[Epoch 40/80] Train Loss: 0.6896, Val Loss: 0.6862\n",
      "[Epoch 45/80] Train Loss: 0.6884, Val Loss: 0.6820\n",
      "[Epoch 50/80] Train Loss: 0.6876, Val Loss: 0.6845\n",
      "[Epoch 55/80] Train Loss: 0.6870, Val Loss: 0.6838\n",
      "[Epoch 60/80] Train Loss: 0.6851, Val Loss: 0.6791\n",
      "[Epoch 65/80] Train Loss: 0.6851, Val Loss: 0.6807\n",
      "[Epoch 70/80] Train Loss: 0.6826, Val Loss: 0.6819\n",
      "[Epoch 75/80] Train Loss: 0.6800, Val Loss: 0.6803\n",
      "[Epoch 80/80] Train Loss: 0.6776, Val Loss: 0.6789\n",
      "Accuracy on Nonzero Predictions: 0.5496\n",
      "28\n",
      "Training AAPL | LR: 0.009883203468677031 | Epochs: 80 | Batch: 32 | Security type: single_name\n",
      "[Epoch 5/80] Train Loss: 0.6939, Val Loss: 0.6948\n",
      "[Epoch 10/80] Train Loss: 0.6936, Val Loss: 0.6923\n",
      "[Epoch 15/80] Train Loss: 0.6936, Val Loss: 0.6920\n",
      "[Epoch 20/80] Train Loss: 0.6936, Val Loss: 0.6918\n",
      "[Epoch 25/80] Train Loss: 0.6938, Val Loss: 0.6921\n",
      "[Epoch 30/80] Train Loss: 0.6931, Val Loss: 0.6891\n",
      "[Epoch 35/80] Train Loss: 0.6921, Val Loss: 0.6931\n",
      "[Epoch 40/80] Train Loss: 0.6909, Val Loss: 0.6898\n",
      "[Epoch 45/80] Train Loss: 0.6907, Val Loss: 0.6869\n",
      "[Epoch 50/80] Train Loss: 0.6896, Val Loss: 0.6844\n",
      "[Epoch 55/80] Train Loss: 0.6885, Val Loss: 0.6899\n",
      "[Epoch 60/80] Train Loss: 0.6857, Val Loss: 0.6875\n",
      "[Epoch 65/80] Train Loss: 0.6899, Val Loss: 0.6872\n",
      "[Epoch 70/80] Train Loss: 0.6872, Val Loss: 0.6909\n",
      "[Epoch 75/80] Train Loss: 0.6824, Val Loss: 0.6891\n",
      "[Epoch 80/80] Train Loss: 0.6829, Val Loss: 0.6874\n",
      "Accuracy on Nonzero Predictions: 0.4046\n",
      "29\n",
      "Training AAPL | LR: 0.009883203468677031 | Epochs: 80 | Batch: 32 | Security type: single_name\n",
      "[Epoch 5/80] Train Loss: 0.6940, Val Loss: 0.6936\n",
      "[Epoch 10/80] Train Loss: 0.6936, Val Loss: 0.6924\n",
      "[Epoch 15/80] Train Loss: 0.6936, Val Loss: 0.6923\n",
      "[Epoch 20/80] Train Loss: 0.6936, Val Loss: 0.6922\n",
      "[Epoch 25/80] Train Loss: 0.6934, Val Loss: 0.6927\n",
      "[Epoch 30/80] Train Loss: 0.6938, Val Loss: 0.6926\n",
      "[Epoch 35/80] Train Loss: 0.6931, Val Loss: 0.6929\n",
      "[Epoch 40/80] Train Loss: 0.6926, Val Loss: 0.6895\n",
      "[Epoch 45/80] Train Loss: 0.6925, Val Loss: 0.6886\n",
      "[Epoch 50/80] Train Loss: 0.6907, Val Loss: 0.6862\n",
      "[Epoch 55/80] Train Loss: 0.6916, Val Loss: 0.6841\n",
      "[Epoch 60/80] Train Loss: 0.6915, Val Loss: 0.6857\n",
      "[Epoch 65/80] Train Loss: 0.6894, Val Loss: 0.6832\n",
      "[Epoch 70/80] Train Loss: 0.6849, Val Loss: 0.6849\n",
      "[Epoch 75/80] Train Loss: 0.6863, Val Loss: 0.6872\n",
      "[Epoch 80/80] Train Loss: 0.6870, Val Loss: 0.6842\n",
      "Accuracy on Nonzero Predictions: 0.5420\n",
      "30\n",
      "Training AAPL | LR: 0.009883203468677031 | Epochs: 80 | Batch: 32 | Security type: single_name\n",
      "[Epoch 5/80] Train Loss: 0.6939, Val Loss: 0.6933\n",
      "[Epoch 10/80] Train Loss: 0.6936, Val Loss: 0.6924\n",
      "[Epoch 15/80] Train Loss: 0.6936, Val Loss: 0.6924\n",
      "[Epoch 20/80] Train Loss: 0.6936, Val Loss: 0.6925\n",
      "[Epoch 25/80] Train Loss: 0.6935, Val Loss: 0.6925\n",
      "[Epoch 30/80] Train Loss: 0.6929, Val Loss: 0.6933\n",
      "[Epoch 35/80] Train Loss: 0.6916, Val Loss: 0.6917\n",
      "[Epoch 40/80] Train Loss: 0.6910, Val Loss: 0.6929\n",
      "[Epoch 45/80] Train Loss: 0.6891, Val Loss: 0.6938\n",
      "[Epoch 50/80] Train Loss: 0.6904, Val Loss: 0.6926\n",
      "[Epoch 55/80] Train Loss: 0.6898, Val Loss: 0.6924\n",
      "[Epoch 60/80] Train Loss: 0.6891, Val Loss: 0.6956\n",
      "[Epoch 65/80] Train Loss: 0.6865, Val Loss: 0.6955\n",
      "[Epoch 70/80] Train Loss: 0.6879, Val Loss: 0.6912\n",
      "[Epoch 75/80] Train Loss: 0.6871, Val Loss: 0.6926\n",
      "[Epoch 80/80] Train Loss: 0.6813, Val Loss: 0.6961\n",
      "Accuracy on Nonzero Predictions: 0.5954\n",
      "31\n",
      "Training AAPL | LR: 0.009883203468677031 | Epochs: 80 | Batch: 32 | Security type: single_name\n",
      "[Epoch 5/80] Train Loss: 0.6939, Val Loss: 0.6936\n",
      "[Epoch 10/80] Train Loss: 0.6936, Val Loss: 0.6925\n",
      "[Epoch 15/80] Train Loss: 0.6936, Val Loss: 0.6924\n",
      "[Epoch 20/80] Train Loss: 0.6936, Val Loss: 0.6923\n",
      "[Epoch 25/80] Train Loss: 0.6935, Val Loss: 0.6925\n",
      "[Epoch 30/80] Train Loss: 0.6924, Val Loss: 0.6912\n",
      "[Epoch 35/80] Train Loss: 0.6910, Val Loss: 0.6918\n",
      "[Epoch 40/80] Train Loss: 0.6904, Val Loss: 0.6899\n",
      "[Epoch 45/80] Train Loss: 0.6888, Val Loss: 0.6875\n",
      "[Epoch 50/80] Train Loss: 0.6899, Val Loss: 0.6887\n",
      "[Epoch 55/80] Train Loss: 0.6848, Val Loss: 0.6877\n",
      "[Epoch 60/80] Train Loss: 0.6862, Val Loss: 0.6848\n",
      "[Epoch 65/80] Train Loss: 0.6849, Val Loss: 0.6854\n",
      "[Epoch 70/80] Train Loss: 0.6834, Val Loss: 0.6849\n",
      "[Epoch 75/80] Train Loss: 0.6818, Val Loss: 0.6838\n",
      "[Epoch 80/80] Train Loss: 0.6786, Val Loss: 0.6877\n",
      "Accuracy on Nonzero Predictions: 0.5954\n",
      "32\n",
      "Training AAPL | LR: 0.009883203468677031 | Epochs: 80 | Batch: 32 | Security type: single_name\n",
      "[Epoch 5/80] Train Loss: 0.6940, Val Loss: 0.6919\n",
      "[Epoch 10/80] Train Loss: 0.6936, Val Loss: 0.6924\n",
      "[Epoch 15/80] Train Loss: 0.6936, Val Loss: 0.6923\n",
      "[Epoch 20/80] Train Loss: 0.6936, Val Loss: 0.6923\n",
      "[Epoch 25/80] Train Loss: 0.6936, Val Loss: 0.6922\n",
      "[Epoch 30/80] Train Loss: 0.6936, Val Loss: 0.6923\n",
      "[Epoch 35/80] Train Loss: 0.6938, Val Loss: 0.6921\n",
      "[Epoch 40/80] Train Loss: 0.6934, Val Loss: 0.6918\n",
      "[Epoch 45/80] Train Loss: 0.6926, Val Loss: 0.6911\n",
      "[Epoch 50/80] Train Loss: 0.6920, Val Loss: 0.6947\n",
      "[Epoch 55/80] Train Loss: 0.6916, Val Loss: 0.6926\n",
      "[Epoch 60/80] Train Loss: 0.6905, Val Loss: 0.7005\n",
      "[Epoch 65/80] Train Loss: 0.6896, Val Loss: 0.7045\n",
      "[Epoch 70/80] Train Loss: 0.6882, Val Loss: 0.7078\n",
      "[Epoch 75/80] Train Loss: 0.6868, Val Loss: 0.7074\n",
      "[Epoch 80/80] Train Loss: 0.6869, Val Loss: 0.7074\n",
      "Accuracy on Nonzero Predictions: 0.5420\n",
      "33\n",
      "Training AAPL | LR: 0.009883203468677031 | Epochs: 80 | Batch: 32 | Security type: single_name\n",
      "[Epoch 5/80] Train Loss: 0.6940, Val Loss: 0.6924\n",
      "[Epoch 10/80] Train Loss: 0.6936, Val Loss: 0.6924\n",
      "[Epoch 15/80] Train Loss: 0.6936, Val Loss: 0.6922\n",
      "[Epoch 20/80] Train Loss: 0.6936, Val Loss: 0.6923\n",
      "[Epoch 25/80] Train Loss: 0.6936, Val Loss: 0.6926\n",
      "[Epoch 30/80] Train Loss: 0.6930, Val Loss: 0.6919\n",
      "[Epoch 35/80] Train Loss: 0.6928, Val Loss: 0.6900\n",
      "[Epoch 40/80] Train Loss: 0.6918, Val Loss: 0.6884\n",
      "[Epoch 45/80] Train Loss: 0.6916, Val Loss: 0.6872\n",
      "[Epoch 50/80] Train Loss: 0.6885, Val Loss: 0.6904\n",
      "[Epoch 55/80] Train Loss: 0.6898, Val Loss: 0.6885\n",
      "[Epoch 60/80] Train Loss: 0.6884, Val Loss: 0.6853\n",
      "[Epoch 65/80] Train Loss: 0.6886, Val Loss: 0.6864\n",
      "[Epoch 70/80] Train Loss: 0.6852, Val Loss: 0.6829\n",
      "[Epoch 75/80] Train Loss: 0.6855, Val Loss: 0.6847\n",
      "[Epoch 80/80] Train Loss: 0.6796, Val Loss: 0.6844\n",
      "Accuracy on Nonzero Predictions: 0.5420\n",
      "34\n",
      "Training AAPL | LR: 0.009883203468677031 | Epochs: 80 | Batch: 32 | Security type: single_name\n",
      "[Epoch 5/80] Train Loss: 0.6939, Val Loss: 0.6937\n",
      "[Epoch 10/80] Train Loss: 0.6936, Val Loss: 0.6926\n",
      "[Epoch 15/80] Train Loss: 0.6936, Val Loss: 0.6924\n",
      "[Epoch 20/80] Train Loss: 0.6936, Val Loss: 0.6922\n",
      "[Epoch 25/80] Train Loss: 0.6936, Val Loss: 0.6928\n",
      "[Epoch 30/80] Train Loss: 0.6929, Val Loss: 0.6928\n",
      "[Epoch 35/80] Train Loss: 0.6924, Val Loss: 0.6924\n",
      "[Epoch 40/80] Train Loss: 0.6924, Val Loss: 0.6932\n",
      "[Epoch 45/80] Train Loss: 0.6915, Val Loss: 0.6930\n",
      "[Epoch 50/80] Train Loss: 0.6902, Val Loss: 0.6961\n",
      "[Epoch 55/80] Train Loss: 0.6879, Val Loss: 0.6955\n",
      "[Epoch 60/80] Train Loss: 0.6885, Val Loss: 0.6950\n",
      "[Epoch 65/80] Train Loss: 0.6880, Val Loss: 0.6963\n",
      "[Epoch 70/80] Train Loss: 0.6867, Val Loss: 0.6900\n",
      "[Epoch 75/80] Train Loss: 0.6864, Val Loss: 0.6894\n",
      "[Epoch 80/80] Train Loss: 0.6843, Val Loss: 0.6868\n",
      "Accuracy on Nonzero Predictions: 0.5573\n",
      "35\n",
      "Training AAPL | LR: 0.009883203468677031 | Epochs: 80 | Batch: 32 | Security type: single_name\n",
      "[Epoch 5/80] Train Loss: 0.6940, Val Loss: 0.6939\n",
      "[Epoch 10/80] Train Loss: 0.6936, Val Loss: 0.6927\n",
      "[Epoch 15/80] Train Loss: 0.6937, Val Loss: 0.6926\n",
      "[Epoch 20/80] Train Loss: 0.6929, Val Loss: 0.6914\n",
      "[Epoch 25/80] Train Loss: 0.6918, Val Loss: 0.6871\n",
      "[Epoch 30/80] Train Loss: 0.6914, Val Loss: 0.6901\n",
      "[Epoch 35/80] Train Loss: 0.6901, Val Loss: 0.6841\n",
      "[Epoch 40/80] Train Loss: 0.6893, Val Loss: 0.6884\n",
      "[Epoch 45/80] Train Loss: 0.6899, Val Loss: 0.6927\n",
      "[Epoch 50/80] Train Loss: 0.6870, Val Loss: 0.6893\n",
      "[Epoch 55/80] Train Loss: 0.6872, Val Loss: 0.6863\n",
      "[Epoch 60/80] Train Loss: 0.6855, Val Loss: 0.6792\n",
      "[Epoch 65/80] Train Loss: 0.6842, Val Loss: 0.6850\n",
      "[Epoch 70/80] Train Loss: 0.6814, Val Loss: 0.6882\n",
      "[Epoch 75/80] Train Loss: 0.6808, Val Loss: 0.6863\n",
      "[Epoch 80/80] Train Loss: 0.6772, Val Loss: 0.6918\n",
      "Accuracy on Nonzero Predictions: 0.5725\n",
      "36\n",
      "Training AAPL | LR: 0.009883203468677031 | Epochs: 80 | Batch: 32 | Security type: single_name\n",
      "[Epoch 5/80] Train Loss: 0.6939, Val Loss: 0.6920\n",
      "[Epoch 10/80] Train Loss: 0.6937, Val Loss: 0.6926\n",
      "[Epoch 15/80] Train Loss: 0.6937, Val Loss: 0.6929\n",
      "[Epoch 20/80] Train Loss: 0.6930, Val Loss: 0.6935\n",
      "[Epoch 25/80] Train Loss: 0.6929, Val Loss: 0.6901\n",
      "[Epoch 30/80] Train Loss: 0.6922, Val Loss: 0.6892\n",
      "[Epoch 35/80] Train Loss: 0.6929, Val Loss: 0.6876\n",
      "[Epoch 40/80] Train Loss: 0.6918, Val Loss: 0.6862\n",
      "[Epoch 45/80] Train Loss: 0.6897, Val Loss: 0.6836\n",
      "[Epoch 50/80] Train Loss: 0.6889, Val Loss: 0.6904\n",
      "[Epoch 55/80] Train Loss: 0.6848, Val Loss: 0.6848\n",
      "[Epoch 60/80] Train Loss: 0.6842, Val Loss: 0.6866\n",
      "[Epoch 65/80] Train Loss: 0.6832, Val Loss: 0.6898\n",
      "[Epoch 70/80] Train Loss: 0.6807, Val Loss: 0.6883\n",
      "[Epoch 75/80] Train Loss: 0.6811, Val Loss: 0.6893\n",
      "[Epoch 80/80] Train Loss: 0.6784, Val Loss: 0.6869\n",
      "Accuracy on Nonzero Predictions: 0.5954\n",
      "37\n",
      "Training AAPL | LR: 0.009883203468677031 | Epochs: 80 | Batch: 32 | Security type: single_name\n",
      "[Epoch 5/80] Train Loss: 0.6939, Val Loss: 0.6923\n",
      "[Epoch 10/80] Train Loss: 0.6937, Val Loss: 0.6927\n",
      "[Epoch 15/80] Train Loss: 0.6934, Val Loss: 0.6934\n",
      "[Epoch 20/80] Train Loss: 0.6933, Val Loss: 0.6908\n",
      "[Epoch 25/80] Train Loss: 0.6916, Val Loss: 0.6911\n",
      "[Epoch 30/80] Train Loss: 0.6912, Val Loss: 0.6944\n",
      "[Epoch 35/80] Train Loss: 0.6914, Val Loss: 0.6902\n",
      "[Epoch 40/80] Train Loss: 0.6920, Val Loss: 0.6878\n",
      "[Epoch 45/80] Train Loss: 0.6894, Val Loss: 0.6857\n",
      "[Epoch 50/80] Train Loss: 0.6871, Val Loss: 0.6835\n",
      "[Epoch 55/80] Train Loss: 0.6876, Val Loss: 0.6838\n",
      "[Epoch 60/80] Train Loss: 0.6863, Val Loss: 0.6840\n",
      "[Epoch 65/80] Train Loss: 0.6852, Val Loss: 0.6842\n",
      "[Epoch 70/80] Train Loss: 0.6828, Val Loss: 0.6836\n",
      "[Epoch 75/80] Train Loss: 0.6793, Val Loss: 0.6809\n",
      "[Epoch 80/80] Train Loss: 0.6808, Val Loss: 0.6852\n",
      "Accuracy on Nonzero Predictions: 0.5573\n",
      "38\n",
      "Training AAPL | LR: 0.009883203468677031 | Epochs: 80 | Batch: 32 | Security type: single_name\n",
      "[Epoch 5/80] Train Loss: 0.6940, Val Loss: 0.6943\n",
      "[Epoch 10/80] Train Loss: 0.6936, Val Loss: 0.6924\n",
      "[Epoch 15/80] Train Loss: 0.6936, Val Loss: 0.6922\n",
      "[Epoch 20/80] Train Loss: 0.6936, Val Loss: 0.6918\n",
      "[Epoch 25/80] Train Loss: 0.6934, Val Loss: 0.6923\n",
      "[Epoch 30/80] Train Loss: 0.6932, Val Loss: 0.6933\n",
      "[Epoch 35/80] Train Loss: 0.6927, Val Loss: 0.6898\n",
      "[Epoch 40/80] Train Loss: 0.6919, Val Loss: 0.6886\n",
      "[Epoch 45/80] Train Loss: 0.6911, Val Loss: 0.6875\n",
      "[Epoch 50/80] Train Loss: 0.6915, Val Loss: 0.6885\n",
      "[Epoch 55/80] Train Loss: 0.6891, Val Loss: 0.6892\n",
      "[Epoch 60/80] Train Loss: 0.6897, Val Loss: 0.6897\n",
      "[Epoch 65/80] Train Loss: 0.6883, Val Loss: 0.6856\n",
      "[Epoch 70/80] Train Loss: 0.6865, Val Loss: 0.6856\n",
      "[Epoch 75/80] Train Loss: 0.6868, Val Loss: 0.6896\n",
      "[Epoch 80/80] Train Loss: 0.6855, Val Loss: 0.6860\n",
      "Accuracy on Nonzero Predictions: 0.5420\n",
      "39\n",
      "Training AAPL | LR: 0.009883203468677031 | Epochs: 80 | Batch: 32 | Security type: single_name\n",
      "[Epoch 5/80] Train Loss: 0.6941, Val Loss: 0.6940\n",
      "[Epoch 10/80] Train Loss: 0.6936, Val Loss: 0.6922\n",
      "[Epoch 15/80] Train Loss: 0.6936, Val Loss: 0.6920\n",
      "[Epoch 20/80] Train Loss: 0.6936, Val Loss: 0.6921\n",
      "[Epoch 25/80] Train Loss: 0.6935, Val Loss: 0.6907\n",
      "[Epoch 30/80] Train Loss: 0.6936, Val Loss: 0.6922\n",
      "[Epoch 35/80] Train Loss: 0.6937, Val Loss: 0.6920\n",
      "[Epoch 40/80] Train Loss: 0.6936, Val Loss: 0.6925\n",
      "[Epoch 45/80] Train Loss: 0.6936, Val Loss: 0.6921\n",
      "[Epoch 50/80] Train Loss: 0.6933, Val Loss: 0.6915\n",
      "[Epoch 55/80] Train Loss: 0.6930, Val Loss: 0.6921\n",
      "[Epoch 60/80] Train Loss: 0.6929, Val Loss: 0.6912\n",
      "[Epoch 65/80] Train Loss: 0.6931, Val Loss: 0.6908\n",
      "[Epoch 70/80] Train Loss: 0.6911, Val Loss: 0.6872\n",
      "[Epoch 75/80] Train Loss: 0.6893, Val Loss: 0.6863\n",
      "[Epoch 80/80] Train Loss: 0.6898, Val Loss: 0.6826\n",
      "Accuracy on Nonzero Predictions: 0.5649\n",
      "40\n",
      "Training AAPL | LR: 0.009883203468677031 | Epochs: 80 | Batch: 32 | Security type: single_name\n",
      "[Epoch 5/80] Train Loss: 0.6940, Val Loss: 0.6935\n",
      "[Epoch 10/80] Train Loss: 0.6937, Val Loss: 0.6925\n",
      "[Epoch 15/80] Train Loss: 0.6936, Val Loss: 0.6922\n",
      "[Epoch 20/80] Train Loss: 0.6936, Val Loss: 0.6923\n",
      "[Epoch 25/80] Train Loss: 0.6933, Val Loss: 0.6934\n",
      "[Epoch 30/80] Train Loss: 0.6927, Val Loss: 0.6910\n",
      "[Epoch 35/80] Train Loss: 0.6926, Val Loss: 0.6922\n",
      "[Epoch 40/80] Train Loss: 0.6920, Val Loss: 0.6911\n",
      "[Epoch 45/80] Train Loss: 0.6909, Val Loss: 0.6919\n",
      "[Epoch 50/80] Train Loss: 0.6906, Val Loss: 0.6935\n",
      "[Epoch 55/80] Train Loss: 0.6891, Val Loss: 0.6953\n",
      "[Epoch 60/80] Train Loss: 0.6897, Val Loss: 0.6925\n",
      "[Epoch 65/80] Train Loss: 0.6882, Val Loss: 0.6925\n",
      "[Epoch 70/80] Train Loss: 0.6882, Val Loss: 0.6904\n",
      "[Epoch 75/80] Train Loss: 0.6850, Val Loss: 0.6948\n",
      "[Epoch 80/80] Train Loss: 0.6832, Val Loss: 0.6928\n",
      "Accuracy on Nonzero Predictions: 0.5573\n",
      "41\n",
      "Training AAPL | LR: 0.009883203468677031 | Epochs: 80 | Batch: 32 | Security type: single_name\n",
      "[Epoch 5/80] Train Loss: 0.6939, Val Loss: 0.6940\n",
      "[Epoch 10/80] Train Loss: 0.6936, Val Loss: 0.6925\n",
      "[Epoch 15/80] Train Loss: 0.6936, Val Loss: 0.6925\n",
      "[Epoch 20/80] Train Loss: 0.6936, Val Loss: 0.6925\n",
      "[Epoch 25/80] Train Loss: 0.6936, Val Loss: 0.6926\n",
      "[Epoch 30/80] Train Loss: 0.6934, Val Loss: 0.6922\n",
      "[Epoch 35/80] Train Loss: 0.6934, Val Loss: 0.6920\n",
      "[Epoch 40/80] Train Loss: 0.6930, Val Loss: 0.6923\n",
      "[Epoch 45/80] Train Loss: 0.6931, Val Loss: 0.6923\n",
      "[Epoch 50/80] Train Loss: 0.6919, Val Loss: 0.6912\n",
      "[Epoch 55/80] Train Loss: 0.6905, Val Loss: 0.6910\n",
      "[Epoch 60/80] Train Loss: 0.6886, Val Loss: 0.6859\n",
      "[Epoch 65/80] Train Loss: 0.6895, Val Loss: 0.6843\n",
      "[Epoch 70/80] Train Loss: 0.6893, Val Loss: 0.6872\n",
      "[Epoch 75/80] Train Loss: 0.6882, Val Loss: 0.6821\n",
      "[Epoch 80/80] Train Loss: 0.6887, Val Loss: 0.6851\n",
      "Accuracy on Nonzero Predictions: 0.4656\n",
      "42\n",
      "Training AAPL | LR: 0.009883203468677031 | Epochs: 80 | Batch: 32 | Security type: single_name\n",
      "[Epoch 5/80] Train Loss: 0.6938, Val Loss: 0.6959\n",
      "[Epoch 10/80] Train Loss: 0.6936, Val Loss: 0.6926\n",
      "[Epoch 15/80] Train Loss: 0.6936, Val Loss: 0.6923\n",
      "[Epoch 20/80] Train Loss: 0.6937, Val Loss: 0.6922\n",
      "[Epoch 25/80] Train Loss: 0.6935, Val Loss: 0.6922\n",
      "[Epoch 30/80] Train Loss: 0.6928, Val Loss: 0.6919\n",
      "[Epoch 35/80] Train Loss: 0.6924, Val Loss: 0.6915\n",
      "[Epoch 40/80] Train Loss: 0.6908, Val Loss: 0.6956\n",
      "[Epoch 45/80] Train Loss: 0.6906, Val Loss: 0.7012\n",
      "[Epoch 50/80] Train Loss: 0.6885, Val Loss: 0.7000\n",
      "[Epoch 55/80] Train Loss: 0.6860, Val Loss: 0.7106\n",
      "[Epoch 60/80] Train Loss: 0.6863, Val Loss: 0.7054\n",
      "[Epoch 65/80] Train Loss: 0.6862, Val Loss: 0.7067\n",
      "[Epoch 70/80] Train Loss: 0.6828, Val Loss: 0.7047\n",
      "[Epoch 75/80] Train Loss: 0.6849, Val Loss: 0.7041\n",
      "[Epoch 80/80] Train Loss: 0.6822, Val Loss: 0.7000\n",
      "Accuracy on Nonzero Predictions: 0.4046\n",
      "43\n",
      "Training AAPL | LR: 0.009883203468677031 | Epochs: 80 | Batch: 32 | Security type: single_name\n",
      "[Epoch 5/80] Train Loss: 0.6940, Val Loss: 0.6928\n",
      "[Epoch 10/80] Train Loss: 0.6936, Val Loss: 0.6923\n",
      "[Epoch 15/80] Train Loss: 0.6936, Val Loss: 0.6924\n",
      "[Epoch 20/80] Train Loss: 0.6936, Val Loss: 0.6923\n",
      "[Epoch 25/80] Train Loss: 0.6936, Val Loss: 0.6923\n",
      "[Epoch 30/80] Train Loss: 0.6936, Val Loss: 0.6923\n",
      "[Epoch 35/80] Train Loss: 0.6936, Val Loss: 0.6923\n",
      "[Epoch 40/80] Train Loss: 0.6936, Val Loss: 0.6923\n",
      "[Epoch 45/80] Train Loss: 0.6936, Val Loss: 0.6923\n",
      "[Epoch 50/80] Train Loss: 0.6936, Val Loss: 0.6922\n",
      "[Epoch 55/80] Train Loss: 0.6936, Val Loss: 0.6923\n",
      "[Epoch 60/80] Train Loss: 0.6936, Val Loss: 0.6923\n",
      "[Epoch 65/80] Train Loss: 0.6936, Val Loss: 0.6923\n",
      "[Epoch 70/80] Train Loss: 0.6936, Val Loss: 0.6923\n",
      "[Epoch 75/80] Train Loss: 0.6933, Val Loss: 0.6920\n",
      "[Epoch 80/80] Train Loss: 0.6936, Val Loss: 0.6923\n",
      "Accuracy on Nonzero Predictions: 0.5878\n",
      "44\n",
      "Training AAPL | LR: 0.009883203468677031 | Epochs: 80 | Batch: 32 | Security type: single_name\n",
      "[Epoch 5/80] Train Loss: 0.6939, Val Loss: 0.6933\n",
      "[Epoch 10/80] Train Loss: 0.6937, Val Loss: 0.6924\n",
      "[Epoch 15/80] Train Loss: 0.6935, Val Loss: 0.6922\n",
      "[Epoch 20/80] Train Loss: 0.6935, Val Loss: 0.6921\n",
      "[Epoch 25/80] Train Loss: 0.6929, Val Loss: 0.6928\n",
      "[Epoch 30/80] Train Loss: 0.6925, Val Loss: 0.6915\n",
      "[Epoch 35/80] Train Loss: 0.6919, Val Loss: 0.6929\n",
      "[Epoch 40/80] Train Loss: 0.6907, Val Loss: 0.6843\n",
      "[Epoch 45/80] Train Loss: 0.6896, Val Loss: 0.6881\n",
      "[Epoch 50/80] Train Loss: 0.6893, Val Loss: 0.6796\n",
      "[Epoch 55/80] Train Loss: 0.6869, Val Loss: 0.6812\n",
      "[Epoch 60/80] Train Loss: 0.6855, Val Loss: 0.6806\n",
      "[Epoch 65/80] Train Loss: 0.6872, Val Loss: 0.6785\n",
      "[Epoch 70/80] Train Loss: 0.6852, Val Loss: 0.6783\n",
      "[Epoch 75/80] Train Loss: 0.6825, Val Loss: 0.6790\n",
      "[Epoch 80/80] Train Loss: 0.6837, Val Loss: 0.6721\n",
      "Accuracy on Nonzero Predictions: 0.5802\n",
      "45\n",
      "Training AAPL | LR: 0.009883203468677031 | Epochs: 80 | Batch: 32 | Security type: single_name\n",
      "[Epoch 5/80] Train Loss: 0.6938, Val Loss: 0.6938\n",
      "[Epoch 10/80] Train Loss: 0.6936, Val Loss: 0.6924\n",
      "[Epoch 15/80] Train Loss: 0.6936, Val Loss: 0.6924\n",
      "[Epoch 20/80] Train Loss: 0.6936, Val Loss: 0.6923\n",
      "[Epoch 25/80] Train Loss: 0.6936, Val Loss: 0.6923\n",
      "[Epoch 30/80] Train Loss: 0.6936, Val Loss: 0.6923\n",
      "[Epoch 35/80] Train Loss: 0.6936, Val Loss: 0.6923\n",
      "[Epoch 40/80] Train Loss: 0.6933, Val Loss: 0.6924\n",
      "[Epoch 45/80] Train Loss: 0.6923, Val Loss: 0.6921\n",
      "[Epoch 50/80] Train Loss: 0.6918, Val Loss: 0.6912\n",
      "[Epoch 55/80] Train Loss: 0.6916, Val Loss: 0.6867\n",
      "[Epoch 60/80] Train Loss: 0.6909, Val Loss: 0.6889\n",
      "[Epoch 65/80] Train Loss: 0.6907, Val Loss: 0.6851\n",
      "[Epoch 70/80] Train Loss: 0.6898, Val Loss: 0.6837\n",
      "[Epoch 75/80] Train Loss: 0.6900, Val Loss: 0.6840\n",
      "[Epoch 80/80] Train Loss: 0.6888, Val Loss: 0.6813\n",
      "Accuracy on Nonzero Predictions: 0.4427\n",
      "46\n",
      "Training AAPL | LR: 0.009883203468677031 | Epochs: 80 | Batch: 32 | Security type: single_name\n",
      "[Epoch 5/80] Train Loss: 0.6939, Val Loss: 0.6927\n",
      "[Epoch 10/80] Train Loss: 0.6936, Val Loss: 0.6923\n",
      "[Epoch 15/80] Train Loss: 0.6936, Val Loss: 0.6924\n",
      "[Epoch 20/80] Train Loss: 0.6936, Val Loss: 0.6923\n",
      "[Epoch 25/80] Train Loss: 0.6936, Val Loss: 0.6923\n",
      "[Epoch 30/80] Train Loss: 0.6936, Val Loss: 0.6923\n",
      "[Epoch 35/80] Train Loss: 0.6936, Val Loss: 0.6923\n",
      "[Epoch 40/80] Train Loss: 0.6936, Val Loss: 0.6923\n",
      "[Epoch 45/80] Train Loss: 0.6936, Val Loss: 0.6923\n",
      "[Epoch 50/80] Train Loss: 0.6937, Val Loss: 0.6925\n",
      "[Epoch 55/80] Train Loss: 0.6937, Val Loss: 0.6931\n",
      "[Epoch 60/80] Train Loss: 0.6936, Val Loss: 0.6918\n",
      "[Epoch 65/80] Train Loss: 0.6929, Val Loss: 0.6903\n",
      "[Epoch 70/80] Train Loss: 0.6926, Val Loss: 0.6916\n",
      "[Epoch 75/80] Train Loss: 0.6926, Val Loss: 0.6898\n",
      "[Epoch 80/80] Train Loss: 0.6911, Val Loss: 0.6909\n",
      "Accuracy on Nonzero Predictions: 0.4885\n",
      "47\n",
      "Training AAPL | LR: 0.009883203468677031 | Epochs: 80 | Batch: 32 | Security type: single_name\n",
      "[Epoch 5/80] Train Loss: 0.6940, Val Loss: 0.6931\n",
      "[Epoch 10/80] Train Loss: 0.6936, Val Loss: 0.6924\n",
      "[Epoch 15/80] Train Loss: 0.6936, Val Loss: 0.6923\n",
      "[Epoch 20/80] Train Loss: 0.6937, Val Loss: 0.6930\n",
      "[Epoch 25/80] Train Loss: 0.6928, Val Loss: 0.6903\n",
      "[Epoch 30/80] Train Loss: 0.6923, Val Loss: 0.6909\n",
      "[Epoch 35/80] Train Loss: 0.6918, Val Loss: 0.6889\n",
      "[Epoch 40/80] Train Loss: 0.6894, Val Loss: 0.6878\n",
      "[Epoch 45/80] Train Loss: 0.6901, Val Loss: 0.6878\n",
      "[Epoch 50/80] Train Loss: 0.6890, Val Loss: 0.6864\n",
      "[Epoch 55/80] Train Loss: 0.6873, Val Loss: 0.6864\n",
      "[Epoch 60/80] Train Loss: 0.6865, Val Loss: 0.6881\n",
      "[Epoch 65/80] Train Loss: 0.6850, Val Loss: 0.6889\n",
      "[Epoch 70/80] Train Loss: 0.6846, Val Loss: 0.6871\n",
      "[Epoch 75/80] Train Loss: 0.6797, Val Loss: 0.6852\n",
      "[Epoch 80/80] Train Loss: 0.6819, Val Loss: 0.6899\n",
      "Accuracy on Nonzero Predictions: 0.5802\n",
      "48\n",
      "Training AAPL | LR: 0.009883203468677031 | Epochs: 80 | Batch: 32 | Security type: single_name\n",
      "[Epoch 5/80] Train Loss: 0.6938, Val Loss: 0.6929\n",
      "[Epoch 10/80] Train Loss: 0.6936, Val Loss: 0.6925\n",
      "[Epoch 15/80] Train Loss: 0.6936, Val Loss: 0.6924\n",
      "[Epoch 20/80] Train Loss: 0.6936, Val Loss: 0.6924\n",
      "[Epoch 25/80] Train Loss: 0.6936, Val Loss: 0.6923\n",
      "[Epoch 30/80] Train Loss: 0.6936, Val Loss: 0.6923\n",
      "[Epoch 35/80] Train Loss: 0.6936, Val Loss: 0.6923\n",
      "[Epoch 40/80] Train Loss: 0.6936, Val Loss: 0.6923\n",
      "[Epoch 45/80] Train Loss: 0.6936, Val Loss: 0.6924\n",
      "[Epoch 50/80] Train Loss: 0.6936, Val Loss: 0.6923\n",
      "[Epoch 55/80] Train Loss: 0.6936, Val Loss: 0.6923\n",
      "[Epoch 60/80] Train Loss: 0.6939, Val Loss: 0.6922\n",
      "[Epoch 65/80] Train Loss: 0.6944, Val Loss: 0.6923\n",
      "[Epoch 70/80] Train Loss: 0.6935, Val Loss: 0.6901\n",
      "[Epoch 75/80] Train Loss: 0.6923, Val Loss: 0.6879\n",
      "[Epoch 80/80] Train Loss: 0.6918, Val Loss: 0.6885\n",
      "Accuracy on Nonzero Predictions: 0.4809\n",
      "49\n",
      "Training AAPL | LR: 0.009883203468677031 | Epochs: 80 | Batch: 32 | Security type: single_name\n",
      "[Epoch 5/80] Train Loss: 0.6939, Val Loss: 0.6939\n",
      "[Epoch 10/80] Train Loss: 0.6936, Val Loss: 0.6924\n",
      "[Epoch 15/80] Train Loss: 0.6936, Val Loss: 0.6925\n",
      "[Epoch 20/80] Train Loss: 0.6933, Val Loss: 0.6919\n",
      "[Epoch 25/80] Train Loss: 0.6930, Val Loss: 0.6879\n",
      "[Epoch 30/80] Train Loss: 0.6909, Val Loss: 0.6901\n",
      "[Epoch 35/80] Train Loss: 0.6909, Val Loss: 0.6842\n",
      "[Epoch 40/80] Train Loss: 0.6890, Val Loss: 0.6845\n",
      "[Epoch 45/80] Train Loss: 0.6883, Val Loss: 0.6910\n",
      "[Epoch 50/80] Train Loss: 0.6876, Val Loss: 0.6945\n",
      "[Epoch 55/80] Train Loss: 0.6867, Val Loss: 0.6910\n",
      "[Epoch 60/80] Train Loss: 0.6869, Val Loss: 0.6971\n",
      "[Epoch 65/80] Train Loss: 0.6851, Val Loss: 0.6945\n",
      "[Epoch 70/80] Train Loss: 0.6846, Val Loss: 0.6958\n",
      "[Epoch 75/80] Train Loss: 0.6801, Val Loss: 0.6964\n",
      "[Epoch 80/80] Train Loss: 0.6813, Val Loss: 0.7064\n",
      "Accuracy on Nonzero Predictions: 0.5344\n"
     ]
    }
   ],
   "source": [
    "results_list = []\n",
    "stock = \"AAPL\"\n",
    "period = \"10y\"\n",
    "possible_train_size = 95\n",
    "batch_size = 32\n",
    "num_epochs = 80\n",
    "window_size = 3\n",
    "\n",
    "prediction_threshold = 0.5\n",
    "\n",
    "output_folder = os.path.join(results_dir, f\"individual_trials\") \n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "processing = \"clean\"\n",
    "security_types = [\"single_name\"]\n",
    "learning_rate = 0.009883203468677031\n",
    "model_type = \"gru\"\n",
    "\n",
    "results_list = []\n",
    "for security_type in security_types:\n",
    "    initial_data_dir = os.path.join(project_dir, f\"00_data/{processing}\") \n",
    "    \n",
    "    # 1) Load original data (info only)\n",
    "    filename = f\"{security_type}/{stock}/{period}_data.csv\"\n",
    "    original_input_filepath = os.path.join(initial_data_dir, filename)\n",
    "    original_data = pd.read_csv(original_input_filepath)\n",
    "\n",
    "    print(f\"\\n----- LEARNING_RATE: {learning_rate}, SECURITY_TYPE: {security_type}, MODEL_TYPE: {model_type} -----\")\n",
    "\n",
    "    # 2) Load the preprocessed data\n",
    "    pkl_filename = f\"{processing}/{security_type}/{stock}/{period}_{window_size}_data.pkl\"\n",
    "    input_filepath = os.path.join(horizontal_data_dir, pkl_filename)\n",
    "    input_df = pd.read_pickle(input_filepath)\n",
    "    \n",
    "    # 3) Reshape\n",
    "    X_resampled, y_resampled = reshape_remove_characters(input_df)\n",
    "\n",
    "    input_size = X_resampled.shape[2]\n",
    "    train_size = int(X_resampled.shape[0] * possible_train_size / 100)\n",
    "    test_size = X_resampled.shape[0] - train_size\n",
    "\n",
    "    for i in range(50):\n",
    "        print(i)\n",
    "        \n",
    "        # for model_type in model_types:\n",
    "            # 4) Initialize the model\n",
    "            # if model_type == \"gru\":\n",
    "            #     \n",
    "            # elif model_type == \"lstm\":\n",
    "        # model = StockPriceLSTM(input_size, hidden_size, output_size)\n",
    "        model = GRU3DClassifier(input_size, hidden_size, output_size, num_layers, dropout)\n",
    "        model = model.to(device)\n",
    "\n",
    "        # 5) Set up optimizer\n",
    "        optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "        print(f\"Training {stock} | LR: {learning_rate} | Epochs: {num_epochs} \"\n",
    "              f\"| Batch: {batch_size} | Security type: {security_type}\")\n",
    "\n",
    "        result = evaluate_rolling_unchanged_model_threshold(\n",
    "            model=model,\n",
    "            X=X_resampled,\n",
    "            y=y_resampled,\n",
    "            criterion=criterion,\n",
    "            optimizer=optimizer,\n",
    "            device=device,\n",
    "            train_size=train_size,\n",
    "            batch_size=batch_size,\n",
    "            num_epochs=num_epochs,\n",
    "            lower_threshold=0.5\n",
    "        )\n",
    "\n",
    "        # 7) Extract results\n",
    "        rolling_predictions = result[\"rolling_predictions\"]\n",
    "        rolling_targets = result[\"rolling_targets\"]\n",
    "        test_accuracy = result[\"accuracy_nonzero\"]\n",
    "        loss_decrease_percentage = result[\"loss_decrease_percentage\"]\n",
    "        nonzero_preds = np.count_nonzero(rolling_predictions)\n",
    "        final_train_loss = result[\"final_train_loss\"]\n",
    "\n",
    "        # 9) Create a record (dictionary) for this run\n",
    "        run_record = {\n",
    "            \"STOCK\": stock,\n",
    "            \"DATA_TYPE\": security_type,\n",
    "            \"MODEL\": model_type.upper(),\n",
    "            \"PROCESSING\": processing,\n",
    "            \"ACCURACY\": test_accuracy,\n",
    "            \"TRAIN_PCT_DECREASE\": loss_decrease_percentage,\n",
    "            \"FINAL_TRAIN_LOSS\": final_train_loss\n",
    "        }\n",
    "\n",
    "        # 10) Append to the results_list\n",
    "        results_list.append(run_record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fb0b4846-38a7-4d32-8a3d-7832033a6ad9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>STOCK</th>\n",
       "      <th>DATA_TYPE</th>\n",
       "      <th>MODEL</th>\n",
       "      <th>PROCESSING</th>\n",
       "      <th>ACCURACY</th>\n",
       "      <th>TRAIN_PCT_DECREASE</th>\n",
       "      <th>FINAL_TRAIN_LOSS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>options</td>\n",
       "      <td>LSTM</td>\n",
       "      <td>pca</td>\n",
       "      <td>0.511450</td>\n",
       "      <td>-93.096637</td>\n",
       "      <td>0.048020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>options</td>\n",
       "      <td>LSTM</td>\n",
       "      <td>pca</td>\n",
       "      <td>0.511450</td>\n",
       "      <td>-93.591413</td>\n",
       "      <td>0.044553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>options</td>\n",
       "      <td>LSTM</td>\n",
       "      <td>pca</td>\n",
       "      <td>0.473282</td>\n",
       "      <td>-97.815809</td>\n",
       "      <td>0.015188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>options</td>\n",
       "      <td>LSTM</td>\n",
       "      <td>pca</td>\n",
       "      <td>0.480916</td>\n",
       "      <td>-96.266795</td>\n",
       "      <td>0.025945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>options</td>\n",
       "      <td>LSTM</td>\n",
       "      <td>pca</td>\n",
       "      <td>0.603053</td>\n",
       "      <td>-96.150294</td>\n",
       "      <td>0.026757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>technical</td>\n",
       "      <td>LSTM</td>\n",
       "      <td>pca</td>\n",
       "      <td>0.579365</td>\n",
       "      <td>-37.621747</td>\n",
       "      <td>0.432957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>technical</td>\n",
       "      <td>LSTM</td>\n",
       "      <td>pca</td>\n",
       "      <td>0.515873</td>\n",
       "      <td>-83.982725</td>\n",
       "      <td>0.111194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>technical</td>\n",
       "      <td>LSTM</td>\n",
       "      <td>pca</td>\n",
       "      <td>0.420635</td>\n",
       "      <td>-30.648799</td>\n",
       "      <td>0.481398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>technical</td>\n",
       "      <td>LSTM</td>\n",
       "      <td>pca</td>\n",
       "      <td>0.547619</td>\n",
       "      <td>-80.392040</td>\n",
       "      <td>0.136304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>technical</td>\n",
       "      <td>LSTM</td>\n",
       "      <td>pca</td>\n",
       "      <td>0.484127</td>\n",
       "      <td>-83.581409</td>\n",
       "      <td>0.114011</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   STOCK  DATA_TYPE MODEL PROCESSING  ACCURACY  TRAIN_PCT_DECREASE  \\\n",
       "0   AAPL    options  LSTM        pca  0.511450          -93.096637   \n",
       "1   AAPL    options  LSTM        pca  0.511450          -93.591413   \n",
       "2   AAPL    options  LSTM        pca  0.473282          -97.815809   \n",
       "3   AAPL    options  LSTM        pca  0.480916          -96.266795   \n",
       "4   AAPL    options  LSTM        pca  0.603053          -96.150294   \n",
       "..   ...        ...   ...        ...       ...                 ...   \n",
       "95  AAPL  technical  LSTM        pca  0.579365          -37.621747   \n",
       "96  AAPL  technical  LSTM        pca  0.515873          -83.982725   \n",
       "97  AAPL  technical  LSTM        pca  0.420635          -30.648799   \n",
       "98  AAPL  technical  LSTM        pca  0.547619          -80.392040   \n",
       "99  AAPL  technical  LSTM        pca  0.484127          -83.581409   \n",
       "\n",
       "    FINAL_TRAIN_LOSS  \n",
       "0           0.048020  \n",
       "1           0.044553  \n",
       "2           0.015188  \n",
       "3           0.025945  \n",
       "4           0.026757  \n",
       "..               ...  \n",
       "95          0.432957  \n",
       "96          0.111194  \n",
       "97          0.481398  \n",
       "98          0.136304  \n",
       "99          0.114011  \n",
       "\n",
       "[100 rows x 7 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(results_list)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8623f683-030f-4649-8050-9ad1641a18ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_folder = os.path.join(results_dir, f\"individual_trials\") \n",
    "df.to_csv(os.path.join(output_folder, f\"lstm_pca_accuracy_bydatatype.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ffd110a6-55ab-4d4a-8df3-61bc533d60c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_stats = df.groupby([\"MODEL\", \"DATA_TYPE\"])[\"ACCURACY\"].agg([\"mean\", \"max\", \"min\", \"std\"]).reset_index()\n",
    "summary_stats.to_csv(os.path.join(output_folder, f\"lstm_pca_bydatatype_summarystats.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "7b7dac04-d30e-414b-bf74-b1928cd4b7d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/jupyter-tfg2425paula/prediction_project_v3/02_results/individual_trials'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_folder = os.path.join(results_dir, 'individual_trials')\n",
    "output_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a53216d9-1202-440c-a272-ffb244ed33a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>STOCK</th>\n",
       "      <th>DATA_TYPE</th>\n",
       "      <th>MODEL</th>\n",
       "      <th>PROCESSING</th>\n",
       "      <th>ACCURACY</th>\n",
       "      <th>TRAIN_PCT_DECREASE</th>\n",
       "      <th>FINAL_TRAIN_LOSS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>single_name</td>\n",
       "      <td>GRU</td>\n",
       "      <td>clean</td>\n",
       "      <td>0.564885</td>\n",
       "      <td>-2.183248</td>\n",
       "      <td>0.680376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>single_name</td>\n",
       "      <td>GRU</td>\n",
       "      <td>clean</td>\n",
       "      <td>0.595420</td>\n",
       "      <td>-2.057334</td>\n",
       "      <td>0.681497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>single_name</td>\n",
       "      <td>GRU</td>\n",
       "      <td>clean</td>\n",
       "      <td>0.587786</td>\n",
       "      <td>-1.778138</td>\n",
       "      <td>0.683358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>single_name</td>\n",
       "      <td>GRU</td>\n",
       "      <td>clean</td>\n",
       "      <td>0.564885</td>\n",
       "      <td>-2.430913</td>\n",
       "      <td>0.678785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>single_name</td>\n",
       "      <td>GRU</td>\n",
       "      <td>clean</td>\n",
       "      <td>0.541985</td>\n",
       "      <td>-2.174941</td>\n",
       "      <td>0.680178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>technical</td>\n",
       "      <td>GRU</td>\n",
       "      <td>clean</td>\n",
       "      <td>0.468254</td>\n",
       "      <td>-78.074882</td>\n",
       "      <td>0.153156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>technical</td>\n",
       "      <td>GRU</td>\n",
       "      <td>clean</td>\n",
       "      <td>0.515873</td>\n",
       "      <td>-64.541400</td>\n",
       "      <td>0.246790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>technical</td>\n",
       "      <td>GRU</td>\n",
       "      <td>clean</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>-62.280494</td>\n",
       "      <td>0.263006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>technical</td>\n",
       "      <td>GRU</td>\n",
       "      <td>clean</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>-68.898970</td>\n",
       "      <td>0.216749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>technical</td>\n",
       "      <td>GRU</td>\n",
       "      <td>clean</td>\n",
       "      <td>0.492063</td>\n",
       "      <td>-30.221871</td>\n",
       "      <td>0.487812</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>300 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    STOCK    DATA_TYPE MODEL PROCESSING  ACCURACY  TRAIN_PCT_DECREASE  \\\n",
       "0    AAPL  single_name   GRU      clean  0.564885           -2.183248   \n",
       "1    AAPL  single_name   GRU      clean  0.595420           -2.057334   \n",
       "2    AAPL  single_name   GRU      clean  0.587786           -1.778138   \n",
       "3    AAPL  single_name   GRU      clean  0.564885           -2.430913   \n",
       "4    AAPL  single_name   GRU      clean  0.541985           -2.174941   \n",
       "..    ...          ...   ...        ...       ...                 ...   \n",
       "295  AAPL    technical   GRU      clean  0.468254          -78.074882   \n",
       "296  AAPL    technical   GRU      clean  0.515873          -64.541400   \n",
       "297  AAPL    technical   GRU      clean  0.500000          -62.280494   \n",
       "298  AAPL    technical   GRU      clean  0.500000          -68.898970   \n",
       "299  AAPL    technical   GRU      clean  0.492063          -30.221871   \n",
       "\n",
       "     FINAL_TRAIN_LOSS  \n",
       "0            0.680376  \n",
       "1            0.681497  \n",
       "2            0.683358  \n",
       "3            0.678785  \n",
       "4            0.680178  \n",
       "..                ...  \n",
       "295          0.153156  \n",
       "296          0.246790  \n",
       "297          0.263006  \n",
       "298          0.216749  \n",
       "299          0.487812  \n",
       "\n",
       "[300 rows x 7 columns]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_csv_path = \n",
    "\n",
    "if len(results_list) > 0:\n",
    "    df = pd.DataFrame(results_list)\n",
    "\n",
    "    if os.path.exists(results_csv_path):\n",
    "        # Append without header\n",
    "        df.to_csv(results_csv_path, mode='a', header=False, index=False)\n",
    "    else:\n",
    "        # Write new file with header\n",
    "        df.to_csv(results_csv_path, index=False)\n",
    "\n",
    "    # Clear the list before next iteration\n",
    "    results_list = []"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
