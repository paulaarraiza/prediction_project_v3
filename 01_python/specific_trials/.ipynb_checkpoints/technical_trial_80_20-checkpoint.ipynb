{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64e5407f-c4e7-47d4-9958-8edea3392661",
   "metadata": {},
   "source": [
    "## **This notebook aims to combine multiple assets databases**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e46c228-21f4-4e80-a4e8-b62981ab4799",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.metrics import accuracy_score\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "63a7fdf4-b5ef-45ba-9d5c-cf41578fbd20",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"  # Use GPU 0 in first notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bf032534-2a6b-4f78-b6db-06262620ad55",
   "metadata": {},
   "outputs": [],
   "source": [
    "project_dir = \"/home/jupyter-tfg2425paula/prediction_project_v3\"\n",
    "os.chdir(project_dir)\n",
    "\n",
    "clean_data_dir = os.path.join(project_dir, \"00_data/clean\")\n",
    "horizontal_data_dir = os.path.join(project_dir, \"00_data/horizontal_structure\")\n",
    "results_dir = os.path.join(project_dir, \"02_results\")\n",
    "plots_dir = os.path.join(project_dir, \"03_plots\")\n",
    "pca_data_dir = os.path.join(project_dir, \"00_data/pca\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a34d7d2f-94f1-4a7e-a36a-65b386cd2711",
   "metadata": {},
   "source": [
    "### **GRU Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6be503cf-ab00-40f9-8287-6e871eb53021",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRU3DClassifier(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers, dropout):\n",
    "        super(GRU3DClassifier, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "\n",
    "        out, _ = self.gru(x, h0)\n",
    "        out = self.fc(out[:, -1, :]) \n",
    "        # return self.sigmoid(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e169a1d-ac5f-4796-8b5a-4447f1612a1c",
   "metadata": {},
   "source": [
    "### **LSTM Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c420d354-a19b-453e-9de3-78a238bf5920",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StockPriceLSTM(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers=1, dropout=0.0):\n",
    "        super(StockPriceLSTM, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers=num_layers, \n",
    "                            batch_first=True, dropout=dropout)\n",
    "    \n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)  # Get the batch size dynamically\n",
    "\n",
    "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_dim).to(x.device)  # (num_layers, batch_size, hidden_dim)\n",
    "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_dim).to(x.device)  # (num_layers, batch_size, hidden_dim)\n",
    "        \n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "\n",
    "        out = self.fc(out[:, -1, :]) \n",
    "        # out = self.sigmoid(out)\n",
    "        return out\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a8ade47-ae99-4802-a9be-59473a8a6598",
   "metadata": {},
   "source": [
    "### **Set folders**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fb30d8df-ff8b-48ef-a786-4385d72eed1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds = [0.3, 0.35, 0.4, 0.45, 0.5]\n",
    "thresholds = [0.5]\n",
    "learning_rates = [0.005, 0.008, 0.009, 0.01]\n",
    "learning_rates = [0.01]\n",
    "num_epochs_list = [100, 200]\n",
    "num_epochs_list = [100]\n",
    "batch_sizes = [16, 32]\n",
    "batch_sizes = [32]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7176635e-f447-4149-bab2-e997a4728295",
   "metadata": {},
   "source": [
    "#### **Model and Hyperparameters**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "273f20be-b345-4786-a715-dcf8fbcde47a",
   "metadata": {},
   "source": [
    "#### **Last data modifications**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e43ae2c5-e4c6-4be8-b82a-73c02d66705b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_remove_characters(df):\n",
    "\n",
    "    X = np.array([np.stack(row) for row in df.drop(columns=['Target']).values])\n",
    "    y = df['Target'].values\n",
    "\n",
    "    smote = SMOTE(random_state=42)\n",
    "    n_samples, timesteps, n_features = X.shape\n",
    "    X_flat = X.reshape((n_samples, timesteps * n_features))\n",
    "    X_flat = np.where(X_flat == 'ç', 0, X_flat)\n",
    "\n",
    "    X_resampled = X_flat.reshape((-1, timesteps, n_features))\n",
    "    \n",
    "    return X_resampled, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ddf708b-f9db-4ffc-88dd-06723ad78d3c",
   "metadata": {},
   "source": [
    "### **Evaluation function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bcba527d-6d55-4e90-9c55-fc058f338f90",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def evaluate_model_plot(\n",
    "    model, \n",
    "    X, \n",
    "    y, \n",
    "    criterion, \n",
    "    optimizer, \n",
    "    device, \n",
    "    train_size, \n",
    "    batch_size, \n",
    "    num_epochs, \n",
    "    lower_threshold,\n",
    "    plots_dir=None,\n",
    "    plot_filename=None\n",
    "):\n",
    "\n",
    "    # -------------------------------\n",
    "    # 0) Prepare Tensors & Splits\n",
    "    # -------------------------------\n",
    "    X = torch.tensor(X, dtype=torch.float32)\n",
    "    y = torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "    total_size = len(X)\n",
    "    # Determine actual train_size index\n",
    "    if train_size < 1.0:\n",
    "        lower_bound = int(train_size * total_size)\n",
    "    else:\n",
    "        lower_bound = train_size\n",
    "\n",
    "    # Training portion\n",
    "    X_train = X[:lower_bound].to(device)\n",
    "    y_train = y[:lower_bound].to(device)\n",
    "\n",
    "    train_dataset = TensorDataset(X_train, y_train)\n",
    "    trainloader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,   # Set True if you prefer shuffling\n",
    "        drop_last=False\n",
    "    )\n",
    "\n",
    "    if lower_bound < total_size:\n",
    "        X_val = X[lower_bound:].to(device)\n",
    "        y_val = y[lower_bound:].to(device)\n",
    "        val_dataset = TensorDataset(X_val, y_val)\n",
    "        valloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    else:\n",
    "        # If there's no leftover data for \"test\", handle gracefully\n",
    "        X_val = None\n",
    "        y_val = None\n",
    "        valloader = None\n",
    "\n",
    "    # ---------------------------------------------\n",
    "    # 1) SINGLE TRAINING PHASE + Track Loss Curves\n",
    "    # ---------------------------------------------\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "\n",
    "    epoch_train_losses = []\n",
    "    epoch_test_losses = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # TRAINING PASS\n",
    "        epoch_loss = 0.0\n",
    "        for X_batch, y_batch in trainloader:\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward\n",
    "            pred_y = model(X_batch)\n",
    "            loss = criterion(pred_y, y_batch)\n",
    "\n",
    "            # Backprop & update\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  # optional\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = epoch_loss / len(trainloader)\n",
    "        epoch_train_losses.append(avg_train_loss)\n",
    "\n",
    "        \n",
    "        # VALIDATION PASS (Optional but needed to get test_loss_curve)\n",
    "        if valloader is not None:\n",
    "            model.eval()\n",
    "            val_loss = 0.0\n",
    "            with torch.no_grad():\n",
    "                for Xb, yb in valloader:\n",
    "                    pred_yb = model(Xb)\n",
    "                    loss_b = criterion(pred_yb, yb)\n",
    "                    val_loss += loss_b.item()\n",
    "            avg_val_loss = val_loss / len(valloader)\n",
    "            epoch_test_losses.append(avg_val_loss)\n",
    "\n",
    "            model.train()  # Switch back to train mode\n",
    "\n",
    "        else:\n",
    "            # If no validation set, just store None or 0\n",
    "            epoch_test_losses.append(None)\n",
    "\n",
    "        # Print progress every 5 epochs or last epoch\n",
    "        if (epoch + 1) % 5 == 0 or (epoch == num_epochs - 1):\n",
    "            if epoch_test_losses[-1] is not None:\n",
    "                print(f\"[Epoch {epoch+1}/{num_epochs}] \",\n",
    "                      f\"Train Loss: {avg_train_loss:.4f}\")\n",
    "            else:\n",
    "                print(f\"[Epoch {epoch+1}/{num_epochs}] \"\n",
    "                      f\"Train Loss: {avg_train_loss:.4f}\")\n",
    "\n",
    "    # % decrease from first to last train loss\n",
    "    if len(epoch_train_losses) > 1:\n",
    "        loss_decrease_percentage = ((epoch_train_losses[-1] - epoch_train_losses[0])\n",
    "                                    / epoch_train_losses[0]) * 100\n",
    "    else:\n",
    "        loss_decrease_percentage = 0.0\n",
    "\n",
    "    final_train_loss = epoch_train_losses[-1]\n",
    "\n",
    "    # -------------------------------\n",
    "    # 2) ROLLING PREDICTIONS, NO UPDATE\n",
    "    # -------------------------------\n",
    "    model.eval()\n",
    "    rolling_predictions = []\n",
    "    rolling_targets = []\n",
    "\n",
    "    for i in range(lower_bound, total_size):\n",
    "        X_test = X[i:i+1].to(device)\n",
    "        y_test = y[i:i+1].to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            pred_y = model(X_test)\n",
    "            probabilities = torch.softmax(pred_y, dim=1).cpu().numpy()\n",
    "            prob_class_1 = probabilities[:, 1]  # shape: (1,)\n",
    "\n",
    "            # Threshold-based logic\n",
    "            pred_classes = np.zeros_like(prob_class_1)\n",
    "            # Predict -1 if prob < lower_threshold\n",
    "            pred_classes[prob_class_1 < lower_threshold] = -1\n",
    "            # Predict +1 if prob > (1 - lower_threshold)\n",
    "            pred_classes[prob_class_1 > (1 - lower_threshold)] = 1\n",
    "\n",
    "        rolling_predictions.append(pred_classes[0])\n",
    "        rolling_targets.append(y_test.item())\n",
    "\n",
    "    rolling_predictions = np.array(rolling_predictions)\n",
    "    rolling_targets = np.array(rolling_targets).astype(int)\n",
    "\n",
    "    # If original labels might be {0,1}, adapt as needed\n",
    "    rolling_targets[rolling_targets == 0] = -1\n",
    "\n",
    "    # Filter out zero predictions\n",
    "    nonzero_mask = (rolling_predictions != 0)\n",
    "    filtered_preds = rolling_predictions[nonzero_mask]\n",
    "    filtered_targets = rolling_targets[nonzero_mask]\n",
    "\n",
    "    if len(filtered_preds) == 0:\n",
    "        accuracy_nonzero = None\n",
    "        print(\"No nonzero predictions, cannot compute thresholded accuracy.\")\n",
    "    else:\n",
    "        accuracy_nonzero = accuracy_score(filtered_targets, filtered_preds)\n",
    "        print(f\"Accuracy on Nonzero Predictions: {accuracy_nonzero:.4f}\")\n",
    "\n",
    "    # -------------------------------------------------\n",
    "    # 3) PLOT (if plots_dir is set and there's test data)\n",
    "    # -------------------------------------------------\n",
    "    if plots_dir is not None:\n",
    "        os.makedirs(plots_dir, exist_ok=True)\n",
    "\n",
    "        # If user didn't provide a filename, create a default\n",
    "        if plot_filename is None:\n",
    "            plot_filename = \"train_test_loss_curve.png\"\n",
    "        plot_path = os.path.join(plots_dir, plot_filename)\n",
    "\n",
    "        # Plot the training and validation (test) loss curves\n",
    "        plt.figure(figsize=(8, 5))\n",
    "        plt.plot(epoch_train_losses, label=\"Train Loss\")\n",
    "        # Only plot test loss if it isn't None\n",
    "        if any(x is not None for x in epoch_test_losses):\n",
    "            plt.plot(epoch_test_losses, label=\"Test Loss\")\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.title(\"Train vs. Test Loss per Epoch\")\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "\n",
    "        # plt.savefig(plot_path)\n",
    "        plt.show()\n",
    "        print(f\"Loss curves saved to: {plot_path}\")\n",
    "\n",
    "    # ----------------\n",
    "    # 4) Return results\n",
    "    # ----------------\n",
    "    return {\n",
    "        \"rolling_predictions\": rolling_predictions,\n",
    "        \"rolling_targets\": rolling_targets,\n",
    "        \"filtered_predictions\": filtered_preds,\n",
    "        \"filtered_targets\": filtered_targets,\n",
    "        \"accuracy_nonzero\": accuracy_nonzero,\n",
    "        \"loss_decrease_percentage\": loss_decrease_percentage,\n",
    "        \"final_train_loss\": final_train_loss,\n",
    "        \"train_loss_curve\": epoch_train_losses,\n",
    "        \"test_loss_curve\": epoch_test_losses\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e5e03d-d5ce-406e-a4a2-e91cab13cd87",
   "metadata": {},
   "source": [
    "### **Execute evaluation funcion**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2397b7ed-82c7-4255-ab1f-848f0b531098",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_stocks_pkl_df(processing, security_type, period, window_size, project_dir, stocks):\n",
    "\n",
    "    X_list = []\n",
    "    y_list = []\n",
    "\n",
    "    for stock in stocks:\n",
    "        initial_data_dir = os.path.join(project_dir, f\"00_data/{processing}\") \n",
    "\n",
    "        # 1) Load original data (info only)\n",
    "        filename = f\"{security_type}/{stock}/{period}_data.csv\"\n",
    "        original_input_filepath = os.path.join(initial_data_dir, filename)\n",
    "        original_data = pd.read_csv(original_input_filepath)\n",
    "\n",
    "        print(f\"----- Appending stock: {stock}, period: {period}, data_type: {security_type} -----\")\n",
    "\n",
    "        # 2) Load the preprocessed data\n",
    "        pkl_filename = f\"{processing}/{security_type}/{stock}/{period}_{window_size}_data.pkl\"\n",
    "        input_filepath = os.path.join(horizontal_data_dir, pkl_filename)\n",
    "        input_df = pd.read_pickle(input_filepath)\n",
    "\n",
    "        # 3) Reshape\n",
    "        X_resampled, y_resampled = reshape_remove_characters(input_df)\n",
    "\n",
    "        # Store in lists\n",
    "        X_list.append(X_resampled)\n",
    "        y_list.append(y_resampled)\n",
    "\n",
    "    # Concatenate all stocks along the sample dimension\n",
    "    X_combined = np.concatenate(X_list, axis=0)  # Shape: (total_samples, num_features, window_size)\n",
    "    y_combined = np.concatenate(y_list, axis=0)  # Shape: (total_samples,)\n",
    "\n",
    "    # Print shapes to verify\n",
    "    print(\"Final X shape:\", X_combined.shape)\n",
    "    print(\"Final y shape:\", y_combined.shape)\n",
    "    \n",
    "    return X_combined, y_combined\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf981ac3-95b4-41e6-b2f2-c44849b4280e",
   "metadata": {},
   "source": [
    "## **Apply concatenation to specified datasets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4e9684f3-3f8d-480e-9630-246d35135098",
   "metadata": {},
   "outputs": [],
   "source": [
    "processing = \"clean\"\n",
    "security_type = \"technical\"\n",
    "period = \"10y\"\n",
    "stocks = ['AAPL', 'MSFT', 'AMZN', 'NVDA', 'SPX']\n",
    "# window_size = 50\n",
    "\n",
    "# X_combined, y_combined = combine_stocks_pkl_df(processing, security_type, period, window_size, project_dir, stocks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb390b9-4ec7-4591-a161-727b96bd36b9",
   "metadata": {},
   "source": [
    "## **Run LSTM and GRU Models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "47587092-2c27-4ea0-b488-436522249985",
   "metadata": {},
   "outputs": [],
   "source": [
    "possible_train_size = 95\n",
    "\n",
    "num_epochs = 100\n",
    "learning_rate = 0.01\n",
    "prediction_threshold = 0.5\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "batch_size = 64\n",
    "hidden_size = 64  \n",
    "output_size = 2  \n",
    "num_layers = 2\n",
    "dropout = 0.2\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a18c2d3-6e23-4c1e-a3b0-3cc638c384ae",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Appending stock: AAPL, period: 10y, data_type: technical -----\n"
     ]
    }
   ],
   "source": [
    "model_types = [\"lstm\", \"gru\"]   \n",
    "sample_size = 50\n",
    "window_sizes = [5, 10, 50, 100]\n",
    "window_size = 5\n",
    "\n",
    "results_list = []\n",
    "\n",
    "\n",
    "X_combined, y_combined = combine_stocks_pkl_df(processing, security_type, period, window_size, project_dir, stocks)\n",
    "input_size = X_combined.shape[2]\n",
    "train_size = int(X_combined.shape[0] * possible_train_size / 100)\n",
    "# train_size = X_combined.shape[0]-30\n",
    "test_size = X_combined.shape[0] - train_size\n",
    "\n",
    "\n",
    "for i in range(sample_size):\n",
    "    if model_type == \"gru\":\n",
    "        model = GRU3DClassifier(input_size, hidden_size, output_size, num_layers, dropout)\n",
    "    elif model_type == \"lstm\":\n",
    "        model = StockPriceLSTM(input_size, hidden_size, output_size)\n",
    "\n",
    "    model = model.to(device)\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    print(f\"Training {stocks[-1]} | LR: {learning_rate} | Epochs: {num_epochs} \"\n",
    "          f\"| Batch: {batch_size} | Security type: {security_type}\")\n",
    "\n",
    "\n",
    "    result = evaluate_model_plot(\n",
    "        model, \n",
    "        X_combined, \n",
    "        y_combined, \n",
    "        criterion, \n",
    "        optimizer, \n",
    "        device, \n",
    "        train_size, \n",
    "        batch_size, \n",
    "        num_epochs, \n",
    "        lower_threshold = 0.5,\n",
    "        plots_dir=plots_dir,\n",
    "        plot_filename=None\n",
    "    )\n",
    "\n",
    "    # Store results in a list\n",
    "    results_list.append({\n",
    "        \"security_type\": security_type,\n",
    "        \"window_size\": window_size,\n",
    "        \"model_type\": model_type,\n",
    "        \"iteration\": i + 1,\n",
    "        \"accuracy_nonzero\": result[\"accuracy_nonzero\"],\n",
    "        \"loss_decrease_percentage\": result[\"loss_decrease_percentage\"],\n",
    "        \"final_train_loss\": result[\"final_train_loss\"]\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6684d41d-3665-41e0-b20c-15716fb40ffb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b27199d-23d3-4384-8a00-68ac9d0a814c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
