{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "940889cf-06e0-4e8c-8efa-1eeacc97ca57",
   "metadata": {},
   "source": [
    "## **This notebook aims to compare some models and store them in a reasonable fashion**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2e46c228-21f4-4e80-a4e8-b62981ab4799",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.metrics import accuracy_score\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a11a538-40d5-49c3-92e9-e26fdcefe083",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"  # Use GPU 0 in first notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7b259658-5fed-4231-9f06-f9db567e1d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "project_dir = \"/home/jupyter-tfg2425paula/prediction_project_v3\"\n",
    "os.chdir(project_dir)\n",
    "\n",
    "clean_data_dir = os.path.join(project_dir, \"00_data/clean\")\n",
    "horizontal_data_dir = os.path.join(project_dir, \"00_data/horizontal_structure\")\n",
    "results_dir = os.path.join(project_dir, \"02_results\")\n",
    "pca_data_dir = os.path.join(project_dir, \"00_data/pca\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7319029-ba4c-4c90-91b9-a0a02b5be9af",
   "metadata": {},
   "source": [
    "### **GRU Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a7a1485d-99bb-4358-a216-1c1c75e50ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRU3DClassifier(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers, dropout):\n",
    "        super(GRU3DClassifier, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "\n",
    "        out, _ = self.gru(x, h0)\n",
    "        out = self.fc(out[:, -1, :]) \n",
    "        # return self.sigmoid(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3061a6b4-dad3-4129-a2ed-0098e04440e4",
   "metadata": {},
   "source": [
    "### **LSTM Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "95ffa6e2-2c35-48de-8c95-b7ce86fe9dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StockPriceLSTM(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers=1, dropout=0.0):\n",
    "        super(StockPriceLSTM, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers=num_layers, \n",
    "                            batch_first=True, dropout=dropout)\n",
    "    \n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)  # Get the batch size dynamically\n",
    "\n",
    "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_dim).to(x.device)  # (num_layers, batch_size, hidden_dim)\n",
    "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_dim).to(x.device)  # (num_layers, batch_size, hidden_dim)\n",
    "        \n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "\n",
    "        out = self.fc(out[:, -1, :]) \n",
    "        # out = self.sigmoid(out)\n",
    "        return out\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df6d4b78-1492-45ea-a9af-854d8d69b8de",
   "metadata": {},
   "source": [
    "### **Set folders**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e67b679f-204f-4e15-89fe-09879ed7246c",
   "metadata": {},
   "source": [
    "Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2b80ed82-98cd-453d-94e9-b7f42c61040e",
   "metadata": {},
   "outputs": [],
   "source": [
    "processing_types = [\"clean\", \"pca\"]\n",
    "processing_types= [\"clean\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da3781cf-0f0b-4858-bff8-09f839e2da04",
   "metadata": {},
   "source": [
    "Folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a2958856-f0de-4f93-85b5-9fdd169a2a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stocks = ['AAPL', 'MSFT', 'AMZN', 'NVDA', 'SPX']\n",
    "stocks = ['AAPL']\n",
    "# types_securities = [\"single_name\", \"options\", \"technical\"]\n",
    "types_securities = [\"options\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2e929df-0399-4e41-9b2b-1863f8c215b5",
   "metadata": {},
   "source": [
    "Different files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9b853df6-38b4-4bd7-85e5-6a232f17905e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# years = [\"15y\", \"10y\", \"5y\", \"2y\"]\n",
    "years = [\"10y\"]\n",
    "# window_sizes = [5, 10, 50, 100]\n",
    "window_sizes = [5]\n",
    "# train_sizes = [80, 90, 95]\n",
    "train_sizes = [95]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f9fb2cc-d8de-4e4f-be9d-f8cb7aac8e16",
   "metadata": {},
   "source": [
    "Same file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "145e5ba2-35c7-4a71-b100-624ca6ed1ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds = [0.3, 0.35, 0.4, 0.45, 0.5]\n",
    "thresholds = [0.5]\n",
    "learning_rates = [0.005, 0.008, 0.009, 0.01]\n",
    "learning_rates = [0.01]\n",
    "num_epochs_list = [100, 200]\n",
    "num_epochs_list = [100]\n",
    "batch_sizes = [16, 32]\n",
    "batch_sizes = [16]\n",
    "prediction_thresholds = [0.35, 0.4, 0.45, 0.5]\n",
    "prediction_thresholds = [0.5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbbee35f-0147-48b0-93ef-d9687984bbd0",
   "metadata": {},
   "source": [
    "#### **Model and Hyperparameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "71861bc1-7067-47f8-b3dd-a5cef2bc0f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "hidden_size = 64  \n",
    "output_size = 2  \n",
    "num_layers = 2\n",
    "dropout = 0.2\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "60900dea-a72e-4b53-979b-aded1e725489",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_types = [\"lstm\", \"gru\"]   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d873f70-b583-4e84-9289-bc303ef0446f",
   "metadata": {},
   "source": [
    "#### **Last data modifications**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "85729e3c-f060-4dc2-a4fc-a80573fefd73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_remove_characters(df):\n",
    "\n",
    "    X = np.array([np.stack(row) for row in df.drop(columns=['Target']).values])\n",
    "    y = df['Target'].values\n",
    "\n",
    "    smote = SMOTE(random_state=42)\n",
    "    n_samples, timesteps, n_features = X.shape\n",
    "    X_flat = X.reshape((n_samples, timesteps * n_features))\n",
    "    X_flat = np.where(X_flat == 'ç', 0, X_flat)\n",
    "\n",
    "    X_resampled = X_flat.reshape((-1, timesteps, n_features))\n",
    "    \n",
    "    return X_resampled, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88656719-d0c4-4bf2-aef1-31f00b167d2a",
   "metadata": {},
   "source": [
    "### **Evaluation function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "74c49327-1c0d-4706-9943-54b795cb0687",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_rolling_unchanged_model_threshold(\n",
    "    model, \n",
    "    X, \n",
    "    y, \n",
    "    criterion, \n",
    "    optimizer, \n",
    "    device, \n",
    "    train_size, \n",
    "    batch_size, \n",
    "    num_epochs, \n",
    "    lower_threshold\n",
    "):\n",
    "    \"\"\"\n",
    "    Evaluate a PyTorch model using a rolling prediction approach for time series,\n",
    "    training the model only once on the initial training set. For each time step\n",
    "    after train_size, the model makes a prediction without further parameter updates.\n",
    "    Only predicts +1 or -1 if the probability of class 1 is above/below given thresholds;\n",
    "    otherwise, predicts 0. Accuracy is computed only on nonzero predictions.\n",
    "\n",
    "    Args:\n",
    "        model:          PyTorch model to evaluate.\n",
    "        X:              Feature data (numpy array).\n",
    "        y:              Target data (numpy array).\n",
    "        criterion:      Loss function (e.g., CrossEntropyLoss).\n",
    "        optimizer:      Optimizer (e.g., Adam).\n",
    "        device:         Device for computation (CPU or GPU).\n",
    "        train_size:     Initial size of the training data (int or float).\n",
    "                        If < 1, treated as fraction of total length.\n",
    "        batch_size:     Batch size for training.\n",
    "        num_epochs:     Number of epochs for initial training only.\n",
    "        lower_threshold: Probability threshold below which model predicts -1.\n",
    "        upper_threshold: Probability threshold above which model predicts +1.\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary with the following keys:\n",
    "            - \"rolling_predictions\": All predictions (-1, 0, +1) across the test period.\n",
    "            - \"rolling_targets\": Corresponding true targets in [-1, +1].\n",
    "            - \"filtered_predictions\": Nonzero predictions only.\n",
    "            - \"filtered_targets\": Targets corresponding to nonzero predictions.\n",
    "            - \"accuracy_nonzero\": Accuracy computed only on nonzero predictions.\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert X, y to tensors\n",
    "    X = torch.tensor(X, dtype=torch.float32)\n",
    "    y = torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "    # Determine initial training set size\n",
    "    if train_size < 1.0:\n",
    "        lower_bound = int(train_size * len(X))\n",
    "    else:\n",
    "        lower_bound = train_size\n",
    "\n",
    "    # -------------------------\n",
    "    # 1) SINGLE TRAINING PHASE\n",
    "    # -------------------------\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "    \n",
    "    X_train = X[:lower_bound].to(device)\n",
    "    y_train = y[:lower_bound].to(device)\n",
    "\n",
    "    train_dataset = TensorDataset(X_train, y_train)\n",
    "    trainloader = DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size=batch_size, \n",
    "        shuffle=False,         # Keep False if order matters; True for better generalization\n",
    "        # num_workers=4,         # Adjust based on your CPU cores\n",
    "        # pin_memory=True,       # Speeds up transfer if using GPUs\n",
    "        drop_last=False        # Ensure the last batch is included\n",
    "    )\n",
    "\n",
    "    epoch_train_losses = []\n",
    "    for epoch in range(num_epochs):\n",
    "        # torch.cuda.empty_cache()\n",
    "        epoch_loss = 0.0\n",
    "        for X_batch, y_batch in trainloader:\n",
    "            X_batch = X_batch.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            pred_y = model(X_batch)   # [batch_size, num_classes]\n",
    "            loss = criterion(pred_y, y_batch)\n",
    "            loss.backward()\n",
    "\n",
    "            # Gradient clipping (optional)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "               \n",
    "        if (epoch + 1) % 5 == 0 or epoch == num_epochs - 1:\n",
    "            print(f\"[Train] Epoch {epoch+1}/{num_epochs}, Loss={epoch_loss/len(trainloader):.4f}\")\n",
    "\n",
    "        epoch_train_losses.append(epoch_loss/len(trainloader))\n",
    "        \n",
    "    loss_decrease_percentage = ((epoch_train_losses[-1] - epoch_train_losses[0]) / epoch_train_losses[0]) * 100\n",
    "    # ---------------------------------\n",
    "    # 2) ROLLING PREDICTIONS, NO UPDATE\n",
    "    # ---------------------------------\n",
    "    model.eval()\n",
    "\n",
    "    rolling_predictions = []\n",
    "    rolling_targets     = []\n",
    "\n",
    "    for i in range(lower_bound, len(X)):\n",
    "        # Single-step \"test\" sample\n",
    "        X_test = X[i:i+1].to(device)  # shape: (1, num_features)\n",
    "        y_test = y[i:i+1].to(device)  # shape: (1, )\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # Forward pass\n",
    "            pred_y = model(X_test)  # [1, num_classes]\n",
    "            probabilities = torch.softmax(pred_y, dim=1).cpu().numpy()  # shape: (1, 2)\n",
    "            prob_class_1  = probabilities[:, 1]  # shape: (1,)\n",
    "\n",
    "            # Threshold-based logic\n",
    "            # Initialize all predictions to 0\n",
    "            pred_classes = np.zeros_like(prob_class_1)\n",
    "            # Predict -1 if prob < lower_threshold\n",
    "            pred_classes[prob_class_1 < lower_threshold] = -1\n",
    "            # Predict +1 if prob > upper_threshold\n",
    "            pred_classes[prob_class_1 > 1-lower_threshold] = 1\n",
    "\n",
    "        rolling_predictions.append(pred_classes[0])  # scalar\n",
    "        rolling_targets.append(y_test.item())\n",
    "\n",
    "    rolling_predictions = np.array(rolling_predictions)\n",
    "    rolling_targets = np.array(rolling_targets).astype(int)\n",
    "\n",
    "    # Convert any 0-labeled targets to -1 if your original data is in [-1, +1]\n",
    "    # (Sometimes y might be {0,1} or {-1, +1}; adapt as needed.)\n",
    "    rolling_targets[rolling_targets == 0] = -1\n",
    "\n",
    "    # Filter out zero predictions\n",
    "    nonzero_mask = rolling_predictions != 0\n",
    "    filtered_preds = rolling_predictions[nonzero_mask]\n",
    "    filtered_targets = rolling_targets[nonzero_mask]\n",
    "\n",
    "    if len(filtered_preds) == 0:\n",
    "        accuracy_nonzero = None\n",
    "        print(\"No nonzero predictions, cannot compute thresholded accuracy.\")\n",
    "    else:\n",
    "        accuracy_nonzero = accuracy_score(filtered_targets, filtered_preds)\n",
    "        print(f\"Accuracy on Nonzero Predictions: {accuracy_nonzero:.4f}\")\n",
    "\n",
    "    return {\n",
    "        \"rolling_predictions\": rolling_predictions,\n",
    "        \"rolling_targets\": rolling_targets,\n",
    "        \"filtered_predictions\": filtered_preds,\n",
    "        \"filtered_targets\": filtered_targets,\n",
    "        \"accuracy_nonzero\": accuracy_nonzero,\n",
    "        \"loss_decrease_percentage\": loss_decrease_percentage,\n",
    "        \"final_train_loss\": epoch_train_losses[-1] \n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b794e699-83fb-4ca2-8c93-318858018a2e",
   "metadata": {},
   "source": [
    "### **2nd Type of comparison:**\n",
    "\n",
    "Window sizes, for AAPL 10y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "672857df-0a56-49c8-a1f9-8972057d026b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5, lstm\n",
      "/home/jupyter-tfg2425paula/prediction_project_v3/00_data/horizontal_structure/clean/single_name/AAPL/10y_5_data.pkl\n",
      "Training AAPL | LR: 0.01 | Epochs: 100 | Batch: 32 | Prediction Threshold: 0.5\n",
      "[Train] Epoch 5/100, Loss=0.6935\n",
      "[Train] Epoch 10/100, Loss=0.6910\n",
      "[Train] Epoch 15/100, Loss=0.6882\n",
      "[Train] Epoch 20/100, Loss=0.6849\n",
      "[Train] Epoch 25/100, Loss=0.6813\n",
      "[Train] Epoch 30/100, Loss=0.6769\n",
      "[Train] Epoch 35/100, Loss=0.6725\n",
      "[Train] Epoch 40/100, Loss=0.6677\n",
      "[Train] Epoch 45/100, Loss=0.6637\n",
      "[Train] Epoch 50/100, Loss=0.6600\n",
      "[Train] Epoch 55/100, Loss=0.6566\n",
      "[Train] Epoch 60/100, Loss=0.6537\n",
      "[Train] Epoch 65/100, Loss=0.6505\n",
      "[Train] Epoch 70/100, Loss=0.6467\n",
      "[Train] Epoch 75/100, Loss=0.6432\n",
      "[Train] Epoch 80/100, Loss=0.6399\n",
      "[Train] Epoch 85/100, Loss=0.6365\n",
      "[Train] Epoch 90/100, Loss=0.6322\n",
      "[Train] Epoch 95/100, Loss=0.6268\n",
      "[Train] Epoch 100/100, Loss=0.6216\n",
      "Accuracy on Nonzero Predictions: 0.5191\n",
      "5, gru\n",
      "/home/jupyter-tfg2425paula/prediction_project_v3/00_data/horizontal_structure/clean/single_name/AAPL/10y_5_data.pkl\n",
      "Training AAPL | LR: 0.01 | Epochs: 100 | Batch: 32 | Prediction Threshold: 0.5\n",
      "[Train] Epoch 5/100, Loss=0.6936\n",
      "[Train] Epoch 10/100, Loss=0.6936\n",
      "[Train] Epoch 15/100, Loss=0.6930\n",
      "[Train] Epoch 20/100, Loss=0.6914\n",
      "[Train] Epoch 25/100, Loss=0.6877\n",
      "[Train] Epoch 30/100, Loss=0.6833\n",
      "[Train] Epoch 35/100, Loss=0.6820\n",
      "[Train] Epoch 40/100, Loss=0.6762\n",
      "[Train] Epoch 45/100, Loss=0.6655\n",
      "[Train] Epoch 50/100, Loss=0.6614\n",
      "[Train] Epoch 55/100, Loss=0.6550\n",
      "[Train] Epoch 60/100, Loss=0.6490\n",
      "[Train] Epoch 65/100, Loss=0.6383\n",
      "[Train] Epoch 70/100, Loss=0.6308\n",
      "[Train] Epoch 75/100, Loss=0.6255\n",
      "[Train] Epoch 80/100, Loss=0.6174\n",
      "[Train] Epoch 85/100, Loss=0.6144\n",
      "[Train] Epoch 90/100, Loss=0.6046\n",
      "[Train] Epoch 95/100, Loss=0.6034\n",
      "[Train] Epoch 100/100, Loss=0.5788\n",
      "Accuracy on Nonzero Predictions: 0.4809\n",
      "10, lstm\n",
      "/home/jupyter-tfg2425paula/prediction_project_v3/00_data/horizontal_structure/clean/single_name/AAPL/10y_10_data.pkl\n",
      "Training AAPL | LR: 0.01 | Epochs: 100 | Batch: 32 | Prediction Threshold: 0.5\n",
      "[Train] Epoch 5/100, Loss=0.6896\n",
      "[Train] Epoch 10/100, Loss=0.6665\n",
      "[Train] Epoch 15/100, Loss=0.6292\n",
      "[Train] Epoch 20/100, Loss=0.5848\n",
      "[Train] Epoch 25/100, Loss=0.5355\n",
      "[Train] Epoch 30/100, Loss=0.4820\n",
      "[Train] Epoch 35/100, Loss=0.4309\n",
      "[Train] Epoch 40/100, Loss=0.3836\n",
      "[Train] Epoch 45/100, Loss=0.3422\n",
      "[Train] Epoch 50/100, Loss=0.2986\n",
      "[Train] Epoch 55/100, Loss=0.2617\n",
      "[Train] Epoch 60/100, Loss=0.2233\n",
      "[Train] Epoch 65/100, Loss=0.1861\n",
      "[Train] Epoch 70/100, Loss=0.1559\n",
      "[Train] Epoch 75/100, Loss=0.1319\n",
      "[Train] Epoch 80/100, Loss=0.1343\n",
      "[Train] Epoch 85/100, Loss=0.1096\n",
      "[Train] Epoch 90/100, Loss=0.0805\n",
      "[Train] Epoch 95/100, Loss=0.0663\n",
      "[Train] Epoch 100/100, Loss=0.0542\n",
      "Accuracy on Nonzero Predictions: 0.3893\n",
      "10, gru\n",
      "/home/jupyter-tfg2425paula/prediction_project_v3/00_data/horizontal_structure/clean/single_name/AAPL/10y_10_data.pkl\n",
      "Training AAPL | LR: 0.01 | Epochs: 100 | Batch: 32 | Prediction Threshold: 0.5\n",
      "[Train] Epoch 5/100, Loss=0.6938\n",
      "[Train] Epoch 10/100, Loss=0.6819\n",
      "[Train] Epoch 15/100, Loss=0.6565\n",
      "[Train] Epoch 20/100, Loss=0.6105\n",
      "[Train] Epoch 25/100, Loss=0.5782\n",
      "[Train] Epoch 30/100, Loss=0.5410\n",
      "[Train] Epoch 35/100, Loss=0.5057\n",
      "[Train] Epoch 40/100, Loss=0.4762\n",
      "[Train] Epoch 45/100, Loss=0.4548\n",
      "[Train] Epoch 50/100, Loss=0.4111\n",
      "[Train] Epoch 55/100, Loss=0.4116\n",
      "[Train] Epoch 60/100, Loss=0.4007\n",
      "[Train] Epoch 65/100, Loss=0.3916\n",
      "[Train] Epoch 70/100, Loss=0.3825\n",
      "[Train] Epoch 75/100, Loss=0.3549\n",
      "[Train] Epoch 80/100, Loss=0.3603\n",
      "[Train] Epoch 85/100, Loss=0.3347\n",
      "[Train] Epoch 90/100, Loss=0.3442\n",
      "[Train] Epoch 95/100, Loss=0.3183\n",
      "[Train] Epoch 100/100, Loss=0.3334\n",
      "Accuracy on Nonzero Predictions: 0.4198\n",
      "50, lstm\n",
      "/home/jupyter-tfg2425paula/prediction_project_v3/00_data/horizontal_structure/clean/single_name/AAPL/10y_50_data.pkl\n",
      "Training AAPL | LR: 0.01 | Epochs: 100 | Batch: 32 | Prediction Threshold: 0.5\n",
      "[Train] Epoch 5/100, Loss=0.4192\n",
      "[Train] Epoch 10/100, Loss=0.0273\n",
      "[Train] Epoch 15/100, Loss=0.0055\n",
      "[Train] Epoch 20/100, Loss=0.0023\n",
      "[Train] Epoch 25/100, Loss=0.0013\n",
      "[Train] Epoch 30/100, Loss=0.0008\n",
      "[Train] Epoch 35/100, Loss=0.0005\n",
      "[Train] Epoch 40/100, Loss=0.0004\n",
      "[Train] Epoch 45/100, Loss=0.0003\n",
      "[Train] Epoch 50/100, Loss=0.0002\n",
      "[Train] Epoch 55/100, Loss=0.0002\n",
      "[Train] Epoch 60/100, Loss=0.0001\n",
      "[Train] Epoch 65/100, Loss=0.0001\n",
      "[Train] Epoch 70/100, Loss=0.0001\n",
      "[Train] Epoch 75/100, Loss=0.0001\n",
      "[Train] Epoch 80/100, Loss=0.0000\n",
      "[Train] Epoch 85/100, Loss=0.0000\n",
      "[Train] Epoch 90/100, Loss=0.0000\n",
      "[Train] Epoch 95/100, Loss=0.0000\n",
      "[Train] Epoch 100/100, Loss=0.0000\n",
      "Accuracy on Nonzero Predictions: 0.5116\n",
      "50, gru\n",
      "/home/jupyter-tfg2425paula/prediction_project_v3/00_data/horizontal_structure/clean/single_name/AAPL/10y_50_data.pkl\n",
      "Training AAPL | LR: 0.01 | Epochs: 100 | Batch: 32 | Prediction Threshold: 0.5\n",
      "[Train] Epoch 5/100, Loss=0.6728\n",
      "[Train] Epoch 10/100, Loss=0.4289\n",
      "[Train] Epoch 15/100, Loss=0.2458\n",
      "[Train] Epoch 20/100, Loss=0.1849\n",
      "[Train] Epoch 25/100, Loss=0.1295\n",
      "[Train] Epoch 30/100, Loss=0.1056\n",
      "[Train] Epoch 35/100, Loss=0.0856\n",
      "[Train] Epoch 40/100, Loss=0.0871\n",
      "[Train] Epoch 45/100, Loss=0.0745\n",
      "[Train] Epoch 50/100, Loss=0.0641\n",
      "[Train] Epoch 55/100, Loss=0.0543\n",
      "[Train] Epoch 60/100, Loss=0.0695\n",
      "[Train] Epoch 65/100, Loss=0.0599\n",
      "[Train] Epoch 70/100, Loss=0.0563\n",
      "[Train] Epoch 75/100, Loss=0.0691\n",
      "[Train] Epoch 80/100, Loss=0.0696\n",
      "[Train] Epoch 85/100, Loss=0.0530\n",
      "[Train] Epoch 90/100, Loss=0.0403\n",
      "[Train] Epoch 95/100, Loss=0.0571\n",
      "[Train] Epoch 100/100, Loss=0.0543\n",
      "Accuracy on Nonzero Predictions: 0.4961\n",
      "100, lstm\n",
      "/home/jupyter-tfg2425paula/prediction_project_v3/00_data/horizontal_structure/clean/single_name/AAPL/10y_100_data.pkl\n",
      "Training AAPL | LR: 0.01 | Epochs: 100 | Batch: 32 | Prediction Threshold: 0.5\n",
      "[Train] Epoch 5/100, Loss=0.1481\n",
      "[Train] Epoch 10/100, Loss=0.0047\n",
      "[Train] Epoch 15/100, Loss=0.0018\n",
      "[Train] Epoch 20/100, Loss=0.0009\n",
      "[Train] Epoch 25/100, Loss=0.0006\n",
      "[Train] Epoch 30/100, Loss=0.0004\n",
      "[Train] Epoch 35/100, Loss=0.0003\n",
      "[Train] Epoch 40/100, Loss=0.0002\n",
      "[Train] Epoch 45/100, Loss=0.0001\n",
      "[Train] Epoch 50/100, Loss=0.0001\n",
      "[Train] Epoch 55/100, Loss=0.0001\n",
      "[Train] Epoch 60/100, Loss=0.0001\n",
      "[Train] Epoch 65/100, Loss=0.0000\n",
      "[Train] Epoch 70/100, Loss=0.0000\n",
      "[Train] Epoch 75/100, Loss=0.0000\n",
      "[Train] Epoch 80/100, Loss=0.0000\n",
      "[Train] Epoch 85/100, Loss=0.0000\n",
      "[Train] Epoch 90/100, Loss=0.0000\n",
      "[Train] Epoch 95/100, Loss=0.0000\n",
      "[Train] Epoch 100/100, Loss=0.0000\n",
      "Accuracy on Nonzero Predictions: 0.4048\n",
      "100, gru\n",
      "/home/jupyter-tfg2425paula/prediction_project_v3/00_data/horizontal_structure/clean/single_name/AAPL/10y_100_data.pkl\n",
      "Training AAPL | LR: 0.01 | Epochs: 100 | Batch: 32 | Prediction Threshold: 0.5\n",
      "[Train] Epoch 5/100, Loss=0.6652\n",
      "[Train] Epoch 10/100, Loss=0.3281\n",
      "[Train] Epoch 15/100, Loss=0.1529\n",
      "[Train] Epoch 20/100, Loss=0.0839\n",
      "[Train] Epoch 25/100, Loss=0.0615\n",
      "[Train] Epoch 30/100, Loss=0.0659\n",
      "[Train] Epoch 35/100, Loss=0.0521\n",
      "[Train] Epoch 40/100, Loss=0.0602\n",
      "[Train] Epoch 45/100, Loss=0.0512\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [18], line 58\u001b[0m\n\u001b[1;32m     54\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mlearning_rate)\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstock\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | LR: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlearning_rate\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | Epochs: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     56\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m| Batch: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbatch_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | Prediction Threshold: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprediction_threshold\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 58\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_rolling_unchanged_model_threshold\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_resampled\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_resampled\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlower_threshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprediction_threshold\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m     \n\u001b[1;32m     71\u001b[0m rolling_predictions \u001b[38;5;241m=\u001b[39m result[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrolling_predictions\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     72\u001b[0m rolling_targets \u001b[38;5;241m=\u001b[39m result[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrolling_targets\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "Cell \u001b[0;32mIn [14], line 81\u001b[0m, in \u001b[0;36mevaluate_rolling_unchanged_model_threshold\u001b[0;34m(model, X, y, criterion, optimizer, device, train_size, batch_size, num_epochs, lower_threshold)\u001b[0m\n\u001b[1;32m     78\u001b[0m y_batch \u001b[38;5;241m=\u001b[39m y_batch\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     80\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 81\u001b[0m pred_y \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_batch\u001b[49m\u001b[43m)\u001b[49m   \u001b[38;5;66;03m# [batch_size, num_classes]\u001b[39;00m\n\u001b[1;32m     82\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(pred_y, y_batch)\n\u001b[1;32m     83\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn [5], line 14\u001b[0m, in \u001b[0;36mGRU3DClassifier.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m     12\u001b[0m     h0 \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers, x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden_size)\u001b[38;5;241m.\u001b[39mto(x\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m---> 14\u001b[0m     out, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgru\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mh0\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc(out[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :]) \n\u001b[1;32m     16\u001b[0m     \u001b[38;5;66;03m# return self.sigmoid(out)\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/rnn.py:1392\u001b[0m, in \u001b[0;36mGRU.forward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m   1390\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_forward_args(\u001b[38;5;28minput\u001b[39m, hx, batch_sizes)\n\u001b[1;32m   1391\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_sizes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1392\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgru\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1393\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1394\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1395\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_flat_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1396\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1397\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_layers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1398\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1399\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1400\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbidirectional\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1401\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_first\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1402\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1403\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1404\u001b[0m     result \u001b[38;5;241m=\u001b[39m _VF\u001b[38;5;241m.\u001b[39mgru(\n\u001b[1;32m   1405\u001b[0m         \u001b[38;5;28minput\u001b[39m,\n\u001b[1;32m   1406\u001b[0m         batch_sizes,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1413\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbidirectional,\n\u001b[1;32m   1414\u001b[0m     )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "results_list = []\n",
    "stock = \"AAPL\"\n",
    "period = \"10y\"\n",
    "possible_train_size = 95\n",
    "window_size = 5\n",
    "batch_size = 32\n",
    "num_epochs = 100\n",
    "\n",
    "prediction_threshold = 0.5\n",
    "\n",
    "output_folder = os.path.join(results_dir, f\"inidividual_trials\") \n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "results_csv_path = os.path.join(output_folder, f\"01_{stock}_proc_model_type.csv\")\n",
    "processing = \"clean\"\n",
    "security_type = \"single_name\"\n",
    "window_sizes = [5, 10, 50, 100, 500, 1000]\n",
    "\n",
    "learning_rate = 0.01\n",
    "\n",
    "results_list = []\n",
    "for window_size in window_sizes:\n",
    "    initial_data_dir = os.path.join(project_dir, f\"00_data/{processing_type}\") \n",
    "    for model_type in model_types:\n",
    "\n",
    "        # Load original data (info only)\n",
    "        filename = f\"{security_type}/{stock}/{period}_data.csv\"\n",
    "        original_input_filepath = os.path.join(initial_data_dir, filename)\n",
    "        original_data = pd.read_csv(original_input_filepath)\n",
    "\n",
    "        # Iterate over window sizes\n",
    "        print(f\"{window_size}, {model_type}\")\n",
    "\n",
    "        # Load data using the 'processing' variable in path\n",
    "        pkl_filename = f\"{processing}/{security_type}/{stock}/{period}_{window_size}_data.pkl\"\n",
    "        input_filepath = os.path.join(horizontal_data_dir, pkl_filename)\n",
    "        print(input_filepath)\n",
    "        input_df = pd.read_pickle(input_filepath)\n",
    "\n",
    "        X_resampled, y_resampled = reshape_remove_characters(input_df)\n",
    "\n",
    "        input_size = X_resampled.shape[2]\n",
    "        train_size = int(X_resampled.shape[0] * possible_train_size / 100)\n",
    "        test_size = X_resampled.shape[0] - train_size\n",
    "\n",
    "        # Generate model\n",
    "        if model_type == \"gru\":\n",
    "            model = GRU3DClassifier(input_size, hidden_size, output_size, num_layers, dropout)\n",
    "        elif model_type == \"lstm\":\n",
    "            model = StockPriceLSTM(input_size, hidden_size, output_size)\n",
    "\n",
    "        model = model.to(device)\n",
    "\n",
    "        optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "        print(f\"Training {stock} | LR: {learning_rate} | Epochs: {num_epochs} \"\n",
    "              f\"| Batch: {batch_size} | Prediction Threshold: {prediction_threshold}\")\n",
    "\n",
    "        result = evaluate_rolling_unchanged_model_threshold(\n",
    "            model, \n",
    "            X_resampled, \n",
    "            y_resampled, \n",
    "            criterion, \n",
    "            optimizer, \n",
    "            device, \n",
    "            train_size, \n",
    "            batch_size, \n",
    "            num_epochs, \n",
    "            lower_threshold=prediction_threshold\n",
    "        )     \n",
    "\n",
    "        rolling_predictions = result[\"rolling_predictions\"]\n",
    "        rolling_targets = result[\"rolling_targets\"]\n",
    "        test_accuracy = result[\"accuracy_nonzero\"]\n",
    "        loss_decrease_percentage = result[\"loss_decrease_percentage\"]\n",
    "        nonzero_preds = np.count_nonzero(rolling_predictions)\n",
    "        final_train_loss = result[\"final_train_loss\"]\n",
    "        \n",
    "        # 1) Create a record (dictionary) for this run\n",
    "        run_record = {\"STOCK\": stock,\n",
    "            \"DATA_TYPE\": security_type,\n",
    "            \"MODEL\": model_type.upper(),  # Convert to uppercase for consistency\n",
    "            \"PROCESSING\": processing,\n",
    "            \"ACCURACY\": test_accuracy,\n",
    "            \"TRAIN_PCT_DECREASE\": loss_decrease_percentage,\n",
    "            \"FINAL_TRAIN_LOSS\": final_train_loss}\n",
    "\n",
    "        # 2) Append to list\n",
    "        results_list.append(run_record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a1abaee6-0df1-44b7-b371-b87b92bff765",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) Write to CSV once for this batch of window_sizes (avoid partial duplication)\n",
    "if len(results_list) > 0:\n",
    "    df = pd.DataFrame(results_list)\n",
    "\n",
    "    if os.path.exists(results_csv_path):\n",
    "        # Append without header\n",
    "        df.to_csv(results_csv_path, mode='a', header=False, index=False)\n",
    "    else:\n",
    "        # Write new file with header\n",
    "        df.to_csv(results_csv_path, index=False)\n",
    "\n",
    "    # Clear the list before next iteration\n",
    "    results_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "796b209d-9eed-4763-9567-5e143e2577b6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
