{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d97c7787-9453-4ac8-94aa-ab3611d118d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2583d74d-73bc-45bc-9206-22cc43f0ffe7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "project_dir = \"/home/jupyter-tfg2425paula/prediction_project_v3\"\n",
    "os.chdir(project_dir)\n",
    "\n",
    "clean_data_dir = os.path.join(project_dir, \"00_data/clean\")\n",
    "horizontal_data_dir = os.path.join(project_dir, \"00_data/horizontal_structure\")\n",
    "results_dir = os.path.join(project_dir, \"02_results\")\n",
    "plots_dir = os.path.join(project_dir, \"03_plots\")\n",
    "pca_data_dir = os.path.join(project_dir, \"00_data/pca\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "37220d0e-0f2f-447b-8342-4e50fdfd002c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Returns shape: (2610,)\n",
      "Targets shape: (2610,)\n"
     ]
    }
   ],
   "source": [
    "stock = \"AAPL\"\n",
    "period = \"10y\"\n",
    "data_type = \"single_name\"\n",
    "\n",
    "\n",
    "df = pd.read_csv(os.path.join(clean_data_dir, f\"{data_type}/{stock}/{period}_data.csv\"), parse_dates=[\"Date\"])\n",
    "\n",
    "# Sort by date if not already sorted\n",
    "df = df.sort_values(\"Date\").reset_index(drop=True)\n",
    "\n",
    "# Extract raw arrays\n",
    "returns = df[\"Return\"].values  # shape: (N,)\n",
    "targets = df[\"Target\"].values  # shape: (N,)\n",
    "\n",
    "# Optional: if your \"Target\" is sometimes -1/+1, you might map it to 0/1:\n",
    "# targets = np.where(targets == -1, 0, 1)\n",
    "\n",
    "# Check shapes\n",
    "print(\"Returns shape:\", returns.shape)\n",
    "print(\"Targets shape:\", targets.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "725a7165-b3af-4403-8ab9-f1540f3cda94",
   "metadata": {},
   "source": [
    "**Create sequences**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b937a612-a947-456e-8f7f-38a799d8d78a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequences(feature_array, target_array, seq_length=5):\n",
    "    \"\"\"\n",
    "    Transforms a 1D feature array and target array into\n",
    "    LSTM-friendly sequences of shape [batch, seq_length, 1].\n",
    "    The label is the value at the (i+seq_length)-th position in target_array.\n",
    "    \"\"\"\n",
    "    X, y = [], []\n",
    "    for i in range(len(feature_array) - seq_length):\n",
    "        X.append(feature_array[i : i + seq_length])        # chunk of length seq_length\n",
    "        y.append(target_array[i + seq_length])             # next pointâ€™s label\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "    \n",
    "    # Reshape X to [batch, seq_length, 1] for an LSTM with single feature\n",
    "    X = X.reshape(X.shape[0], X.shape[1], 1)\n",
    "    return X, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c80cdc2d-017c-4ae2-96cc-00307218547a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleLSTM(nn.Module):\n",
    "    def __init__(self, input_dim=1, hidden_dim=16, num_layers=1):\n",
    "        super(SimpleLSTM, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.fc = nn.Linear(hidden_dim, 1)  # For binary classification -> single output node\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x shape: [batch, seq_length, input_dim]\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).to(x.device)\n",
    "        \n",
    "        out, _ = self.lstm(x, (h0, c0))    # out shape: [batch, seq_length, hidden_dim]\n",
    "        out = out[:, -1, :]               # take the last time step\n",
    "        out = self.fc(out)                # shape: [batch, 1]\n",
    "        return torch.sigmoid(out).squeeze()  # shape: [batch], squashed to (0,1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "76c19b68-bd55-472e-aff5-9674e8b5b099",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 1) Your rolling evaluation function (unchanged from your snippet)\n",
    "#    Just make sure you have it defined in your code.\n",
    "# -------------------------------------------------------------------\n",
    "def evaluate_rolling_unchanged_model_threshold(\n",
    "    model, \n",
    "    X, \n",
    "    y, \n",
    "    criterion, \n",
    "    optimizer, \n",
    "    device, \n",
    "    train_size, \n",
    "    batch_size, \n",
    "    num_epochs, \n",
    "    lower_threshold,\n",
    "    upper_threshold\n",
    "):\n",
    "    \"\"\"\n",
    "    Evaluate a PyTorch model using a rolling prediction approach for time series,\n",
    "    training the model only once on the initial training set. For each time step\n",
    "    after train_size, the model makes a prediction without further parameter updates.\n",
    "    Only predicts +1 or -1 if the probability of class 1 is above/below given thresholds;\n",
    "    otherwise, predicts 0. Accuracy is computed only on nonzero predictions.\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    import torch\n",
    "\n",
    "    # Convert X, y to tensors\n",
    "    X = torch.tensor(X, dtype=torch.float32)\n",
    "    y = torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "    # Determine initial training set size\n",
    "    if isinstance(train_size, float) and train_size < 1.0:\n",
    "        lower_bound = int(train_size * len(X))\n",
    "    else:\n",
    "        lower_bound = train_size\n",
    "\n",
    "    # -------------------------\n",
    "    # 1) SINGLE TRAINING PHASE\n",
    "    # -------------------------\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "    \n",
    "    X_train = X[:lower_bound].to(device)\n",
    "    y_train = y[:lower_bound].to(device)\n",
    "\n",
    "    train_dataset = TensorDataset(X_train, y_train)\n",
    "    trainloader = DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size=batch_size, \n",
    "        shuffle=False,\n",
    "        drop_last=False\n",
    "    )\n",
    "\n",
    "    epoch_train_losses = []\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_loss = 0.0\n",
    "        for X_batch, y_batch in trainloader:\n",
    "            X_batch = X_batch.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            pred_y = model(X_batch)   # [batch_size, num_classes]\n",
    "            loss = criterion(pred_y, y_batch)\n",
    "            loss.backward()\n",
    "\n",
    "            # Gradient clipping (optional)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "               \n",
    "        epoch_train_losses.append(epoch_loss/len(trainloader))\n",
    "        \n",
    "        if (epoch + 1) % 5 == 0 or epoch == num_epochs - 1:\n",
    "            print(f\"[Train] Epoch {epoch+1}/{num_epochs}, Loss={epoch_train_losses[-1]:.4f}\")\n",
    "\n",
    "    # Just an extra diagnostic: how the loss changed from first to last epoch\n",
    "    loss_decrease_percentage = ((epoch_train_losses[-1] - epoch_train_losses[0]) \n",
    "                                / epoch_train_losses[0]) * 100\n",
    "\n",
    "    # ---------------------------------\n",
    "    # 2) ROLLING PREDICTIONS, NO UPDATE\n",
    "    # ---------------------------------\n",
    "    model.eval()\n",
    "    rolling_predictions = []\n",
    "    rolling_targets     = []\n",
    "\n",
    "    for i in range(lower_bound, len(X)):\n",
    "        # Single-step test sample\n",
    "        X_test = X[i:i+1].to(device)  # shape: (1, num_features)\n",
    "        y_test = y[i:i+1].to(device)  # shape: (1, )\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # Forward pass\n",
    "            pred_y = model(X_test)  # [1, num_classes]\n",
    "            probabilities = torch.softmax(pred_y, dim=1).cpu().numpy()  # shape: (1, 2)\n",
    "            prob_class_1  = probabilities[:, 1]  # shape: (1,)\n",
    "\n",
    "            # Threshold-based logic\n",
    "            # Initialize prediction to 0\n",
    "            pred_class = 0\n",
    "            if prob_class_1 < lower_threshold:\n",
    "                pred_class = -1\n",
    "            elif prob_class_1 > upper_threshold:\n",
    "                pred_class = 1\n",
    "\n",
    "        rolling_predictions.append(pred_class)\n",
    "        rolling_targets.append(y_test.item())\n",
    "\n",
    "    rolling_predictions = np.array(rolling_predictions)\n",
    "    rolling_targets = np.array(rolling_targets).astype(int)\n",
    "\n",
    "    # Example: if your target data is in {0, 1}, you might not need to do this.\n",
    "    # If your target data is in {0, 1} but you want to interpret 0 as -1:\n",
    "    rolling_targets[rolling_targets == 0] = -1\n",
    "\n",
    "    # Filter out zero predictions\n",
    "    nonzero_mask = rolling_predictions != 0\n",
    "    filtered_preds = rolling_predictions[nonzero_mask]\n",
    "    filtered_targets = rolling_targets[nonzero_mask]\n",
    "\n",
    "    if len(filtered_preds) == 0:\n",
    "        accuracy_nonzero = None\n",
    "        print(\"No nonzero predictions, cannot compute thresholded accuracy.\")\n",
    "    else:\n",
    "        accuracy_nonzero = accuracy_score(filtered_targets, filtered_preds)\n",
    "        print(f\"Accuracy on Nonzero Predictions: {accuracy_nonzero:.4f}\")\n",
    "\n",
    "    return {\n",
    "        \"rolling_predictions\": rolling_predictions,\n",
    "        \"rolling_targets\": rolling_targets,\n",
    "        \"filtered_predictions\": filtered_preds,\n",
    "        \"filtered_targets\": filtered_targets,\n",
    "        \"accuracy_nonzero\": accuracy_nonzero,\n",
    "        \"loss_decrease_percentage\": loss_decrease_percentage\n",
    "    }\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 2) Define a model that outputs 2 classes (for CrossEntropyLoss)\n",
    "# -------------------------------------------------------------------\n",
    "class SimpleLSTMClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    A simple single-feature LSTM that outputs 2 logits for classification.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim=1, hidden_dim=16, num_layers=1):\n",
    "        super(SimpleLSTMClassifier, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # We'll treat the input as (batch_size, 1 feature) => reshape as (batch_size, seq_len=1, input_dim=1)\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True\n",
    "        )\n",
    "        # For 2-class classification, output dimension = 2\n",
    "        self.fc = nn.Linear(hidden_dim, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: [batch_size, input_dim] or [batch_size, num_features]\n",
    "        # We treat each sample as a \"sequence\" of length 1.\n",
    "        # If you truly have multiple timesteps, adapt accordingly.\n",
    "        x = x.unsqueeze(1)  # => [batch_size, seq_len=1, input_dim]\n",
    "        \n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim, device=x.device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim, device=x.device)\n",
    "        \n",
    "        out, _ = self.lstm(x, (h0, c0))       # out shape: [batch_size, 1, hidden_dim]\n",
    "        out = out[:, -1, :]                   # => [batch_size, hidden_dim]\n",
    "        out = self.fc(out)                    # => [batch_size, 2] (logits for 2 classes)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7656cbc1-1f84-4771-be98-82ad6098b065",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "LSTM: Expected input to be 2D or 3D, got 4D instead",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [30], line 36\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# Probability thresholds:\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m#   If prob_class_1 < lower_threshold => predict -1\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m#   If prob_class_1 > upper_threshold => predict +1\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m#   Else => predict 0\u001b[39;00m\n\u001b[1;32m     34\u001b[0m lower_threshold \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.5\u001b[39m\n\u001b[0;32m---> 36\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_rolling_unchanged_model_threshold\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_cont\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_cont\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlower_threshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlower_threshold\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[43m    \u001b[49m\u001b[43mupper_threshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.5\u001b[39;49m\n\u001b[1;32m     48\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRolling Evaluation Results:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Nonzero Accuracy:\u001b[39m\u001b[38;5;124m\"\u001b[39m, results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maccuracy_nonzero\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "Cell \u001b[0;32mIn [26], line 71\u001b[0m, in \u001b[0;36mevaluate_rolling_unchanged_model_threshold\u001b[0;34m(model, X, y, criterion, optimizer, device, train_size, batch_size, num_epochs, lower_threshold, upper_threshold)\u001b[0m\n\u001b[1;32m     68\u001b[0m y_batch \u001b[38;5;241m=\u001b[39m y_batch\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     70\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 71\u001b[0m pred_y \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_batch\u001b[49m\u001b[43m)\u001b[49m   \u001b[38;5;66;03m# [batch_size, num_classes]\u001b[39;00m\n\u001b[1;32m     72\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(pred_y, y_batch)\n\u001b[1;32m     73\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn [26], line 178\u001b[0m, in \u001b[0;36mSimpleLSTMClassifier.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    175\u001b[0m h0 \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers, x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden_dim, device\u001b[38;5;241m=\u001b[39mx\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    176\u001b[0m c0 \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers, x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden_dim, device\u001b[38;5;241m=\u001b[39mx\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m--> 178\u001b[0m out, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mh0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc0\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m       \u001b[38;5;66;03m# out shape: [batch_size, 1, hidden_dim]\u001b[39;00m\n\u001b[1;32m    179\u001b[0m out \u001b[38;5;241m=\u001b[39m out[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :]                   \u001b[38;5;66;03m# => [batch_size, hidden_dim]\u001b[39;00m\n\u001b[1;32m    180\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc(out)                    \u001b[38;5;66;03m# => [batch_size, 2] (logits for 2 classes)\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/rnn.py:1074\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m   1072\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1073\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m):\n\u001b[0;32m-> 1074\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1075\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLSTM: Expected input to be 2D or 3D, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdim()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124mD instead\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1076\u001b[0m         )\n\u001b[1;32m   1077\u001b[0m     is_batched \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m\n\u001b[1;32m   1078\u001b[0m     batch_dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_first \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[0;31mValueError\u001b[0m: LSTM: Expected input to be 2D or 3D, got 4D instead"
     ]
    }
   ],
   "source": [
    "# -------------------------------------------------------------------\n",
    "# 3) Example usage with single-feature \"returns\" data\n",
    "# -------------------------------------------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    X_cont, y_cont = create_sequences(returns, targets, seq_length=seq_length)\n",
    "\n",
    "    # -- 4B) Train/validation split --\n",
    "    X_train_c, X_val_c, y_train_c, y_val_c = train_test_split(\n",
    "        X_cont, y_cont, test_size=0.2, shuffle=False\n",
    "    )\n",
    "\n",
    "    ################################\n",
    "    # B) Instantiate model, loss, optimizer\n",
    "    ################################\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model = SimpleLSTMClassifier(input_dim=1, hidden_dim=16, num_layers=1)\n",
    "    criterion = nn.CrossEntropyLoss()  # because we have 2 logits\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.007)\n",
    "\n",
    "    ################################\n",
    "    # C) Rolling evaluation\n",
    "    ################################\n",
    "    # Example: Use first 70% to train, then roll.\n",
    "    # We'll let the function do all the training in one block:\n",
    "    train_size = 0.7  \n",
    "    batch_size = 32\n",
    "    num_epochs = 50\n",
    "\n",
    "    # Probability thresholds:\n",
    "    #   If prob_class_1 < lower_threshold => predict -1\n",
    "    #   If prob_class_1 > upper_threshold => predict +1\n",
    "    #   Else => predict 0\n",
    "    lower_threshold = 0.5\n",
    "\n",
    "    results = evaluate_rolling_unchanged_model_threshold(\n",
    "        model=model,\n",
    "        X=X_cont,\n",
    "        y=y_cont,\n",
    "        criterion=criterion,\n",
    "        optimizer=optimizer,\n",
    "        device=device,\n",
    "        train_size=train_size,\n",
    "        batch_size=batch_size,\n",
    "        num_epochs=num_epochs,\n",
    "        lower_threshold=lower_threshold,\n",
    "        upper_threshold=0.5\n",
    "    )\n",
    "\n",
    "    print(\"Rolling Evaluation Results:\")\n",
    "    print(\" Nonzero Accuracy:\", results[\"accuracy_nonzero\"])\n",
    "    print(\" Loss Decrease %: \", results[\"loss_decrease_percentage\"])\n",
    "    print(\" Rolling Predictions:\", results[\"rolling_predictions\"])\n",
    "    print(\" Rolling Targets:    \", results[\"rolling_targets\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9adf5f02-8a9e-4a04-a274-7c940e8d90a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100 | Train Loss: 0.6933 | Val Loss: 0.6927 | Val Acc: 0.5202\n",
      "Epoch 2/100 | Train Loss: 0.6933 | Val Loss: 0.6927 | Val Acc: 0.5202\n",
      "Epoch 3/100 | Train Loss: 0.6933 | Val Loss: 0.6927 | Val Acc: 0.5202\n",
      "Epoch 4/100 | Train Loss: 0.6933 | Val Loss: 0.6927 | Val Acc: 0.5202\n",
      "Epoch 5/100 | Train Loss: 0.6933 | Val Loss: 0.6927 | Val Acc: 0.5202\n",
      "Epoch 6/100 | Train Loss: 0.6932 | Val Loss: 0.6927 | Val Acc: 0.5202\n",
      "Epoch 7/100 | Train Loss: 0.6932 | Val Loss: 0.6927 | Val Acc: 0.5202\n",
      "Epoch 8/100 | Train Loss: 0.6932 | Val Loss: 0.6927 | Val Acc: 0.5202\n",
      "Epoch 9/100 | Train Loss: 0.6932 | Val Loss: 0.6927 | Val Acc: 0.5202\n",
      "Epoch 10/100 | Train Loss: 0.6932 | Val Loss: 0.6927 | Val Acc: 0.5202\n",
      "Epoch 11/100 | Train Loss: 0.6932 | Val Loss: 0.6927 | Val Acc: 0.5202\n",
      "Epoch 12/100 | Train Loss: 0.6932 | Val Loss: 0.6927 | Val Acc: 0.5202\n",
      "Epoch 13/100 | Train Loss: 0.6932 | Val Loss: 0.6927 | Val Acc: 0.5202\n",
      "Epoch 14/100 | Train Loss: 0.6932 | Val Loss: 0.6927 | Val Acc: 0.5202\n",
      "Epoch 15/100 | Train Loss: 0.6932 | Val Loss: 0.6927 | Val Acc: 0.5202\n",
      "Epoch 16/100 | Train Loss: 0.6932 | Val Loss: 0.6927 | Val Acc: 0.5202\n",
      "Epoch 17/100 | Train Loss: 0.6932 | Val Loss: 0.6927 | Val Acc: 0.5202\n",
      "Epoch 18/100 | Train Loss: 0.6932 | Val Loss: 0.6927 | Val Acc: 0.5202\n",
      "Epoch 19/100 | Train Loss: 0.6932 | Val Loss: 0.6927 | Val Acc: 0.5202\n",
      "Epoch 20/100 | Train Loss: 0.6932 | Val Loss: 0.6927 | Val Acc: 0.5202\n",
      "Epoch 21/100 | Train Loss: 0.6931 | Val Loss: 0.6927 | Val Acc: 0.5202\n",
      "Epoch 22/100 | Train Loss: 0.6931 | Val Loss: 0.6927 | Val Acc: 0.5202\n",
      "Epoch 23/100 | Train Loss: 0.6931 | Val Loss: 0.6927 | Val Acc: 0.5202\n",
      "Epoch 24/100 | Train Loss: 0.6931 | Val Loss: 0.6927 | Val Acc: 0.5202\n",
      "Epoch 25/100 | Train Loss: 0.6931 | Val Loss: 0.6927 | Val Acc: 0.5202\n",
      "Epoch 26/100 | Train Loss: 0.6931 | Val Loss: 0.6927 | Val Acc: 0.5202\n",
      "Epoch 27/100 | Train Loss: 0.6931 | Val Loss: 0.6927 | Val Acc: 0.5202\n",
      "Epoch 28/100 | Train Loss: 0.6931 | Val Loss: 0.6927 | Val Acc: 0.5202\n",
      "Epoch 29/100 | Train Loss: 0.6931 | Val Loss: 0.6927 | Val Acc: 0.5240\n",
      "Epoch 30/100 | Train Loss: 0.6931 | Val Loss: 0.6927 | Val Acc: 0.5182\n",
      "Epoch 31/100 | Train Loss: 0.6931 | Val Loss: 0.6927 | Val Acc: 0.5182\n",
      "Epoch 32/100 | Train Loss: 0.6931 | Val Loss: 0.6927 | Val Acc: 0.5182\n",
      "Epoch 33/100 | Train Loss: 0.6931 | Val Loss: 0.6927 | Val Acc: 0.5106\n",
      "Epoch 34/100 | Train Loss: 0.6931 | Val Loss: 0.6927 | Val Acc: 0.5106\n",
      "Epoch 35/100 | Train Loss: 0.6931 | Val Loss: 0.6927 | Val Acc: 0.5010\n",
      "Epoch 36/100 | Train Loss: 0.6931 | Val Loss: 0.6927 | Val Acc: 0.5278\n",
      "Epoch 37/100 | Train Loss: 0.6931 | Val Loss: 0.6927 | Val Acc: 0.5278\n",
      "Epoch 38/100 | Train Loss: 0.6930 | Val Loss: 0.6927 | Val Acc: 0.5278\n",
      "Epoch 39/100 | Train Loss: 0.6930 | Val Loss: 0.6927 | Val Acc: 0.5278\n",
      "Epoch 40/100 | Train Loss: 0.6930 | Val Loss: 0.6927 | Val Acc: 0.5278\n",
      "Epoch 41/100 | Train Loss: 0.6930 | Val Loss: 0.6927 | Val Acc: 0.5182\n",
      "Epoch 42/100 | Train Loss: 0.6930 | Val Loss: 0.6927 | Val Acc: 0.5182\n",
      "Epoch 43/100 | Train Loss: 0.6930 | Val Loss: 0.6927 | Val Acc: 0.5182\n",
      "Epoch 44/100 | Train Loss: 0.6930 | Val Loss: 0.6927 | Val Acc: 0.5240\n",
      "Epoch 45/100 | Train Loss: 0.6930 | Val Loss: 0.6927 | Val Acc: 0.5240\n",
      "Epoch 46/100 | Train Loss: 0.6930 | Val Loss: 0.6927 | Val Acc: 0.5240\n",
      "Epoch 47/100 | Train Loss: 0.6930 | Val Loss: 0.6927 | Val Acc: 0.5240\n",
      "Epoch 48/100 | Train Loss: 0.6930 | Val Loss: 0.6927 | Val Acc: 0.5240\n",
      "Epoch 49/100 | Train Loss: 0.6930 | Val Loss: 0.6927 | Val Acc: 0.5240\n",
      "Epoch 50/100 | Train Loss: 0.6930 | Val Loss: 0.6927 | Val Acc: 0.5240\n",
      "Epoch 51/100 | Train Loss: 0.6930 | Val Loss: 0.6927 | Val Acc: 0.5240\n",
      "Epoch 52/100 | Train Loss: 0.6930 | Val Loss: 0.6927 | Val Acc: 0.5336\n",
      "Epoch 53/100 | Train Loss: 0.6930 | Val Loss: 0.6927 | Val Acc: 0.5336\n",
      "Epoch 54/100 | Train Loss: 0.6930 | Val Loss: 0.6927 | Val Acc: 0.5336\n",
      "Epoch 55/100 | Train Loss: 0.6929 | Val Loss: 0.6927 | Val Acc: 0.5336\n",
      "Epoch 56/100 | Train Loss: 0.6929 | Val Loss: 0.6927 | Val Acc: 0.5336\n",
      "Epoch 57/100 | Train Loss: 0.6929 | Val Loss: 0.6927 | Val Acc: 0.5336\n",
      "Epoch 58/100 | Train Loss: 0.6929 | Val Loss: 0.6927 | Val Acc: 0.5336\n",
      "Epoch 59/100 | Train Loss: 0.6929 | Val Loss: 0.6927 | Val Acc: 0.5336\n",
      "Epoch 60/100 | Train Loss: 0.6929 | Val Loss: 0.6927 | Val Acc: 0.5336\n",
      "Epoch 61/100 | Train Loss: 0.6929 | Val Loss: 0.6927 | Val Acc: 0.5336\n",
      "Epoch 62/100 | Train Loss: 0.6929 | Val Loss: 0.6927 | Val Acc: 0.5336\n",
      "Epoch 63/100 | Train Loss: 0.6929 | Val Loss: 0.6928 | Val Acc: 0.5336\n",
      "Epoch 64/100 | Train Loss: 0.6929 | Val Loss: 0.6928 | Val Acc: 0.5336\n",
      "Epoch 65/100 | Train Loss: 0.6928 | Val Loss: 0.6928 | Val Acc: 0.5336\n",
      "Epoch 66/100 | Train Loss: 0.6928 | Val Loss: 0.6928 | Val Acc: 0.5336\n",
      "Epoch 67/100 | Train Loss: 0.6928 | Val Loss: 0.6928 | Val Acc: 0.5336\n",
      "Epoch 68/100 | Train Loss: 0.6928 | Val Loss: 0.6928 | Val Acc: 0.5336\n",
      "Epoch 69/100 | Train Loss: 0.6928 | Val Loss: 0.6928 | Val Acc: 0.5336\n",
      "Epoch 70/100 | Train Loss: 0.6928 | Val Loss: 0.6928 | Val Acc: 0.5317\n",
      "Epoch 71/100 | Train Loss: 0.6928 | Val Loss: 0.6929 | Val Acc: 0.5317\n",
      "Epoch 72/100 | Train Loss: 0.6927 | Val Loss: 0.6929 | Val Acc: 0.5317\n",
      "Epoch 73/100 | Train Loss: 0.6927 | Val Loss: 0.6929 | Val Acc: 0.5298\n",
      "Epoch 74/100 | Train Loss: 0.6927 | Val Loss: 0.6929 | Val Acc: 0.5298\n",
      "Epoch 75/100 | Train Loss: 0.6927 | Val Loss: 0.6930 | Val Acc: 0.5298\n",
      "Epoch 76/100 | Train Loss: 0.6927 | Val Loss: 0.6930 | Val Acc: 0.5298\n",
      "Epoch 77/100 | Train Loss: 0.6927 | Val Loss: 0.6930 | Val Acc: 0.5298\n",
      "Epoch 78/100 | Train Loss: 0.6926 | Val Loss: 0.6930 | Val Acc: 0.5298\n",
      "Epoch 79/100 | Train Loss: 0.6926 | Val Loss: 0.6931 | Val Acc: 0.5298\n",
      "Epoch 80/100 | Train Loss: 0.6926 | Val Loss: 0.6931 | Val Acc: 0.5298\n",
      "Epoch 81/100 | Train Loss: 0.6926 | Val Loss: 0.6931 | Val Acc: 0.5298\n",
      "Epoch 82/100 | Train Loss: 0.6926 | Val Loss: 0.6932 | Val Acc: 0.5298\n",
      "Epoch 83/100 | Train Loss: 0.6926 | Val Loss: 0.6932 | Val Acc: 0.5298\n",
      "Epoch 84/100 | Train Loss: 0.6926 | Val Loss: 0.6932 | Val Acc: 0.5298\n",
      "Epoch 85/100 | Train Loss: 0.6925 | Val Loss: 0.6932 | Val Acc: 0.5393\n",
      "Epoch 86/100 | Train Loss: 0.6925 | Val Loss: 0.6933 | Val Acc: 0.5393\n",
      "Epoch 87/100 | Train Loss: 0.6925 | Val Loss: 0.6933 | Val Acc: 0.5393\n",
      "Epoch 88/100 | Train Loss: 0.6925 | Val Loss: 0.6933 | Val Acc: 0.5393\n",
      "Epoch 89/100 | Train Loss: 0.6925 | Val Loss: 0.6933 | Val Acc: 0.5393\n",
      "Epoch 90/100 | Train Loss: 0.6925 | Val Loss: 0.6934 | Val Acc: 0.5393\n",
      "Epoch 91/100 | Train Loss: 0.6925 | Val Loss: 0.6934 | Val Acc: 0.5393\n",
      "Epoch 92/100 | Train Loss: 0.6925 | Val Loss: 0.6934 | Val Acc: 0.5393\n",
      "Epoch 93/100 | Train Loss: 0.6925 | Val Loss: 0.6934 | Val Acc: 0.5182\n",
      "Epoch 94/100 | Train Loss: 0.6924 | Val Loss: 0.6935 | Val Acc: 0.5182\n",
      "Epoch 95/100 | Train Loss: 0.6924 | Val Loss: 0.6935 | Val Acc: 0.5182\n",
      "Epoch 96/100 | Train Loss: 0.6924 | Val Loss: 0.6935 | Val Acc: 0.5182\n",
      "Epoch 97/100 | Train Loss: 0.6924 | Val Loss: 0.6935 | Val Acc: 0.5182\n",
      "Epoch 98/100 | Train Loss: 0.6924 | Val Loss: 0.6935 | Val Acc: 0.5182\n",
      "Epoch 99/100 | Train Loss: 0.6924 | Val Loss: 0.6936 | Val Acc: 0.5182\n",
      "Epoch 100/100 | Train Loss: 0.6924 | Val Loss: 0.6936 | Val Acc: 0.5182\n"
     ]
    }
   ],
   "source": [
    "# -- 5A) Convert \"Return\" to 0/1 based on sign --\n",
    "binary_returns = np.where(returns < 0, 0.0, 1.0)\n",
    "\n",
    "# -- 5B) Prepare sequences with the new binary returns data --\n",
    "X_bin, y_bin = create_sequences(binary_returns, targets, seq_length=seq_length)\n",
    "\n",
    "# -- 5C) Train/validation split --\n",
    "X_train_b, X_val_b, y_train_b, y_val_b = train_test_split(\n",
    "    X_bin, y_bin, test_size=0.2, shuffle=False\n",
    ")\n",
    "\n",
    "# -- 5D) Convert to torch tensors --\n",
    "X_train_b = torch.tensor(X_train_b, dtype=torch.float32)\n",
    "y_train_b = torch.tensor(y_train_b, dtype=torch.float32)\n",
    "X_val_b   = torch.tensor(X_val_b,   dtype=torch.float32)\n",
    "y_val_b   = torch.tensor(y_val_b,   dtype=torch.float32)\n",
    "\n",
    "# -- 5E) Instantiate another LSTM model for the binary input --\n",
    "model_bin = SimpleLSTM(input_dim=1, hidden_dim=16, num_layers=1)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model_bin.parameters(), lr=1e-3)\n",
    "\n",
    "# -- 5F) Training loop (similar to above) --\n",
    "num_epochs = 100\n",
    "batch_size = 32\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model_bin.train()\n",
    "    batch_losses = []\n",
    "    \n",
    "    # (Optionally) shuffle data\n",
    "    # idx = torch.randperm(X_train_b.size(0))\n",
    "    # X_train_b = X_train_b[idx]\n",
    "    # y_train_b = y_train_b[idx]\n",
    "    \n",
    "    for i in range(0, X_train_b.size(0), batch_size):\n",
    "        x_batch = X_train_b[i : i + batch_size]\n",
    "        y_batch = y_train_b[i : i + batch_size]\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model_bin(x_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        batch_losses.append(loss.item())\n",
    "    \n",
    "    # Validation\n",
    "    model_bin.eval()\n",
    "    with torch.no_grad():\n",
    "        val_outputs = model_bin(X_val_b)\n",
    "        val_loss = criterion(val_outputs, y_val_b).item()\n",
    "        \n",
    "        val_preds = (val_outputs >= 0.5).int().numpy()\n",
    "        val_acc   = accuracy_score(y_val_b.numpy(), val_preds)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} | \"\n",
    "          f\"Train Loss: {np.mean(batch_losses):.4f} | \"\n",
    "          f\"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8945721e-3e61-4b80-bef1-24596cdbc419",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
