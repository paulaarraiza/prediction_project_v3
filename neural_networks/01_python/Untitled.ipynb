{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "940889cf-06e0-4e8c-8efa-1eeacc97ca57",
   "metadata": {},
   "source": [
    "## **This notebook aims to compare some models and store them in a reasonable fashion**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2e46c228-21f4-4e80-a4e8-b62981ab4799",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.metrics import accuracy_score\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7a11a538-40d5-49c3-92e9-e26fdcefe083",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"  # Use GPU 0 in first notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7b259658-5fed-4231-9f06-f9db567e1d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "project_dir = \"/home/jupyter-tfg2425paula/prediction_project_v3\"\n",
    "os.chdir(project_dir)\n",
    "\n",
    "clean_data_dir = os.path.join(project_dir, \"00_data/clean\")\n",
    "horizontal_data_dir = os.path.join(project_dir, \"00_data/horizontal_structure\")\n",
    "results_dir = os.path.join(project_dir, \"02_results\")\n",
    "pca_data_dir = os.path.join(project_dir, \"00_data/pca\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7319029-ba4c-4c90-91b9-a0a02b5be9af",
   "metadata": {},
   "source": [
    "### **GRU Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a7a1485d-99bb-4358-a216-1c1c75e50ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRU3DClassifier(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers, dropout):\n",
    "        super(GRU3DClassifier, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "\n",
    "        out, _ = self.gru(x, h0)\n",
    "        out = self.fc(out[:, -1, :]) \n",
    "        # return self.sigmoid(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3061a6b4-dad3-4129-a2ed-0098e04440e4",
   "metadata": {},
   "source": [
    "### **LSTM Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "95ffa6e2-2c35-48de-8c95-b7ce86fe9dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StockPriceLSTM(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers=1, dropout=0.0):\n",
    "        super(StockPriceLSTM, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers=num_layers, \n",
    "                            batch_first=True, dropout=dropout)\n",
    "    \n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)  # Get the batch size dynamically\n",
    "\n",
    "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_dim).to(x.device)  # (num_layers, batch_size, hidden_dim)\n",
    "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_dim).to(x.device)  # (num_layers, batch_size, hidden_dim)\n",
    "        \n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "\n",
    "        out = self.fc(out[:, -1, :]) \n",
    "        # out = self.sigmoid(out)\n",
    "        return out\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df6d4b78-1492-45ea-a9af-854d8d69b8de",
   "metadata": {},
   "source": [
    "### **Set folders**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e67b679f-204f-4e15-89fe-09879ed7246c",
   "metadata": {},
   "source": [
    "Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2b80ed82-98cd-453d-94e9-b7f42c61040e",
   "metadata": {},
   "outputs": [],
   "source": [
    "processing_types = [\"clean\", \"pca\"]\n",
    "processing_types= [\"clean\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da3781cf-0f0b-4858-bff8-09f839e2da04",
   "metadata": {},
   "source": [
    "Folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a2958856-f0de-4f93-85b5-9fdd169a2a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stocks = ['AAPL', 'MSFT', 'AMZN', 'NVDA', 'SPX']\n",
    "stocks = ['AAPL']\n",
    "# types_securities = [\"single_name\", \"options\", \"technical\"]\n",
    "types_securities = [\"options\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2e929df-0399-4e41-9b2b-1863f8c215b5",
   "metadata": {},
   "source": [
    "Different files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9b853df6-38b4-4bd7-85e5-6a232f17905e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# years = [\"15y\", \"10y\", \"5y\", \"2y\"]\n",
    "years = [\"10y\"]\n",
    "# window_sizes = [5, 10, 50, 100]\n",
    "window_sizes = [5]\n",
    "# train_sizes = [80, 90, 95]\n",
    "train_sizes = [95]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f9fb2cc-d8de-4e4f-be9d-f8cb7aac8e16",
   "metadata": {},
   "source": [
    "Same file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "145e5ba2-35c7-4a71-b100-624ca6ed1ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds = [0.3, 0.35, 0.4, 0.45, 0.5]\n",
    "thresholds = [0.5]\n",
    "learning_rates = [0.005, 0.008, 0.009, 0.01]\n",
    "learning_rates = [0.01]\n",
    "num_epochs_list = [100, 200]\n",
    "num_epochs_list = [100]\n",
    "batch_sizes = [16, 32]\n",
    "batch_sizes = [16]\n",
    "prediction_thresholds = [0.35, 0.4, 0.45, 0.5]\n",
    "prediction_thresholds = [0.5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbbee35f-0147-48b0-93ef-d9687984bbd0",
   "metadata": {},
   "source": [
    "#### **Model and Hyperparameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "71861bc1-7067-47f8-b3dd-a5cef2bc0f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "hidden_size = 64  \n",
    "output_size = 2  \n",
    "num_layers = 2\n",
    "dropout = 0.2\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "60900dea-a72e-4b53-979b-aded1e725489",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_types = [\"lstm\", \"gru\"]   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d873f70-b583-4e84-9289-bc303ef0446f",
   "metadata": {},
   "source": [
    "#### **Last data modifications**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "85729e3c-f060-4dc2-a4fc-a80573fefd73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_remove_characters(df):\n",
    "\n",
    "    X = np.array([np.stack(row) for row in df.drop(columns=['Target']).values])\n",
    "    y = df['Target'].values\n",
    "\n",
    "    smote = SMOTE(random_state=42)\n",
    "    n_samples, timesteps, n_features = X.shape\n",
    "    X_flat = X.reshape((n_samples, timesteps * n_features))\n",
    "    X_flat = np.where(X_flat == 'ç', 0, X_flat)\n",
    "\n",
    "    X_resampled = X_flat.reshape((-1, timesteps, n_features))\n",
    "    \n",
    "    return X_resampled, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88656719-d0c4-4bf2-aef1-31f00b167d2a",
   "metadata": {},
   "source": [
    "### **Evaluation function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "74c49327-1c0d-4706-9943-54b795cb0687",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_rolling_unchanged_model_threshold(\n",
    "    model, \n",
    "    X, \n",
    "    y, \n",
    "    criterion, \n",
    "    optimizer, \n",
    "    device, \n",
    "    train_size, \n",
    "    batch_size, \n",
    "    num_epochs, \n",
    "    lower_threshold\n",
    "):\n",
    "    \"\"\"\n",
    "    Evaluate a PyTorch model using a rolling prediction approach for time series,\n",
    "    training the model only once on the initial training set. For each time step\n",
    "    after train_size, the model makes a prediction without further parameter updates.\n",
    "    Only predicts +1 or -1 if the probability of class 1 is above/below given thresholds;\n",
    "    otherwise, predicts 0. Accuracy is computed only on nonzero predictions.\n",
    "\n",
    "    Args:\n",
    "        model:          PyTorch model to evaluate.\n",
    "        X:              Feature data (numpy array).\n",
    "        y:              Target data (numpy array).\n",
    "        criterion:      Loss function (e.g., CrossEntropyLoss).\n",
    "        optimizer:      Optimizer (e.g., Adam).\n",
    "        device:         Device for computation (CPU or GPU).\n",
    "        train_size:     Initial size of the training data (int or float).\n",
    "                        If < 1, treated as fraction of total length.\n",
    "        batch_size:     Batch size for training.\n",
    "        num_epochs:     Number of epochs for initial training only.\n",
    "        lower_threshold: Probability threshold below which model predicts -1.\n",
    "        upper_threshold: Probability threshold above which model predicts +1.\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary with the following keys:\n",
    "            - \"rolling_predictions\": All predictions (-1, 0, +1) across the test period.\n",
    "            - \"rolling_targets\": Corresponding true targets in [-1, +1].\n",
    "            - \"filtered_predictions\": Nonzero predictions only.\n",
    "            - \"filtered_targets\": Targets corresponding to nonzero predictions.\n",
    "            - \"accuracy_nonzero\": Accuracy computed only on nonzero predictions.\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert X, y to tensors\n",
    "    X = torch.tensor(X, dtype=torch.float32)\n",
    "    y = torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "    # Determine initial training set size\n",
    "    if train_size < 1.0:\n",
    "        lower_bound = int(train_size * len(X))\n",
    "    else:\n",
    "        lower_bound = train_size\n",
    "\n",
    "    # -------------------------\n",
    "    # 1) SINGLE TRAINING PHASE\n",
    "    # -------------------------\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "    \n",
    "    X_train = X[:lower_bound].to(device)\n",
    "    y_train = y[:lower_bound].to(device)\n",
    "\n",
    "    train_dataset = TensorDataset(X_train, y_train)\n",
    "    trainloader = DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size=batch_size, \n",
    "        shuffle=False,         # Keep False if order matters; True for better generalization\n",
    "        # num_workers=4,         # Adjust based on your CPU cores\n",
    "        # pin_memory=True,       # Speeds up transfer if using GPUs\n",
    "        drop_last=False        # Ensure the last batch is included\n",
    "    )\n",
    "\n",
    "    epoch_train_losses = []\n",
    "    for epoch in range(num_epochs):\n",
    "        # torch.cuda.empty_cache()\n",
    "        epoch_loss = 0.0\n",
    "        for X_batch, y_batch in trainloader:\n",
    "            X_batch = X_batch.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            pred_y = model(X_batch)   # [batch_size, num_classes]\n",
    "            loss = criterion(pred_y, y_batch)\n",
    "            loss.backward()\n",
    "\n",
    "            # Gradient clipping (optional)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "               \n",
    "        if (epoch + 1) % 5 == 0 or epoch == num_epochs - 1:\n",
    "            print(f\"[Train] Epoch {epoch+1}/{num_epochs}, Loss={epoch_loss/len(trainloader):.4f}\")\n",
    "\n",
    "        epoch_train_losses.append(epoch_loss/len(trainloader))\n",
    "        \n",
    "    loss_decrease_percentage = ((epoch_train_losses[-1] - epoch_train_losses[0]) / epoch_train_losses[0]) * 100\n",
    "    # ---------------------------------\n",
    "    # 2) ROLLING PREDICTIONS, NO UPDATE\n",
    "    # ---------------------------------\n",
    "    model.eval()\n",
    "\n",
    "    rolling_predictions = []\n",
    "    rolling_targets     = []\n",
    "\n",
    "    for i in range(lower_bound, len(X)):\n",
    "        # Single-step \"test\" sample\n",
    "        X_test = X[i:i+1].to(device)  # shape: (1, num_features)\n",
    "        y_test = y[i:i+1].to(device)  # shape: (1, )\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # Forward pass\n",
    "            pred_y = model(X_test)  # [1, num_classes]\n",
    "            probabilities = torch.softmax(pred_y, dim=1).cpu().numpy()  # shape: (1, 2)\n",
    "            prob_class_1  = probabilities[:, 1]  # shape: (1,)\n",
    "\n",
    "            # Threshold-based logic\n",
    "            # Initialize all predictions to 0\n",
    "            pred_classes = np.zeros_like(prob_class_1)\n",
    "            # Predict -1 if prob < lower_threshold\n",
    "            pred_classes[prob_class_1 < lower_threshold] = -1\n",
    "            # Predict +1 if prob > upper_threshold\n",
    "            pred_classes[prob_class_1 > 1-lower_threshold] = 1\n",
    "\n",
    "        rolling_predictions.append(pred_classes[0])  # scalar\n",
    "        rolling_targets.append(y_test.item())\n",
    "\n",
    "    rolling_predictions = np.array(rolling_predictions)\n",
    "    rolling_targets = np.array(rolling_targets).astype(int)\n",
    "\n",
    "    # Convert any 0-labeled targets to -1 if your original data is in [-1, +1]\n",
    "    # (Sometimes y might be {0,1} or {-1, +1}; adapt as needed.)\n",
    "    rolling_targets[rolling_targets == 0] = -1\n",
    "\n",
    "    # Filter out zero predictions\n",
    "    nonzero_mask = rolling_predictions != 0\n",
    "    filtered_preds = rolling_predictions[nonzero_mask]\n",
    "    filtered_targets = rolling_targets[nonzero_mask]\n",
    "\n",
    "    if len(filtered_preds) == 0:\n",
    "        accuracy_nonzero = None\n",
    "        print(\"No nonzero predictions, cannot compute thresholded accuracy.\")\n",
    "    else:\n",
    "        accuracy_nonzero = accuracy_score(filtered_targets, filtered_preds)\n",
    "        print(f\"Accuracy on Nonzero Predictions: {accuracy_nonzero:.4f}\")\n",
    "\n",
    "    return {\n",
    "        \"rolling_predictions\": rolling_predictions,\n",
    "        \"rolling_targets\": rolling_targets,\n",
    "        \"filtered_predictions\": filtered_preds,\n",
    "        \"filtered_targets\": filtered_targets,\n",
    "        \"accuracy_nonzero\": accuracy_nonzero,\n",
    "        \"loss_decrease_percentage\": loss_decrease_percentage\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b794e699-83fb-4ca2-8c93-318858018a2e",
   "metadata": {},
   "source": [
    "### **1st Type of comparison:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1ebcf636-0ba3-45b9-b37d-5a075fefc803",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AAPL, options, 15y, 95, 5\n",
      "/home/jupyter-tfg2425paula/prediction_project_v3/00_data/horizontal_structure/clean/options/AAPL/15y_5_data.pkl\n",
      "Training AAPL | LR: 0.01 | Epochs: 100 | Batch: 16 | Prediction Threshold: 0.5\n",
      "[Train] Epoch 5/100, Loss=0.6923\n",
      "[Train] Epoch 10/100, Loss=0.6754\n",
      "[Train] Epoch 15/100, Loss=0.6125\n",
      "[Train] Epoch 20/100, Loss=0.5152\n",
      "[Train] Epoch 25/100, Loss=0.4226\n",
      "[Train] Epoch 30/100, Loss=0.3315\n",
      "[Train] Epoch 35/100, Loss=0.2615\n",
      "[Train] Epoch 40/100, Loss=0.2094\n",
      "[Train] Epoch 45/100, Loss=0.1780\n",
      "[Train] Epoch 50/100, Loss=0.1432\n",
      "[Train] Epoch 55/100, Loss=0.1050\n",
      "[Train] Epoch 60/100, Loss=0.1082\n",
      "[Train] Epoch 65/100, Loss=0.1127\n",
      "[Train] Epoch 70/100, Loss=0.1077\n",
      "[Train] Epoch 75/100, Loss=0.0826\n",
      "[Train] Epoch 80/100, Loss=0.0970\n",
      "[Train] Epoch 85/100, Loss=0.0864\n",
      "[Train] Epoch 90/100, Loss=0.0659\n",
      "[Train] Epoch 95/100, Loss=0.0648\n",
      "[Train] Epoch 100/100, Loss=0.1268\n",
      "Accuracy on Nonzero Predictions: 0.5408\n"
     ]
    }
   ],
   "source": [
    "# folder\n",
    "results_list = []\n",
    "stock = \"AAPL\"\n",
    "period = \"10y\"\n",
    "possible_train_size = 95\n",
    "for processing in processing_types:\n",
    "    initial_data_dir = os.path.join(project_dir, f\"00_data/{processing}\")\n",
    "    for model_type in model_types:\n",
    "        for security_type in types_securities:\n",
    "            output_folder = os.path.join(results_dir, f\"{model_type}/{stock}/{security_type}\") \n",
    "            os.makedirs(output_folder, exist_ok=True)\n",
    "            # files\n",
    "            # load original data as well (for info purposes)\n",
    "            \n",
    "            filename = f\"{security_type}/{stock}/{period}_data.csv\"\n",
    "            original_input_filepath = os.path.join(initial_data_dir, filename)\n",
    "            original_data = pd.read_csv(original_input_filepath)\n",
    "            start_date = original_data.loc[0, \"Date\"]\n",
    "            end_date = original_data.iloc[-1][\"Date\"]\n",
    "            results_csv_path = os.path.join(output_folder, f\"{period}_{possible_train_size}.csv\")\n",
    "\n",
    "            # columns, same file\n",
    "            for window_size in window_sizes:\n",
    "                print(f\"{stock}, {security_type}, {period}, {possible_train_size}, {window_size}\")\n",
    "\n",
    "                # load data\n",
    "                pkl_filename = f\"clean/{security_type}/{stock}/{period}_{window_size}_data.pkl\"\n",
    "                input_filepath = os.path.join(horizontal_data_dir, pkl_filename)\n",
    "                print(input_filepath)\n",
    "                input_df = pd.read_pickle(input_filepath)\n",
    "\n",
    "                X_resampled, y_resampled = reshape_remove_characters(input_df)\n",
    "\n",
    "                input_size = X_resampled.shape[2]\n",
    "                train_size = int(X_resampled.shape[0]*possible_train_size/100)\n",
    "                test_size = X_resampled.shape[0] - train_size\n",
    "\n",
    "                # generate model\n",
    "                if model_type == \"gru\":\n",
    "                    model = GRU3DClassifier(input_size, hidden_size, output_size, num_layers, dropout)\n",
    "                elif model_type == \"lstm\":\n",
    "                    model = StockPriceLSTM(input_size, hidden_size, output_size)\n",
    "\n",
    "                model = model.to(device)\n",
    "\n",
    "                for learning_rate in learning_rates:\n",
    "\n",
    "                    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "                    for num_epochs in num_epochs_list:\n",
    "                        for prediction_threshold in prediction_thresholds:\n",
    "                            for batch_size in batch_sizes:\n",
    "\n",
    "                                print(f\"Training {stock} | LR: {learning_rate} | Epochs: {num_epochs} | Batch: {batch_size} | Prediction Threshold: {prediction_threshold}\")\n",
    "\n",
    "                                start_time = time.time()\n",
    "\n",
    "                                result = evaluate_rolling_unchanged_model_threshold(\n",
    "                                    model, X_resampled, y_resampled, criterion, \n",
    "                                                               optimizer, device, train_size, batch_size, num_epochs, lower_threshold=prediction_threshold)     \n",
    "\n",
    "                                rolling_predictions = result[\"rolling_predictions\"]\n",
    "                                rolling_targets = result[\"rolling_targets\"]\n",
    "                                test_accuracy = result[\"accuracy_nonzero\"]\n",
    "                                loss_decrease_percentage = result[\"loss_decrease_percentage\"]\n",
    "                                nonzero_preds = np.count_nonzero(result[\"rolling_predictions\"])\n",
    "\n",
    "\n",
    "                                end_time = time.time()    \n",
    "                                execution_time = end_time - start_time\n",
    "\n",
    "                                # --------------------------------------------\n",
    "                                # 1) Create a record (dictionary) for this run\n",
    "                                # --------------------------------------------\n",
    "                                run_record = {\n",
    "                                    \"start_date\": start_date,\n",
    "                                    \"end_date\": end_date,\n",
    "                                    \"execution_time\": execution_time,\n",
    "                                    \"test_size\": test_size,\n",
    "                                    \"nonzero_preds\": nonzero_preds,\n",
    "                                    \"accuracy\": test_accuracy,\n",
    "                                    \"prediction_threshold\": prediction_threshold,\n",
    "\n",
    "                                    \"window_size\": window_size,\n",
    "                                    \"learning_rate\": learning_rate,\n",
    "                                    \"num_epochs\": num_epochs,\n",
    "                                    \"train_loss_change_pctg\": loss_decrease_percentage,\n",
    "\n",
    "                                    \"batch_size\": batch_size,\n",
    "\n",
    "                                    \"output_size\": output_size,\n",
    "                                    \"hidden_size\": hidden_size,\n",
    "                                    \"num_layers\": num_layers,\n",
    "                                    \"dropout_rate\": dropout,\n",
    "                                    \"optimizer\": optimizer.__class__.__name__,\n",
    "                                    \"criterion\": criterion\n",
    "\n",
    "                                }\n",
    "\n",
    "                                # --------------------------------------------\n",
    "                                # 2) Append the dictionary to the results list\n",
    "                                # --------------------------------------------\n",
    "                                results_list.append(run_record)\n",
    "\n",
    "            # ----------------------------------------------------------------\n",
    "            # 3) Write to CSV *once* after all window_sizes for this setup\n",
    "            # ----------------------------------------------------------------\n",
    "            if len(results_list) > 0:\n",
    "                df = pd.DataFrame(results_list)\n",
    "\n",
    "                # If CSV already exists, append without header\n",
    "                if os.path.exists(results_csv_path):\n",
    "                    df.to_csv(results_csv_path, mode='a', header=False, index=False)\n",
    "                else:\n",
    "                    df.to_csv(results_csv_path, index=False)\n",
    "\n",
    "                # Clear results_list to avoid duplication\n",
    "                results_list = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "672857df-0a56-49c8-a1f9-8972057d026b",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clean, lstm, technical\n",
      "/home/jupyter-tfg2425paula/prediction_project_v3/00_data/horizontal_structure/clean/technical/AAPL/10y_5_data.pkl\n",
      "Training AAPL | LR: 0.009 | Epochs: 100 | Batch: 32 | Prediction Threshold: 0.5\n",
      "[Train] Epoch 5/100, Loss=0.6928\n",
      "[Train] Epoch 10/100, Loss=0.6901\n",
      "[Train] Epoch 15/100, Loss=0.6520\n",
      "[Train] Epoch 20/100, Loss=0.5889\n",
      "[Train] Epoch 25/100, Loss=0.5191\n",
      "[Train] Epoch 30/100, Loss=0.3657\n",
      "[Train] Epoch 35/100, Loss=0.3002\n",
      "[Train] Epoch 40/100, Loss=0.2570\n",
      "[Train] Epoch 45/100, Loss=0.1730\n",
      "[Train] Epoch 50/100, Loss=0.1429\n",
      "[Train] Epoch 55/100, Loss=0.1172\n",
      "[Train] Epoch 60/100, Loss=0.1050\n",
      "[Train] Epoch 65/100, Loss=0.0820\n",
      "[Train] Epoch 70/100, Loss=0.0898\n",
      "[Train] Epoch 75/100, Loss=0.1006\n",
      "[Train] Epoch 80/100, Loss=0.0774\n",
      "[Train] Epoch 85/100, Loss=0.0783\n",
      "[Train] Epoch 90/100, Loss=0.0950\n",
      "[Train] Epoch 95/100, Loss=0.0630\n",
      "[Train] Epoch 100/100, Loss=0.0670\n",
      "Accuracy on Nonzero Predictions: 0.4921\n",
      "Training AAPL | LR: 0.01 | Epochs: 100 | Batch: 32 | Prediction Threshold: 0.5\n",
      "[Train] Epoch 5/100, Loss=0.1029\n",
      "[Train] Epoch 10/100, Loss=0.0730\n",
      "[Train] Epoch 15/100, Loss=0.1514\n",
      "[Train] Epoch 20/100, Loss=0.0566\n",
      "[Train] Epoch 25/100, Loss=0.1458\n",
      "[Train] Epoch 30/100, Loss=0.1038\n",
      "[Train] Epoch 35/100, Loss=0.0570\n",
      "[Train] Epoch 40/100, Loss=0.1151\n",
      "[Train] Epoch 45/100, Loss=0.0793\n",
      "[Train] Epoch 50/100, Loss=0.0768\n",
      "[Train] Epoch 55/100, Loss=0.0902\n",
      "[Train] Epoch 60/100, Loss=0.0657\n",
      "[Train] Epoch 65/100, Loss=0.0761\n",
      "[Train] Epoch 70/100, Loss=0.0474\n",
      "[Train] Epoch 75/100, Loss=0.0728\n",
      "[Train] Epoch 80/100, Loss=0.0589\n",
      "[Train] Epoch 85/100, Loss=0.1056\n",
      "[Train] Epoch 90/100, Loss=0.1505\n",
      "[Train] Epoch 95/100, Loss=0.0551\n",
      "[Train] Epoch 100/100, Loss=0.0846\n",
      "Accuracy on Nonzero Predictions: 0.5476\n",
      "Training AAPL | LR: 0.011 | Epochs: 100 | Batch: 32 | Prediction Threshold: 0.5\n",
      "[Train] Epoch 5/100, Loss=0.0877\n",
      "[Train] Epoch 10/100, Loss=0.1013\n",
      "[Train] Epoch 15/100, Loss=0.1094\n",
      "[Train] Epoch 20/100, Loss=0.0952\n",
      "[Train] Epoch 25/100, Loss=0.1341\n",
      "[Train] Epoch 30/100, Loss=0.0757\n",
      "[Train] Epoch 35/100, Loss=0.1062\n",
      "[Train] Epoch 40/100, Loss=0.1090\n",
      "[Train] Epoch 45/100, Loss=0.0986\n",
      "[Train] Epoch 50/100, Loss=0.1051\n",
      "[Train] Epoch 55/100, Loss=0.0829\n",
      "[Train] Epoch 60/100, Loss=0.1046\n",
      "[Train] Epoch 65/100, Loss=0.1552\n",
      "[Train] Epoch 70/100, Loss=0.1056\n",
      "[Train] Epoch 75/100, Loss=0.0795\n",
      "[Train] Epoch 80/100, Loss=0.1111\n",
      "[Train] Epoch 85/100, Loss=0.1864\n",
      "[Train] Epoch 90/100, Loss=0.0757\n",
      "[Train] Epoch 95/100, Loss=0.1281\n",
      "[Train] Epoch 100/100, Loss=0.0937\n",
      "Accuracy on Nonzero Predictions: 0.4603\n",
      "[0.49206349206349204, 0.5476190476190477, 0.4603174603174603]\n",
      "[-90.37768961221401, 25.236401231942384, -42.97745368186671]\n",
      "clean, lstm, options\n",
      "/home/jupyter-tfg2425paula/prediction_project_v3/00_data/horizontal_structure/clean/options/AAPL/10y_5_data.pkl\n",
      "Training AAPL | LR: 0.009 | Epochs: 100 | Batch: 32 | Prediction Threshold: 0.5\n",
      "[Train] Epoch 5/100, Loss=0.6891\n",
      "[Train] Epoch 10/100, Loss=0.6806\n",
      "[Train] Epoch 15/100, Loss=0.6450\n",
      "[Train] Epoch 20/100, Loss=0.5809\n",
      "[Train] Epoch 25/100, Loss=0.4545\n",
      "[Train] Epoch 30/100, Loss=0.3276\n",
      "[Train] Epoch 35/100, Loss=0.2224\n",
      "[Train] Epoch 40/100, Loss=0.1383\n",
      "[Train] Epoch 45/100, Loss=0.1042\n",
      "[Train] Epoch 50/100, Loss=0.0876\n",
      "[Train] Epoch 55/100, Loss=0.0340\n",
      "[Train] Epoch 60/100, Loss=0.0552\n",
      "[Train] Epoch 65/100, Loss=0.0608\n",
      "[Train] Epoch 70/100, Loss=0.0391\n",
      "[Train] Epoch 75/100, Loss=0.0475\n",
      "[Train] Epoch 80/100, Loss=0.0319\n",
      "[Train] Epoch 85/100, Loss=0.0128\n",
      "[Train] Epoch 90/100, Loss=0.0300\n",
      "[Train] Epoch 95/100, Loss=0.0809\n",
      "[Train] Epoch 100/100, Loss=0.0352\n",
      "Accuracy on Nonzero Predictions: 0.5191\n",
      "Training AAPL | LR: 0.01 | Epochs: 100 | Batch: 32 | Prediction Threshold: 0.5\n",
      "[Train] Epoch 5/100, Loss=0.0471\n",
      "[Train] Epoch 10/100, Loss=0.0207\n",
      "[Train] Epoch 15/100, Loss=0.0228\n",
      "[Train] Epoch 20/100, Loss=0.0358\n",
      "[Train] Epoch 25/100, Loss=0.0304\n",
      "[Train] Epoch 30/100, Loss=0.0438\n",
      "[Train] Epoch 35/100, Loss=0.0277\n",
      "[Train] Epoch 40/100, Loss=0.0296\n",
      "[Train] Epoch 45/100, Loss=0.0246\n",
      "[Train] Epoch 50/100, Loss=0.0213\n",
      "[Train] Epoch 55/100, Loss=0.1118\n",
      "[Train] Epoch 60/100, Loss=0.0276\n",
      "[Train] Epoch 65/100, Loss=0.0155\n",
      "[Train] Epoch 70/100, Loss=0.0163\n",
      "[Train] Epoch 75/100, Loss=0.0655\n",
      "[Train] Epoch 80/100, Loss=0.0578\n",
      "[Train] Epoch 85/100, Loss=0.0096\n",
      "[Train] Epoch 90/100, Loss=0.0006\n",
      "[Train] Epoch 95/100, Loss=0.0003\n",
      "[Train] Epoch 100/100, Loss=0.0002\n",
      "Accuracy on Nonzero Predictions: 0.5344\n",
      "Training AAPL | LR: 0.011 | Epochs: 100 | Batch: 32 | Prediction Threshold: 0.5\n",
      "[Train] Epoch 5/100, Loss=0.0602\n",
      "[Train] Epoch 10/100, Loss=0.0303\n",
      "[Train] Epoch 15/100, Loss=0.0380\n",
      "[Train] Epoch 20/100, Loss=0.0495\n",
      "[Train] Epoch 25/100, Loss=0.0392\n",
      "[Train] Epoch 30/100, Loss=0.0115\n",
      "[Train] Epoch 35/100, Loss=0.0550\n",
      "[Train] Epoch 40/100, Loss=0.0634\n",
      "[Train] Epoch 45/100, Loss=0.0217\n",
      "[Train] Epoch 50/100, Loss=0.0400\n",
      "[Train] Epoch 55/100, Loss=0.0314\n",
      "[Train] Epoch 60/100, Loss=0.0351\n",
      "[Train] Epoch 65/100, Loss=0.0275\n",
      "[Train] Epoch 70/100, Loss=0.0539\n",
      "[Train] Epoch 75/100, Loss=0.0390\n",
      "[Train] Epoch 80/100, Loss=0.0457\n",
      "[Train] Epoch 85/100, Loss=0.1044\n",
      "[Train] Epoch 90/100, Loss=0.0197\n",
      "[Train] Epoch 95/100, Loss=0.0082\n",
      "[Train] Epoch 100/100, Loss=0.0018\n",
      "Accuracy on Nonzero Predictions: 0.4885\n",
      "[0.5190839694656488, 0.5343511450381679, 0.48854961832061067]\n",
      "[-94.9378069931067, -99.35914257606227, -92.79867161483696]\n",
      "clean, gru, technical\n",
      "/home/jupyter-tfg2425paula/prediction_project_v3/00_data/horizontal_structure/clean/technical/AAPL/10y_5_data.pkl\n",
      "Training AAPL | LR: 0.009 | Epochs: 100 | Batch: 32 | Prediction Threshold: 0.5\n",
      "[Train] Epoch 5/100, Loss=0.6950\n",
      "[Train] Epoch 10/100, Loss=0.7044\n",
      "[Train] Epoch 15/100, Loss=0.7132\n",
      "[Train] Epoch 20/100, Loss=0.7436\n",
      "[Train] Epoch 25/100, Loss=0.7221\n",
      "[Train] Epoch 30/100, Loss=0.7164\n",
      "[Train] Epoch 35/100, Loss=0.7148\n",
      "[Train] Epoch 40/100, Loss=0.7059\n",
      "[Train] Epoch 45/100, Loss=0.7041\n",
      "[Train] Epoch 50/100, Loss=0.7002\n",
      "[Train] Epoch 55/100, Loss=0.7028\n",
      "[Train] Epoch 60/100, Loss=0.7012\n",
      "[Train] Epoch 65/100, Loss=0.7035\n",
      "[Train] Epoch 70/100, Loss=0.7012\n",
      "[Train] Epoch 75/100, Loss=0.7019\n",
      "[Train] Epoch 80/100, Loss=0.7010\n",
      "[Train] Epoch 85/100, Loss=0.7025\n",
      "[Train] Epoch 90/100, Loss=0.6988\n",
      "[Train] Epoch 95/100, Loss=0.6992\n",
      "[Train] Epoch 100/100, Loss=0.6971\n",
      "Accuracy on Nonzero Predictions: 0.6032\n",
      "Training AAPL | LR: 0.01 | Epochs: 100 | Batch: 32 | Prediction Threshold: 0.5\n",
      "[Train] Epoch 5/100, Loss=0.7028\n",
      "[Train] Epoch 10/100, Loss=0.6961\n",
      "[Train] Epoch 15/100, Loss=0.7015\n",
      "[Train] Epoch 20/100, Loss=0.7014\n",
      "[Train] Epoch 25/100, Loss=0.7003\n",
      "[Train] Epoch 30/100, Loss=0.7001\n",
      "[Train] Epoch 35/100, Loss=0.7015\n",
      "[Train] Epoch 40/100, Loss=0.7018\n",
      "[Train] Epoch 45/100, Loss=0.7000\n",
      "[Train] Epoch 50/100, Loss=0.6963\n",
      "[Train] Epoch 55/100, Loss=0.6953\n",
      "[Train] Epoch 60/100, Loss=0.6957\n",
      "[Train] Epoch 65/100, Loss=0.6985\n",
      "[Train] Epoch 70/100, Loss=0.6984\n",
      "[Train] Epoch 75/100, Loss=0.6962\n",
      "[Train] Epoch 80/100, Loss=0.7014\n",
      "[Train] Epoch 85/100, Loss=0.6995\n",
      "[Train] Epoch 90/100, Loss=0.6965\n",
      "[Train] Epoch 95/100, Loss=0.6949\n",
      "[Train] Epoch 100/100, Loss=0.7054\n",
      "Accuracy on Nonzero Predictions: 0.5635\n",
      "Training AAPL | LR: 0.011 | Epochs: 100 | Batch: 32 | Prediction Threshold: 0.5\n",
      "[Train] Epoch 5/100, Loss=0.6959\n",
      "[Train] Epoch 10/100, Loss=0.6991\n",
      "[Train] Epoch 15/100, Loss=0.6966\n",
      "[Train] Epoch 20/100, Loss=0.6965\n",
      "[Train] Epoch 25/100, Loss=0.6964\n",
      "[Train] Epoch 30/100, Loss=0.7054\n",
      "[Train] Epoch 35/100, Loss=0.6946\n",
      "[Train] Epoch 40/100, Loss=0.6934\n",
      "[Train] Epoch 45/100, Loss=0.6953\n",
      "[Train] Epoch 50/100, Loss=0.6966\n",
      "[Train] Epoch 55/100, Loss=0.6914\n",
      "[Train] Epoch 60/100, Loss=0.6915\n",
      "[Train] Epoch 65/100, Loss=0.6944\n",
      "[Train] Epoch 70/100, Loss=0.6933\n",
      "[Train] Epoch 75/100, Loss=0.6979\n",
      "[Train] Epoch 80/100, Loss=0.6885\n",
      "[Train] Epoch 85/100, Loss=0.6885\n",
      "[Train] Epoch 90/100, Loss=0.6917\n",
      "[Train] Epoch 95/100, Loss=0.6908\n",
      "[Train] Epoch 100/100, Loss=0.6926\n",
      "Accuracy on Nonzero Predictions: 0.6349\n",
      "[0.6031746031746031, 0.5634920634920635, 0.6349206349206349]\n",
      "[-0.024383725889679842, 0.8963294861321393, -1.5130746511935689]\n",
      "clean, gru, options\n",
      "/home/jupyter-tfg2425paula/prediction_project_v3/00_data/horizontal_structure/clean/options/AAPL/10y_5_data.pkl\n",
      "Training AAPL | LR: 0.009 | Epochs: 100 | Batch: 32 | Prediction Threshold: 0.5\n",
      "[Train] Epoch 5/100, Loss=0.6911\n",
      "[Train] Epoch 10/100, Loss=0.6894\n",
      "[Train] Epoch 15/100, Loss=0.6878\n",
      "[Train] Epoch 20/100, Loss=0.6838\n",
      "[Train] Epoch 25/100, Loss=0.6783\n",
      "[Train] Epoch 30/100, Loss=0.6707\n",
      "[Train] Epoch 35/100, Loss=0.6707\n",
      "[Train] Epoch 40/100, Loss=0.6515\n",
      "[Train] Epoch 45/100, Loss=0.6445\n",
      "[Train] Epoch 50/100, Loss=0.6301\n",
      "[Train] Epoch 55/100, Loss=0.6334\n",
      "[Train] Epoch 60/100, Loss=0.5998\n",
      "[Train] Epoch 65/100, Loss=0.5919\n",
      "[Train] Epoch 70/100, Loss=0.5724\n",
      "[Train] Epoch 75/100, Loss=0.5557\n",
      "[Train] Epoch 80/100, Loss=0.5347\n",
      "[Train] Epoch 85/100, Loss=0.5160\n",
      "[Train] Epoch 90/100, Loss=0.5340\n",
      "[Train] Epoch 95/100, Loss=0.4906\n",
      "[Train] Epoch 100/100, Loss=0.4817\n",
      "Accuracy on Nonzero Predictions: 0.4733\n",
      "Training AAPL | LR: 0.01 | Epochs: 100 | Batch: 32 | Prediction Threshold: 0.5\n",
      "[Train] Epoch 5/100, Loss=0.4524\n",
      "[Train] Epoch 10/100, Loss=0.4635\n",
      "[Train] Epoch 15/100, Loss=0.4599\n",
      "[Train] Epoch 20/100, Loss=0.4436\n",
      "[Train] Epoch 25/100, Loss=0.4412\n",
      "[Train] Epoch 30/100, Loss=0.4338\n",
      "[Train] Epoch 35/100, Loss=0.4208\n",
      "[Train] Epoch 40/100, Loss=0.4193\n",
      "[Train] Epoch 45/100, Loss=0.4383\n",
      "[Train] Epoch 50/100, Loss=0.4172\n",
      "[Train] Epoch 55/100, Loss=0.4345\n",
      "[Train] Epoch 60/100, Loss=0.4016\n",
      "[Train] Epoch 65/100, Loss=0.3884\n",
      "[Train] Epoch 70/100, Loss=0.3806\n",
      "[Train] Epoch 75/100, Loss=0.3917\n",
      "[Train] Epoch 80/100, Loss=0.4008\n",
      "[Train] Epoch 85/100, Loss=0.3867\n",
      "[Train] Epoch 90/100, Loss=0.3655\n",
      "[Train] Epoch 95/100, Loss=0.4134\n",
      "[Train] Epoch 100/100, Loss=0.3719\n",
      "Accuracy on Nonzero Predictions: 0.4504\n",
      "Training AAPL | LR: 0.011 | Epochs: 100 | Batch: 32 | Prediction Threshold: 0.5\n",
      "[Train] Epoch 5/100, Loss=0.3896\n",
      "[Train] Epoch 10/100, Loss=0.3720\n",
      "[Train] Epoch 15/100, Loss=0.4194\n",
      "[Train] Epoch 20/100, Loss=0.3958\n",
      "[Train] Epoch 25/100, Loss=0.4105\n",
      "[Train] Epoch 30/100, Loss=0.4270\n",
      "[Train] Epoch 35/100, Loss=0.3881\n",
      "[Train] Epoch 40/100, Loss=0.4119\n",
      "[Train] Epoch 45/100, Loss=0.3785\n",
      "[Train] Epoch 50/100, Loss=0.3671\n",
      "[Train] Epoch 55/100, Loss=0.3987\n",
      "[Train] Epoch 60/100, Loss=0.4016\n",
      "[Train] Epoch 65/100, Loss=0.3894\n",
      "[Train] Epoch 70/100, Loss=0.3718\n",
      "[Train] Epoch 75/100, Loss=0.3851\n",
      "[Train] Epoch 80/100, Loss=0.4203\n",
      "[Train] Epoch 85/100, Loss=0.3984\n",
      "[Train] Epoch 90/100, Loss=0.3870\n",
      "[Train] Epoch 95/100, Loss=0.3688\n",
      "[Train] Epoch 100/100, Loss=0.4426\n",
      "Accuracy on Nonzero Predictions: 0.4427\n",
      "[0.4732824427480916, 0.45038167938931295, 0.44274809160305345]\n",
      "[-30.958945785175562, -24.48085407643382, 12.442919118449813]\n",
      "pca, lstm, technical\n",
      "/home/jupyter-tfg2425paula/prediction_project_v3/00_data/horizontal_structure/pca/technical/AAPL/10y_5_data.pkl\n",
      "Training AAPL | LR: 0.009 | Epochs: 100 | Batch: 32 | Prediction Threshold: 0.5\n",
      "[Train] Epoch 5/100, Loss=0.6916\n",
      "[Train] Epoch 10/100, Loss=0.6841\n",
      "[Train] Epoch 15/100, Loss=0.6633\n",
      "[Train] Epoch 20/100, Loss=0.6012\n",
      "[Train] Epoch 25/100, Loss=0.5381\n",
      "[Train] Epoch 30/100, Loss=0.4124\n",
      "[Train] Epoch 35/100, Loss=0.3290\n",
      "[Train] Epoch 40/100, Loss=0.2236\n",
      "[Train] Epoch 45/100, Loss=0.1333\n",
      "[Train] Epoch 50/100, Loss=0.1066\n",
      "[Train] Epoch 55/100, Loss=0.0500\n",
      "[Train] Epoch 60/100, Loss=0.1040\n",
      "[Train] Epoch 65/100, Loss=0.0464\n",
      "[Train] Epoch 70/100, Loss=0.0449\n",
      "[Train] Epoch 75/100, Loss=0.0361\n",
      "[Train] Epoch 80/100, Loss=0.0565\n",
      "[Train] Epoch 85/100, Loss=0.0980\n",
      "[Train] Epoch 90/100, Loss=0.0471\n",
      "[Train] Epoch 95/100, Loss=0.0578\n",
      "[Train] Epoch 100/100, Loss=0.0475\n",
      "Accuracy on Nonzero Predictions: 0.5635\n",
      "Training AAPL | LR: 0.01 | Epochs: 100 | Batch: 32 | Prediction Threshold: 0.5\n",
      "[Train] Epoch 5/100, Loss=0.0690\n",
      "[Train] Epoch 10/100, Loss=0.0715\n",
      "[Train] Epoch 15/100, Loss=0.1046\n",
      "[Train] Epoch 20/100, Loss=0.0810\n",
      "[Train] Epoch 25/100, Loss=0.0441\n",
      "[Train] Epoch 30/100, Loss=0.0459\n",
      "[Train] Epoch 35/100, Loss=0.0636\n",
      "[Train] Epoch 40/100, Loss=0.0789\n",
      "[Train] Epoch 45/100, Loss=0.0615\n",
      "[Train] Epoch 50/100, Loss=0.0626\n",
      "[Train] Epoch 55/100, Loss=0.0607\n",
      "[Train] Epoch 60/100, Loss=0.0830\n",
      "[Train] Epoch 65/100, Loss=0.0586\n",
      "[Train] Epoch 70/100, Loss=0.0466\n",
      "[Train] Epoch 75/100, Loss=0.0351\n",
      "[Train] Epoch 80/100, Loss=0.0912\n",
      "[Train] Epoch 85/100, Loss=0.0668\n",
      "[Train] Epoch 90/100, Loss=0.0526\n",
      "[Train] Epoch 95/100, Loss=0.0600\n",
      "[Train] Epoch 100/100, Loss=0.0595\n",
      "Accuracy on Nonzero Predictions: 0.4603\n",
      "Training AAPL | LR: 0.011 | Epochs: 100 | Batch: 32 | Prediction Threshold: 0.5\n",
      "[Train] Epoch 5/100, Loss=0.0716\n",
      "[Train] Epoch 10/100, Loss=0.0561\n",
      "[Train] Epoch 15/100, Loss=0.0743\n",
      "[Train] Epoch 20/100, Loss=0.0875\n",
      "[Train] Epoch 25/100, Loss=0.0374\n",
      "[Train] Epoch 30/100, Loss=0.0862\n",
      "[Train] Epoch 35/100, Loss=0.0728\n",
      "[Train] Epoch 40/100, Loss=0.0523\n",
      "[Train] Epoch 45/100, Loss=0.0295\n",
      "[Train] Epoch 50/100, Loss=0.1493\n",
      "[Train] Epoch 55/100, Loss=0.0980\n",
      "[Train] Epoch 60/100, Loss=0.0628\n",
      "[Train] Epoch 65/100, Loss=0.0765\n",
      "[Train] Epoch 70/100, Loss=0.0466\n",
      "[Train] Epoch 75/100, Loss=0.0953\n",
      "[Train] Epoch 80/100, Loss=0.0719\n",
      "[Train] Epoch 85/100, Loss=0.0773\n",
      "[Train] Epoch 90/100, Loss=0.0415\n",
      "[Train] Epoch 95/100, Loss=0.0817\n",
      "[Train] Epoch 100/100, Loss=0.0634\n",
      "Accuracy on Nonzero Predictions: 0.4841\n",
      "[0.5634920634920635, 0.4603174603174603, 0.48412698412698413]\n",
      "[-93.15602203745283, -15.739373770712122, -36.201865473233866]\n",
      "pca, lstm, options\n",
      "/home/jupyter-tfg2425paula/prediction_project_v3/00_data/horizontal_structure/pca/options/AAPL/10y_5_data.pkl\n",
      "Training AAPL | LR: 0.009 | Epochs: 100 | Batch: 32 | Prediction Threshold: 0.5\n",
      "[Train] Epoch 5/100, Loss=0.6941\n",
      "[Train] Epoch 10/100, Loss=0.6809\n",
      "[Train] Epoch 15/100, Loss=0.6406\n",
      "[Train] Epoch 20/100, Loss=0.5018\n",
      "[Train] Epoch 25/100, Loss=0.3461\n",
      "[Train] Epoch 30/100, Loss=0.1920\n",
      "[Train] Epoch 35/100, Loss=0.1249\n",
      "[Train] Epoch 40/100, Loss=0.0883\n",
      "[Train] Epoch 45/100, Loss=0.0582\n",
      "[Train] Epoch 50/100, Loss=0.0328\n",
      "[Train] Epoch 55/100, Loss=0.0253\n",
      "[Train] Epoch 60/100, Loss=0.0290\n",
      "[Train] Epoch 65/100, Loss=0.0831\n",
      "[Train] Epoch 70/100, Loss=0.0220\n",
      "[Train] Epoch 75/100, Loss=0.0118\n",
      "[Train] Epoch 80/100, Loss=0.0123\n",
      "[Train] Epoch 85/100, Loss=0.0584\n",
      "[Train] Epoch 90/100, Loss=0.0159\n",
      "[Train] Epoch 95/100, Loss=0.0142\n",
      "[Train] Epoch 100/100, Loss=0.0150\n",
      "Accuracy on Nonzero Predictions: 0.5573\n",
      "Training AAPL | LR: 0.01 | Epochs: 100 | Batch: 32 | Prediction Threshold: 0.5\n",
      "[Train] Epoch 5/100, Loss=0.0456\n",
      "[Train] Epoch 10/100, Loss=0.0167\n",
      "[Train] Epoch 15/100, Loss=0.0077\n",
      "[Train] Epoch 20/100, Loss=0.0812\n",
      "[Train] Epoch 25/100, Loss=0.0243\n",
      "[Train] Epoch 30/100, Loss=0.0125\n",
      "[Train] Epoch 35/100, Loss=0.0512\n",
      "[Train] Epoch 40/100, Loss=0.0321\n",
      "[Train] Epoch 45/100, Loss=0.0006\n",
      "[Train] Epoch 50/100, Loss=0.0002\n",
      "[Train] Epoch 55/100, Loss=0.0002\n",
      "[Train] Epoch 60/100, Loss=0.0001\n",
      "[Train] Epoch 65/100, Loss=0.0001\n",
      "[Train] Epoch 70/100, Loss=0.0001\n",
      "[Train] Epoch 75/100, Loss=0.0001\n",
      "[Train] Epoch 80/100, Loss=0.0000\n",
      "[Train] Epoch 85/100, Loss=0.0000\n",
      "[Train] Epoch 90/100, Loss=0.0000\n",
      "[Train] Epoch 95/100, Loss=0.0000\n",
      "[Train] Epoch 100/100, Loss=0.0000\n",
      "Accuracy on Nonzero Predictions: 0.5344\n",
      "Training AAPL | LR: 0.011 | Epochs: 100 | Batch: 32 | Prediction Threshold: 0.5\n",
      "[Train] Epoch 5/100, Loss=0.0274\n",
      "[Train] Epoch 10/100, Loss=0.0425\n",
      "[Train] Epoch 15/100, Loss=0.0149\n",
      "[Train] Epoch 20/100, Loss=0.0262\n",
      "[Train] Epoch 25/100, Loss=0.1049\n",
      "[Train] Epoch 30/100, Loss=0.0189\n",
      "[Train] Epoch 35/100, Loss=0.0017\n",
      "[Train] Epoch 40/100, Loss=0.0002\n",
      "[Train] Epoch 45/100, Loss=0.0001\n",
      "[Train] Epoch 50/100, Loss=0.0001\n",
      "[Train] Epoch 55/100, Loss=0.0001\n",
      "[Train] Epoch 60/100, Loss=0.0001\n",
      "[Train] Epoch 65/100, Loss=0.0000\n",
      "[Train] Epoch 70/100, Loss=0.0000\n",
      "[Train] Epoch 75/100, Loss=0.0000\n",
      "[Train] Epoch 80/100, Loss=0.0000\n",
      "[Train] Epoch 85/100, Loss=0.0000\n",
      "[Train] Epoch 90/100, Loss=0.0000\n",
      "[Train] Epoch 95/100, Loss=0.0000\n",
      "[Train] Epoch 100/100, Loss=0.0000\n",
      "Accuracy on Nonzero Predictions: 0.5267\n",
      "[0.5572519083969466, 0.5343511450381679, 0.5267175572519084]\n",
      "[-97.85015008122622, -99.90692732570986, -99.8933362240516]\n",
      "pca, gru, technical\n",
      "/home/jupyter-tfg2425paula/prediction_project_v3/00_data/horizontal_structure/pca/technical/AAPL/10y_5_data.pkl\n",
      "Training AAPL | LR: 0.009 | Epochs: 100 | Batch: 32 | Prediction Threshold: 0.5\n",
      "[Train] Epoch 5/100, Loss=0.6920\n",
      "[Train] Epoch 10/100, Loss=0.6933\n",
      "[Train] Epoch 15/100, Loss=0.6870\n",
      "[Train] Epoch 20/100, Loss=0.6980\n",
      "[Train] Epoch 25/100, Loss=0.6980\n",
      "[Train] Epoch 30/100, Loss=0.6969\n",
      "[Train] Epoch 35/100, Loss=0.6923\n",
      "[Train] Epoch 40/100, Loss=0.6908\n",
      "[Train] Epoch 45/100, Loss=0.6920\n",
      "[Train] Epoch 50/100, Loss=0.6878\n",
      "[Train] Epoch 55/100, Loss=0.6924\n",
      "[Train] Epoch 60/100, Loss=0.6870\n",
      "[Train] Epoch 65/100, Loss=0.6868\n",
      "[Train] Epoch 70/100, Loss=0.6917\n",
      "[Train] Epoch 75/100, Loss=0.6835\n",
      "[Train] Epoch 80/100, Loss=0.6827\n",
      "[Train] Epoch 85/100, Loss=0.6824\n",
      "[Train] Epoch 90/100, Loss=0.6799\n",
      "[Train] Epoch 95/100, Loss=0.6739\n",
      "[Train] Epoch 100/100, Loss=0.6683\n",
      "Accuracy on Nonzero Predictions: 0.5714\n",
      "Training AAPL | LR: 0.01 | Epochs: 100 | Batch: 32 | Prediction Threshold: 0.5\n",
      "[Train] Epoch 5/100, Loss=0.6793\n",
      "[Train] Epoch 10/100, Loss=0.6692\n",
      "[Train] Epoch 15/100, Loss=0.6630\n",
      "[Train] Epoch 20/100, Loss=0.6585\n",
      "[Train] Epoch 25/100, Loss=0.6539\n",
      "[Train] Epoch 30/100, Loss=0.6752\n",
      "[Train] Epoch 35/100, Loss=0.6612\n",
      "[Train] Epoch 40/100, Loss=0.7094\n",
      "[Train] Epoch 45/100, Loss=0.6949\n",
      "[Train] Epoch 50/100, Loss=0.6916\n",
      "[Train] Epoch 55/100, Loss=0.6882\n",
      "[Train] Epoch 60/100, Loss=0.6885\n",
      "[Train] Epoch 65/100, Loss=0.6850\n",
      "[Train] Epoch 70/100, Loss=0.6842\n",
      "[Train] Epoch 75/100, Loss=0.6888\n",
      "[Train] Epoch 80/100, Loss=0.6892\n",
      "[Train] Epoch 85/100, Loss=0.6849\n",
      "[Train] Epoch 90/100, Loss=0.6854\n",
      "[Train] Epoch 95/100, Loss=0.6853\n",
      "[Train] Epoch 100/100, Loss=0.6787\n",
      "Accuracy on Nonzero Predictions: 0.5635\n",
      "Training AAPL | LR: 0.011 | Epochs: 100 | Batch: 32 | Prediction Threshold: 0.5\n",
      "[Train] Epoch 5/100, Loss=0.6820\n",
      "[Train] Epoch 10/100, Loss=0.6765\n",
      "[Train] Epoch 15/100, Loss=0.6805\n",
      "[Train] Epoch 20/100, Loss=0.6811\n",
      "[Train] Epoch 25/100, Loss=0.6873\n",
      "[Train] Epoch 30/100, Loss=0.6835\n",
      "[Train] Epoch 35/100, Loss=0.6911\n",
      "[Train] Epoch 40/100, Loss=0.6828\n",
      "[Train] Epoch 45/100, Loss=0.6955\n",
      "[Train] Epoch 50/100, Loss=0.6951\n",
      "[Train] Epoch 55/100, Loss=0.6924\n",
      "[Train] Epoch 60/100, Loss=0.6914\n",
      "[Train] Epoch 65/100, Loss=0.6918\n",
      "[Train] Epoch 70/100, Loss=0.6898\n",
      "[Train] Epoch 75/100, Loss=0.6866\n",
      "[Train] Epoch 80/100, Loss=0.6883\n",
      "[Train] Epoch 85/100, Loss=0.6834\n",
      "[Train] Epoch 90/100, Loss=0.6928\n",
      "[Train] Epoch 95/100, Loss=0.6922\n",
      "[Train] Epoch 100/100, Loss=0.6861\n",
      "Accuracy on Nonzero Predictions: 0.5556\n",
      "[0.5714285714285714, 0.5634920634920635, 0.5555555555555556]\n",
      "[-3.975172059996441, 0.4334734736773418, 1.277579270892743]\n",
      "pca, gru, options\n",
      "/home/jupyter-tfg2425paula/prediction_project_v3/00_data/horizontal_structure/pca/options/AAPL/10y_5_data.pkl\n",
      "Training AAPL | LR: 0.009 | Epochs: 100 | Batch: 32 | Prediction Threshold: 0.5\n",
      "[Train] Epoch 5/100, Loss=0.6893\n",
      "[Train] Epoch 10/100, Loss=0.6883\n",
      "[Train] Epoch 15/100, Loss=0.6857\n",
      "[Train] Epoch 20/100, Loss=0.7193\n",
      "[Train] Epoch 25/100, Loss=0.6860\n",
      "[Train] Epoch 30/100, Loss=0.6833\n",
      "[Train] Epoch 35/100, Loss=0.6811\n",
      "[Train] Epoch 40/100, Loss=0.6789\n",
      "[Train] Epoch 45/100, Loss=0.6745\n",
      "[Train] Epoch 50/100, Loss=0.6691\n",
      "[Train] Epoch 55/100, Loss=0.6592\n",
      "[Train] Epoch 60/100, Loss=0.6636\n",
      "[Train] Epoch 65/100, Loss=0.6465\n",
      "[Train] Epoch 70/100, Loss=0.6326\n",
      "[Train] Epoch 75/100, Loss=0.6152\n",
      "[Train] Epoch 80/100, Loss=0.6165\n",
      "[Train] Epoch 85/100, Loss=0.5962\n",
      "[Train] Epoch 90/100, Loss=0.5831\n",
      "[Train] Epoch 95/100, Loss=0.5539\n",
      "[Train] Epoch 100/100, Loss=0.5390\n",
      "Accuracy on Nonzero Predictions: 0.5649\n",
      "Training AAPL | LR: 0.01 | Epochs: 100 | Batch: 32 | Prediction Threshold: 0.5\n",
      "[Train] Epoch 5/100, Loss=0.5389\n",
      "[Train] Epoch 10/100, Loss=0.5075\n",
      "[Train] Epoch 15/100, Loss=0.5221\n",
      "[Train] Epoch 20/100, Loss=0.4973\n",
      "[Train] Epoch 25/100, Loss=0.5118\n",
      "[Train] Epoch 30/100, Loss=0.4773\n",
      "[Train] Epoch 35/100, Loss=0.4848\n",
      "[Train] Epoch 40/100, Loss=0.4864\n",
      "[Train] Epoch 45/100, Loss=0.4445\n",
      "[Train] Epoch 50/100, Loss=0.4543\n",
      "[Train] Epoch 55/100, Loss=0.4414\n",
      "[Train] Epoch 60/100, Loss=0.4551\n",
      "[Train] Epoch 65/100, Loss=0.4506\n",
      "[Train] Epoch 70/100, Loss=0.4445\n",
      "[Train] Epoch 75/100, Loss=0.4514\n",
      "[Train] Epoch 80/100, Loss=0.4192\n",
      "[Train] Epoch 85/100, Loss=0.4109\n",
      "[Train] Epoch 90/100, Loss=0.4210\n",
      "[Train] Epoch 95/100, Loss=0.4156\n",
      "[Train] Epoch 100/100, Loss=0.3954\n",
      "Accuracy on Nonzero Predictions: 0.5115\n",
      "Training AAPL | LR: 0.011 | Epochs: 100 | Batch: 32 | Prediction Threshold: 0.5\n",
      "[Train] Epoch 5/100, Loss=0.4209\n",
      "[Train] Epoch 10/100, Loss=0.3932\n",
      "[Train] Epoch 15/100, Loss=0.3851\n",
      "[Train] Epoch 20/100, Loss=0.4028\n",
      "[Train] Epoch 25/100, Loss=0.4106\n",
      "[Train] Epoch 30/100, Loss=0.3856\n",
      "[Train] Epoch 35/100, Loss=0.3826\n",
      "[Train] Epoch 40/100, Loss=0.4009\n",
      "[Train] Epoch 45/100, Loss=0.4179\n",
      "[Train] Epoch 50/100, Loss=0.4325\n",
      "[Train] Epoch 55/100, Loss=0.3896\n",
      "[Train] Epoch 60/100, Loss=0.3806\n",
      "[Train] Epoch 65/100, Loss=0.3536\n",
      "[Train] Epoch 70/100, Loss=0.3472\n",
      "[Train] Epoch 75/100, Loss=0.3583\n",
      "[Train] Epoch 80/100, Loss=0.3670\n",
      "[Train] Epoch 85/100, Loss=0.3837\n",
      "[Train] Epoch 90/100, Loss=0.3611\n",
      "[Train] Epoch 95/100, Loss=0.3973\n",
      "[Train] Epoch 100/100, Loss=0.3509\n",
      "Accuracy on Nonzero Predictions: 0.5344\n",
      "[0.5648854961832062, 0.5114503816793893, 0.5343511450381679]\n",
      "[-22.615566032532644, -29.43880430515945, -13.463291786072517]\n"
     ]
    }
   ],
   "source": [
    "results_list = []\n",
    "stock = \"AAPL\"\n",
    "period = \"10y\"\n",
    "possible_train_size = 95\n",
    "window_size = 5\n",
    "batch_size = 32\n",
    "num_epochs = 100\n",
    "\n",
    "prediction_threshold = 0.5\n",
    "\n",
    "output_folder = os.path.join(results_dir, f\"inidividual_trials\") \n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "results_csv_path = os.path.join(output_folder, f\"01_{stock}_proc_model_type.csv\")\n",
    "processing_types = [\"clean\", \"pca\"]\n",
    "types_securities = [\"technical\", \"options\"]\n",
    "\n",
    "results_list = []\n",
    "\n",
    "learning_rates = [0.009, 0.01, 0.011]\n",
    "\n",
    "for processing in processing_types:  # e.g. [\"clean\", \"pca\"]\n",
    "    initial_data_dir = os.path.join(project_dir, f\"00_data/{processing}\") \n",
    "\n",
    "    for model_type in model_types:\n",
    "        for security_type in types_securities:\n",
    "            \n",
    "            test_accuracy_list = []\n",
    "            pct_decrease_list = []\n",
    "            \n",
    "            # Load original data (info only)\n",
    "            filename = f\"{security_type}/{stock}/{period}_data.csv\"\n",
    "            original_input_filepath = os.path.join(initial_data_dir, filename)\n",
    "            original_data = pd.read_csv(original_input_filepath)\n",
    "\n",
    "            # Iterate over window sizes\n",
    "            print(f\"{processing}, {model_type}, {security_type}\")\n",
    "\n",
    "            # Load data using the 'processing' variable in path\n",
    "            pkl_filename = f\"{processing}/{security_type}/{stock}/{period}_{window_size}_data.pkl\"\n",
    "            input_filepath = os.path.join(horizontal_data_dir, pkl_filename)\n",
    "            print(input_filepath)\n",
    "            input_df = pd.read_pickle(input_filepath)\n",
    "\n",
    "            X_resampled, y_resampled = reshape_remove_characters(input_df)\n",
    "\n",
    "            input_size = X_resampled.shape[2]\n",
    "            train_size = int(X_resampled.shape[0] * possible_train_size / 100)\n",
    "            test_size = X_resampled.shape[0] - train_size\n",
    "\n",
    "            # Generate model\n",
    "            if model_type == \"gru\":\n",
    "                model = GRU3DClassifier(input_size, hidden_size, output_size, num_layers, dropout)\n",
    "            elif model_type == \"lstm\":\n",
    "                model = StockPriceLSTM(input_size, hidden_size, output_size)\n",
    "\n",
    "            model = model.to(device)\n",
    "            \n",
    "            for learning_rate in learning_rates:\n",
    "                optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "                print(f\"Training {stock} | LR: {learning_rate} | Epochs: {num_epochs} \"\n",
    "                      f\"| Batch: {batch_size} | Prediction Threshold: {prediction_threshold}\")\n",
    "\n",
    "                result = evaluate_rolling_unchanged_model_threshold(\n",
    "                    model, \n",
    "                    X_resampled, \n",
    "                    y_resampled, \n",
    "                    criterion, \n",
    "                    optimizer, \n",
    "                    device, \n",
    "                    train_size, \n",
    "                    batch_size, \n",
    "                    num_epochs, \n",
    "                    lower_threshold=prediction_threshold\n",
    "                )     \n",
    "\n",
    "                rolling_predictions = result[\"rolling_predictions\"]\n",
    "                rolling_targets = result[\"rolling_targets\"]\n",
    "                test_accuracy = result[\"accuracy_nonzero\"]\n",
    "                loss_decrease_percentage = result[\"loss_decrease_percentage\"]\n",
    "                nonzero_preds = np.count_nonzero(rolling_predictions)\n",
    "                \n",
    "                test_accuracy_list.append(test_accuracy)\n",
    "                pct_decrease_list.append(loss_decrease_percentage)\n",
    "\n",
    "            print(test_accuracy_list)\n",
    "            print(pct_decrease_list)\n",
    "            # 1) Create a record (dictionary) for this run\n",
    "            run_record = {\"STOCK\": stock,\n",
    "                \"DATA_TYPE\": security_type,\n",
    "                \"MODEL\": model_type.upper(),  # Convert to uppercase for consistency\n",
    "                \"PROCESSING\": processing,\n",
    "                \"MEAN_ACCURACY\": np.mean(test_accuracy_list),\n",
    "                \"MAX_ACCURACY\": np.max(test_accuracy_list),\n",
    "                \"MIN_ACCURACY\": np.min(test_accuracy_list),\n",
    "                \"MEAN_TRAIN_PCT_DECREASE\": np.mean(test_accuracy_list)}\n",
    "\n",
    "            # 2) Append to list\n",
    "            results_list.append(run_record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6614f981-e9af-4d6d-bb40-d94540faddcb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>STOCK</th>\n",
       "      <th>DATA_TYPE</th>\n",
       "      <th>MODEL</th>\n",
       "      <th>PROCESSING</th>\n",
       "      <th>MEAN_ACCURACY</th>\n",
       "      <th>MAX_ACCURACY</th>\n",
       "      <th>MIN_ACCURACY</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>technical</td>\n",
       "      <td>LSTM</td>\n",
       "      <td>clean</td>\n",
       "      <td>0.479365</td>\n",
       "      <td>0.492063</td>\n",
       "      <td>0.460317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>options</td>\n",
       "      <td>LSTM</td>\n",
       "      <td>clean</td>\n",
       "      <td>0.503817</td>\n",
       "      <td>0.511450</td>\n",
       "      <td>0.480916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>technical</td>\n",
       "      <td>GRU</td>\n",
       "      <td>clean</td>\n",
       "      <td>0.534921</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.507937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>options</td>\n",
       "      <td>GRU</td>\n",
       "      <td>clean</td>\n",
       "      <td>0.545038</td>\n",
       "      <td>0.564885</td>\n",
       "      <td>0.496183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>technical</td>\n",
       "      <td>LSTM</td>\n",
       "      <td>pca</td>\n",
       "      <td>0.538095</td>\n",
       "      <td>0.595238</td>\n",
       "      <td>0.452381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>options</td>\n",
       "      <td>LSTM</td>\n",
       "      <td>pca</td>\n",
       "      <td>0.497710</td>\n",
       "      <td>0.541985</td>\n",
       "      <td>0.458015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>technical</td>\n",
       "      <td>GRU</td>\n",
       "      <td>pca</td>\n",
       "      <td>0.482540</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.380952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>options</td>\n",
       "      <td>GRU</td>\n",
       "      <td>pca</td>\n",
       "      <td>0.490076</td>\n",
       "      <td>0.526718</td>\n",
       "      <td>0.450382</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  STOCK  DATA_TYPE MODEL PROCESSING  MEAN_ACCURACY  MAX_ACCURACY  MIN_ACCURACY\n",
       "0  AAPL  technical  LSTM      clean       0.479365      0.492063      0.460317\n",
       "1  AAPL    options  LSTM      clean       0.503817      0.511450      0.480916\n",
       "2  AAPL  technical   GRU      clean       0.534921      0.571429      0.507937\n",
       "3  AAPL    options   GRU      clean       0.545038      0.564885      0.496183\n",
       "4  AAPL  technical  LSTM        pca       0.538095      0.595238      0.452381\n",
       "5  AAPL    options  LSTM        pca       0.497710      0.541985      0.458015\n",
       "6  AAPL  technical   GRU        pca       0.482540      0.555556      0.380952\n",
       "7  AAPL    options   GRU        pca       0.490076      0.526718      0.450382"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(results_list)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a1abaee6-0df1-44b7-b371-b87b92bff765",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) Write to CSV once for this batch of window_sizes (avoid partial duplication)\n",
    "if len(results_list) > 0:\n",
    "    df = pd.DataFrame(results_list)\n",
    "\n",
    "    if os.path.exists(results_csv_path):\n",
    "        # Append without header\n",
    "        df.to_csv(results_csv_path, mode='a', header=False, index=False)\n",
    "    else:\n",
    "        # Write new file with header\n",
    "        df.to_csv(results_csv_path, index=False)\n",
    "\n",
    "    # Clear the list before next iteration\n",
    "    results_list = []"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
