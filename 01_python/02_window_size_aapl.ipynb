{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "940889cf-06e0-4e8c-8efa-1eeacc97ca57",
   "metadata": {},
   "source": [
    "## **This notebook aims to compare some models and store them in a reasonable fashion**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e46c228-21f4-4e80-a4e8-b62981ab4799",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.metrics import accuracy_score\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a11a538-40d5-49c3-92e9-e26fdcefe083",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"  # Use GPU 0 in first notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b259658-5fed-4231-9f06-f9db567e1d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "project_dir = \"/home/jupyter-tfg2425paula/prediction_project_v3\"\n",
    "os.chdir(project_dir)\n",
    "\n",
    "clean_data_dir = os.path.join(project_dir, \"00_data/clean\")\n",
    "horizontal_data_dir = os.path.join(project_dir, \"00_data/horizontal_structure\")\n",
    "results_dir = os.path.join(project_dir, \"02_results\")\n",
    "plots_dir = os.path.join(project_dir, \"03_plots\")\n",
    "pca_data_dir = os.path.join(project_dir, \"00_data/pca\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7319029-ba4c-4c90-91b9-a0a02b5be9af",
   "metadata": {},
   "source": [
    "### **GRU Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a7a1485d-99bb-4358-a216-1c1c75e50ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRU3DClassifier(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers, dropout):\n",
    "        super(GRU3DClassifier, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "\n",
    "        out, _ = self.gru(x, h0)\n",
    "        out = self.fc(out[:, -1, :]) \n",
    "        # return self.sigmoid(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3061a6b4-dad3-4129-a2ed-0098e04440e4",
   "metadata": {},
   "source": [
    "### **LSTM Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "95ffa6e2-2c35-48de-8c95-b7ce86fe9dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StockPriceLSTM(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers=1, dropout=0.0):\n",
    "        super(StockPriceLSTM, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers=num_layers, \n",
    "                            batch_first=True, dropout=dropout)\n",
    "    \n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)  # Get the batch size dynamically\n",
    "\n",
    "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_dim).to(x.device)  # (num_layers, batch_size, hidden_dim)\n",
    "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_dim).to(x.device)  # (num_layers, batch_size, hidden_dim)\n",
    "        \n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "\n",
    "        out = self.fc(out[:, -1, :]) \n",
    "        # out = self.sigmoid(out)\n",
    "        return out\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df6d4b78-1492-45ea-a9af-854d8d69b8de",
   "metadata": {},
   "source": [
    "### **Set folders**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e67b679f-204f-4e15-89fe-09879ed7246c",
   "metadata": {},
   "source": [
    "Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2b80ed82-98cd-453d-94e9-b7f42c61040e",
   "metadata": {},
   "outputs": [],
   "source": [
    "processing_types = [\"clean\", \"pca\"]\n",
    "processing_types= [\"clean\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da3781cf-0f0b-4858-bff8-09f839e2da04",
   "metadata": {},
   "source": [
    "Folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a2958856-f0de-4f93-85b5-9fdd169a2a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stocks = ['AAPL', 'MSFT', 'AMZN', 'NVDA', 'SPX']\n",
    "stocks = ['AAPL']\n",
    "# types_securities = [\"single_name\", \"options\", \"technical\"]\n",
    "types_securities = [\"options\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2e929df-0399-4e41-9b2b-1863f8c215b5",
   "metadata": {},
   "source": [
    "Different files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9b853df6-38b4-4bd7-85e5-6a232f17905e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# years = [\"15y\", \"10y\", \"5y\", \"2y\"]\n",
    "years = [\"10y\"]\n",
    "# window_sizes = [5, 10, 50, 100]\n",
    "window_sizes = [5]\n",
    "# train_sizes = [80, 90, 95]\n",
    "train_sizes = [95]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f9fb2cc-d8de-4e4f-be9d-f8cb7aac8e16",
   "metadata": {},
   "source": [
    "Same file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "145e5ba2-35c7-4a71-b100-624ca6ed1ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds = [0.3, 0.35, 0.4, 0.45, 0.5]\n",
    "thresholds = [0.5]\n",
    "learning_rates = [0.005, 0.008, 0.009, 0.01]\n",
    "learning_rates = [0.01]\n",
    "num_epochs_list = [100, 200]\n",
    "num_epochs_list = [100]\n",
    "batch_sizes = [16, 32]\n",
    "batch_sizes = [16]\n",
    "prediction_thresholds = [0.35, 0.4, 0.45, 0.5]\n",
    "prediction_thresholds = [0.5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbbee35f-0147-48b0-93ef-d9687984bbd0",
   "metadata": {},
   "source": [
    "#### **Model and Hyperparameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "71861bc1-7067-47f8-b3dd-a5cef2bc0f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "hidden_size = 64  \n",
    "output_size = 2  \n",
    "num_layers = 2\n",
    "dropout = 0.2\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "60900dea-a72e-4b53-979b-aded1e725489",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_types = [\"lstm\", \"gru\"]   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d873f70-b583-4e84-9289-bc303ef0446f",
   "metadata": {},
   "source": [
    "#### **Last data modifications**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "85729e3c-f060-4dc2-a4fc-a80573fefd73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_remove_characters(df):\n",
    "\n",
    "    X = np.array([np.stack(row) for row in df.drop(columns=['Target']).values])\n",
    "    y = df['Target'].values\n",
    "\n",
    "    smote = SMOTE(random_state=42)\n",
    "    n_samples, timesteps, n_features = X.shape\n",
    "    X_flat = X.reshape((n_samples, timesteps * n_features))\n",
    "    X_flat = np.where(X_flat == 'ç', 0, X_flat)\n",
    "\n",
    "    X_resampled = X_flat.reshape((-1, timesteps, n_features))\n",
    "    \n",
    "    return X_resampled, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88656719-d0c4-4bf2-aef1-31f00b167d2a",
   "metadata": {},
   "source": [
    "### **Evaluation function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "74c49327-1c0d-4706-9943-54b795cb0687",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_rolling_unchanged_model_threshold(\n",
    "    model, \n",
    "    X, \n",
    "    y, \n",
    "    criterion, \n",
    "    optimizer, \n",
    "    device, \n",
    "    train_size, \n",
    "    batch_size, \n",
    "    num_epochs, \n",
    "    lower_threshold\n",
    "):\n",
    "    \"\"\"\n",
    "    Evaluate a PyTorch model using a rolling prediction approach for time series,\n",
    "    training the model only once on the initial training set. For each time step\n",
    "    after train_size, the model makes a prediction without further parameter updates.\n",
    "    Only predicts +1 or -1 if the probability of class 1 is above/below given thresholds;\n",
    "    otherwise, predicts 0. Accuracy is computed only on nonzero predictions.\n",
    "\n",
    "    Args:\n",
    "        model:          PyTorch model to evaluate.\n",
    "        X:              Feature data (numpy array).\n",
    "        y:              Target data (numpy array).\n",
    "        criterion:      Loss function (e.g., CrossEntropyLoss).\n",
    "        optimizer:      Optimizer (e.g., Adam).\n",
    "        device:         Device for computation (CPU or GPU).\n",
    "        train_size:     Initial size of the training data (int or float).\n",
    "                        If < 1, treated as fraction of total length.\n",
    "        batch_size:     Batch size for training.\n",
    "        num_epochs:     Number of epochs for initial training only.\n",
    "        lower_threshold: Probability threshold below which model predicts -1.\n",
    "        upper_threshold: Probability threshold above which model predicts +1.\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary with the following keys:\n",
    "            - \"rolling_predictions\": All predictions (-1, 0, +1) across the test period.\n",
    "            - \"rolling_targets\": Corresponding true targets in [-1, +1].\n",
    "            - \"filtered_predictions\": Nonzero predictions only.\n",
    "            - \"filtered_targets\": Targets corresponding to nonzero predictions.\n",
    "            - \"accuracy_nonzero\": Accuracy computed only on nonzero predictions.\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert X, y to tensors\n",
    "    X = torch.tensor(X, dtype=torch.float32)\n",
    "    y = torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "    # Determine initial training set size\n",
    "    if train_size < 1.0:\n",
    "        lower_bound = int(train_size * len(X))\n",
    "    else:\n",
    "        lower_bound = train_size\n",
    "\n",
    "    # -------------------------\n",
    "    # 1) SINGLE TRAINING PHASE\n",
    "    # -------------------------\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "    \n",
    "    X_train = X[:lower_bound].to(device)\n",
    "    y_train = y[:lower_bound].to(device)\n",
    "\n",
    "    train_dataset = TensorDataset(X_train, y_train)\n",
    "    trainloader = DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size=batch_size, \n",
    "        shuffle=False,         # Keep False if order matters; True for better generalization\n",
    "        # num_workers=4,         # Adjust based on your CPU cores\n",
    "        # pin_memory=True,       # Speeds up transfer if using GPUs\n",
    "        drop_last=False        # Ensure the last batch is included\n",
    "    )\n",
    "\n",
    "    epoch_train_losses = []\n",
    "    for epoch in range(num_epochs):\n",
    "        # torch.cuda.empty_cache()\n",
    "        epoch_loss = 0.0\n",
    "        for X_batch, y_batch in trainloader:\n",
    "            X_batch = X_batch.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            pred_y = model(X_batch)   # [batch_size, num_classes]\n",
    "            loss = criterion(pred_y, y_batch)\n",
    "            loss.backward()\n",
    "\n",
    "            # Gradient clipping (optional)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "               \n",
    "        if (epoch + 1) % 5 == 0 or epoch == num_epochs - 1:\n",
    "            print(f\"[Train] Epoch {epoch+1}/{num_epochs}, Loss={epoch_loss/len(trainloader):.4f}\")\n",
    "\n",
    "        epoch_train_losses.append(epoch_loss/len(trainloader))\n",
    "        \n",
    "    loss_decrease_percentage = ((epoch_train_losses[-1] - epoch_train_losses[0]) / epoch_train_losses[0]) * 100\n",
    "    # ---------------------------------\n",
    "    # 2) ROLLING PREDICTIONS, NO UPDATE\n",
    "    # ---------------------------------\n",
    "    model.eval()\n",
    "\n",
    "    rolling_predictions = []\n",
    "    rolling_targets     = []\n",
    "\n",
    "    for i in range(lower_bound, len(X)):\n",
    "        # Single-step \"test\" sample\n",
    "        X_test = X[i:i+1].to(device)  # shape: (1, num_features)\n",
    "        y_test = y[i:i+1].to(device)  # shape: (1, )\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # Forward pass\n",
    "            pred_y = model(X_test)\n",
    "            probabilities = torch.softmax(pred_y, dim=1).cpu().numpy()\n",
    "            prob_class_1  = probabilities[:, 1] \n",
    "\n",
    "            # Threshold-based logic\n",
    "            # Initialize all predictions to 0\n",
    "            pred_classes = np.zeros_like(prob_class_1)\n",
    "            # Predict -1 if prob < lower_threshold\n",
    "            pred_classes[prob_class_1 < lower_threshold] = -1\n",
    "            # Predict +1 if prob > upper_threshold\n",
    "            pred_classes[prob_class_1 > 1-lower_threshold] = 1\n",
    "\n",
    "        rolling_predictions.append(pred_classes[0])  # scalar\n",
    "        rolling_targets.append(y_test.item())\n",
    "\n",
    "    rolling_predictions = np.array(rolling_predictions)\n",
    "    rolling_targets = np.array(rolling_targets).astype(int)\n",
    "\n",
    "    # Convert any 0-labeled targets to -1 if your original data is in [-1, +1]\n",
    "    # (Sometimes y might be {0,1} or {-1, +1}; adapt as needed.)\n",
    "    rolling_targets[rolling_targets == 0] = -1\n",
    "\n",
    "    # Filter out zero predictions\n",
    "    nonzero_mask = rolling_predictions != 0\n",
    "    filtered_preds = rolling_predictions[nonzero_mask]\n",
    "    filtered_targets = rolling_targets[nonzero_mask]\n",
    "\n",
    "    if len(filtered_preds) == 0:\n",
    "        accuracy_nonzero = None\n",
    "        print(\"No nonzero predictions, cannot compute thresholded accuracy.\")\n",
    "    else:\n",
    "        accuracy_nonzero = accuracy_score(filtered_targets, filtered_preds)\n",
    "        print(f\"Accuracy on Nonzero Predictions: {accuracy_nonzero:.4f}\")\n",
    "\n",
    "    return {\n",
    "        \"rolling_predictions\": rolling_predictions,\n",
    "        \"rolling_targets\": rolling_targets,\n",
    "        \"filtered_predictions\": filtered_preds,\n",
    "        \"filtered_targets\": filtered_targets,\n",
    "        \"accuracy_nonzero\": accuracy_nonzero,\n",
    "        \"loss_decrease_percentage\": loss_decrease_percentage,\n",
    "        \"final_train_loss\": epoch_train_losses[-1] \n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b794e699-83fb-4ca2-8c93-318858018a2e",
   "metadata": {},
   "source": [
    "### **2nd Type of comparison:**\n",
    "\n",
    "Window sizes, for AAPL 10y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "672857df-0a56-49c8-a1f9-8972057d026b",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2, lstm\n",
      "/home/jupyter-tfg2425paula/prediction_project_v3/00_data/horizontal_structure/clean/single_name/AAPL/10y_2_data.pkl\n",
      "Training AAPL | LR: 0.01 | Epochs: 100 | Batch: 32 | Prediction Threshold: 0.5\n",
      "[Train] Epoch 5/100, Loss=0.6937\n",
      "[Train] Epoch 10/100, Loss=0.6937\n",
      "[Train] Epoch 15/100, Loss=0.6936\n",
      "[Train] Epoch 20/100, Loss=0.6936\n",
      "[Train] Epoch 25/100, Loss=0.6937\n",
      "[Train] Epoch 30/100, Loss=0.6936\n",
      "[Train] Epoch 35/100, Loss=0.6936\n",
      "[Train] Epoch 40/100, Loss=0.6935\n",
      "[Train] Epoch 45/100, Loss=0.6934\n",
      "[Train] Epoch 50/100, Loss=0.6933\n",
      "[Train] Epoch 55/100, Loss=0.6931\n",
      "[Train] Epoch 60/100, Loss=0.6930\n",
      "[Train] Epoch 65/100, Loss=0.6929\n",
      "[Train] Epoch 70/100, Loss=0.6927\n",
      "[Train] Epoch 75/100, Loss=0.6924\n",
      "[Train] Epoch 80/100, Loss=0.6922\n",
      "[Train] Epoch 85/100, Loss=0.6920\n",
      "[Train] Epoch 90/100, Loss=0.6918\n",
      "[Train] Epoch 95/100, Loss=0.6915\n",
      "[Train] Epoch 100/100, Loss=0.6913\n",
      "Accuracy on Nonzero Predictions: 0.4809\n",
      "2, gru\n",
      "/home/jupyter-tfg2425paula/prediction_project_v3/00_data/horizontal_structure/clean/single_name/AAPL/10y_2_data.pkl\n",
      "Training AAPL | LR: 0.01 | Epochs: 100 | Batch: 32 | Prediction Threshold: 0.5\n",
      "[Train] Epoch 5/100, Loss=0.6939\n",
      "[Train] Epoch 10/100, Loss=0.6936\n",
      "[Train] Epoch 15/100, Loss=0.6936\n",
      "[Train] Epoch 20/100, Loss=0.6936\n",
      "[Train] Epoch 25/100, Loss=0.6936\n",
      "[Train] Epoch 30/100, Loss=0.6936\n",
      "[Train] Epoch 35/100, Loss=0.6936\n",
      "[Train] Epoch 40/100, Loss=0.6936\n",
      "[Train] Epoch 45/100, Loss=0.6936\n",
      "[Train] Epoch 50/100, Loss=0.6936\n",
      "[Train] Epoch 55/100, Loss=0.6936\n",
      "[Train] Epoch 60/100, Loss=0.6936\n",
      "[Train] Epoch 65/100, Loss=0.6936\n",
      "[Train] Epoch 70/100, Loss=0.6937\n",
      "[Train] Epoch 75/100, Loss=0.6936\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [18], line 57\u001b[0m\n\u001b[1;32m     53\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mlearning_rate)\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstock\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | LR: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlearning_rate\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | Epochs: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     55\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m| Batch: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbatch_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | Prediction Threshold: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprediction_threshold\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 57\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_rolling_unchanged_model_threshold\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_resampled\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_resampled\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlower_threshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprediction_threshold\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m     \n\u001b[1;32m     70\u001b[0m rolling_predictions \u001b[38;5;241m=\u001b[39m result[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrolling_predictions\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     71\u001b[0m rolling_targets \u001b[38;5;241m=\u001b[39m result[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrolling_targets\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "Cell \u001b[0;32mIn [13], line 80\u001b[0m, in \u001b[0;36mevaluate_rolling_unchanged_model_threshold\u001b[0;34m(model, X, y, criterion, optimizer, device, train_size, batch_size, num_epochs, lower_threshold)\u001b[0m\n\u001b[1;32m     77\u001b[0m X_batch \u001b[38;5;241m=\u001b[39m X_batch\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     78\u001b[0m y_batch \u001b[38;5;241m=\u001b[39m y_batch\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 80\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzero_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     81\u001b[0m pred_y \u001b[38;5;241m=\u001b[39m model(X_batch)   \u001b[38;5;66;03m# [batch_size, num_classes]\u001b[39;00m\n\u001b[1;32m     82\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(pred_y, y_batch)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/_compile.py:32\u001b[0m, in \u001b[0;36m_disable_dynamo.<locals>.inner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     29\u001b[0m     disable_fn \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mdisable(fn, recursive)\n\u001b[1;32m     30\u001b[0m     fn\u001b[38;5;241m.\u001b[39m__dynamo_disable \u001b[38;5;241m=\u001b[39m disable_fn\n\u001b[0;32m---> 32\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdisable_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/_dynamo/eval_frame.py:632\u001b[0m, in \u001b[0;36mDisableContext.__call__.<locals>._fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    630\u001b[0m prior \u001b[38;5;241m=\u001b[39m _maybe_set_eval_frame(callback)\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 632\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    634\u001b[0m     _maybe_set_eval_frame(prior)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/optim/optimizer.py:952\u001b[0m, in \u001b[0;36mOptimizer.zero_grad\u001b[0;34m(self, set_to_none)\u001b[0m\n\u001b[1;32m    950\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m p\u001b[38;5;241m.\u001b[39mgrad \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    951\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m set_to_none:\n\u001b[0;32m--> 952\u001b[0m         \u001b[43mp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrad\u001b[49m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    953\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    954\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m p\u001b[38;5;241m.\u001b[39mgrad\u001b[38;5;241m.\u001b[39mgrad_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "results_list = []\n",
    "stock = \"AAPL\"\n",
    "period = \"10y\"\n",
    "possible_train_size = 95\n",
    "window_size = 3\n",
    "batch_size = 16\n",
    "num_epochs = 100\n",
    "\n",
    "prediction_threshold = 0.5\n",
    "\n",
    "output_folder = os.path.join(results_dir, f\"inidividual_trials\") \n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "processing = \"clean\"\n",
    "security_type = \"single_name\"\n",
    "window_sizes = [2, 3, 4]\n",
    "\n",
    "learning_rate = 0.01\n",
    "\n",
    "results_list = []\n",
    "for window_size in window_sizes:\n",
    "    initial_data_dir = os.path.join(project_dir, f\"00_data/{processing}\") \n",
    "    for model_type in model_types:\n",
    "\n",
    "        # Load original data (info only)\n",
    "        filename = f\"{security_type}/{stock}/{period}_data.csv\"\n",
    "        original_input_filepath = os.path.join(initial_data_dir, filename)\n",
    "        original_data = pd.read_csv(original_input_filepath)\n",
    "\n",
    "        # Iterate over window sizes\n",
    "        print(f\"{window_size}, {model_type}\")\n",
    "\n",
    "        # Load data using the 'processing' variable in path\n",
    "        pkl_filename = f\"{processing}/{security_type}/{stock}/{period}_{window_size}_data.pkl\"\n",
    "        input_filepath = os.path.join(horizontal_data_dir, pkl_filename)\n",
    "        print(input_filepath)\n",
    "        input_df = pd.read_pickle(input_filepath)\n",
    "\n",
    "        X_resampled, y_resampled = reshape_remove_characters(input_df)\n",
    "\n",
    "        input_size = X_resampled.shape[2]\n",
    "        train_size = int(X_resampled.shape[0] * possible_train_size / 100)\n",
    "        test_size = X_resampled.shape[0] - train_size\n",
    "\n",
    "        # Generate model\n",
    "        if model_type == \"gru\":\n",
    "            model = GRU3DClassifier(input_size, hidden_size, output_size, num_layers, dropout)\n",
    "        elif model_type == \"lstm\":\n",
    "            model = StockPriceLSTM(input_size, hidden_size, output_size)\n",
    "\n",
    "        model = model.to(device)\n",
    "\n",
    "        optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "        print(f\"Training {stock} | LR: {learning_rate} | Epochs: {num_epochs} \"\n",
    "              f\"| Batch: {batch_size} | Prediction Threshold: {prediction_threshold}\")\n",
    "\n",
    "        result = evaluate_rolling_unchanged_model_threshold(\n",
    "            model, \n",
    "            X_resampled, \n",
    "            y_resampled, \n",
    "            criterion, \n",
    "            optimizer, \n",
    "            device, \n",
    "            train_size, \n",
    "            batch_size, \n",
    "            num_epochs, \n",
    "            lower_threshold=prediction_threshold\n",
    "        )     \n",
    "\n",
    "        rolling_predictions = result[\"rolling_predictions\"]\n",
    "        rolling_targets = result[\"rolling_targets\"]\n",
    "        test_accuracy = result[\"accuracy_nonzero\"]\n",
    "        loss_decrease_percentage = result[\"loss_decrease_percentage\"]\n",
    "        nonzero_preds = np.count_nonzero(rolling_predictions)\n",
    "        final_train_loss = result[\"final_train_loss\"]\n",
    "        \n",
    "        # 1) Create a record (dictionary) for this run\n",
    "        run_record = {\"STOCK\": stock,\n",
    "            \"DATA_TYPE\": security_type,\n",
    "            \"MODEL\": model_type.upper(),  # Convert to uppercase for consistency\n",
    "            \"PROCESSING\": processing,\n",
    "            \"ACCURACY\": test_accuracy,\n",
    "            \"TRAIN_PCT_DECREASE\": loss_decrease_percentage,\n",
    "            \"FINAL_TRAIN_LOSS\": final_train_loss}\n",
    "\n",
    "        # 2) Append to list\n",
    "        results_list.append(run_record)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "935b7698-c75e-496b-8850-8053044ce7f9",
   "metadata": {},
   "source": [
    "### **Plot curves**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8953eb19-f0c8-4c66-bdfa-9bd8a8a92b1a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def evaluate_rolling_unchanged_model_threshold(\n",
    "    model, \n",
    "    X, \n",
    "    y, \n",
    "    criterion, \n",
    "    optimizer, \n",
    "    device, \n",
    "    train_size, \n",
    "    batch_size, \n",
    "    num_epochs, \n",
    "    lower_threshold,\n",
    "    plots_dir=None,\n",
    "    plot_filename=None\n",
    "):\n",
    "\n",
    "    # -------------------------------\n",
    "    # 0) Prepare Tensors & Splits\n",
    "    # -------------------------------\n",
    "    X = torch.tensor(X, dtype=torch.float32)\n",
    "    y = torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "    total_size = len(X)\n",
    "    # Determine actual train_size index\n",
    "    if train_size < 1.0:\n",
    "        lower_bound = int(train_size * total_size)\n",
    "    else:\n",
    "        lower_bound = train_size\n",
    "\n",
    "    # Training portion\n",
    "    X_train = X[:lower_bound].to(device)\n",
    "    y_train = y[:lower_bound].to(device)\n",
    "\n",
    "    train_dataset = TensorDataset(X_train, y_train)\n",
    "    trainloader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,   # Set True if you prefer shuffling\n",
    "        drop_last=False\n",
    "    )\n",
    "\n",
    "    if lower_bound < total_size:\n",
    "        X_val = X[lower_bound:].to(device)\n",
    "        y_val = y[lower_bound:].to(device)\n",
    "        val_dataset = TensorDataset(X_val, y_val)\n",
    "        valloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    else:\n",
    "        # If there's no leftover data for \"test\", handle gracefully\n",
    "        X_val = None\n",
    "        y_val = None\n",
    "        valloader = None\n",
    "\n",
    "    # ---------------------------------------------\n",
    "    # 1) SINGLE TRAINING PHASE + Track Loss Curves\n",
    "    # ---------------------------------------------\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "\n",
    "    epoch_train_losses = []\n",
    "    epoch_test_losses = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # TRAINING PASS\n",
    "        epoch_loss = 0.0\n",
    "        for X_batch, y_batch in trainloader:\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward\n",
    "            pred_y = model(X_batch)\n",
    "            loss = criterion(pred_y, y_batch)\n",
    "\n",
    "            # Backprop & update\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  # optional\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = epoch_loss / len(trainloader)\n",
    "        epoch_train_losses.append(avg_train_loss)\n",
    "\n",
    "        # VALIDATION PASS (Optional but needed to get test_loss_curve)\n",
    "        if valloader is not None:\n",
    "            model.eval()\n",
    "            val_loss = 0.0\n",
    "            with torch.no_grad():\n",
    "                for Xb, yb in valloader:\n",
    "                    pred_yb = model(Xb)\n",
    "                    loss_b = criterion(pred_yb, yb)\n",
    "                    val_loss += loss_b.item()\n",
    "            avg_val_loss = val_loss / len(valloader)\n",
    "            epoch_test_losses.append(avg_val_loss)\n",
    "\n",
    "            model.train()  # Switch back to train mode\n",
    "\n",
    "        else:\n",
    "            # If no validation set, just store None or 0\n",
    "            epoch_test_losses.append(None)\n",
    "\n",
    "        # Print progress every 5 epochs or last epoch\n",
    "        if (epoch + 1) % 5 == 0 or (epoch == num_epochs - 1):\n",
    "            if epoch_test_losses[-1] is not None:\n",
    "                print(f\"[Epoch {epoch+1}/{num_epochs}] \"\n",
    "                      f\"Train Loss: {avg_train_loss:.4f}, \"\n",
    "                      f\"Val Loss: {avg_val_loss:.4f}\")\n",
    "            else:\n",
    "                print(f\"[Epoch {epoch+1}/{num_epochs}] \"\n",
    "                      f\"Train Loss: {avg_train_loss:.4f}\")\n",
    "\n",
    "    # % decrease from first to last train loss\n",
    "    if len(epoch_train_losses) > 1:\n",
    "        loss_decrease_percentage = ((epoch_train_losses[-1] - epoch_train_losses[0])\n",
    "                                    / epoch_train_losses[0]) * 100\n",
    "    else:\n",
    "        loss_decrease_percentage = 0.0\n",
    "\n",
    "    final_train_loss = epoch_train_losses[-1]\n",
    "\n",
    "    # -------------------------------\n",
    "    # 2) ROLLING PREDICTIONS, NO UPDATE\n",
    "    # -------------------------------\n",
    "    model.eval()\n",
    "    rolling_predictions = []\n",
    "    rolling_targets = []\n",
    "\n",
    "    for i in range(lower_bound, total_size):\n",
    "        X_test = X[i:i+1].to(device)\n",
    "        y_test = y[i:i+1].to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            pred_y = model(X_test)\n",
    "            probabilities = torch.softmax(pred_y, dim=1).cpu().numpy()\n",
    "            prob_class_1 = probabilities[:, 1]  # shape: (1,)\n",
    "\n",
    "            # Threshold-based logic\n",
    "            pred_classes = np.zeros_like(prob_class_1)\n",
    "            # Predict -1 if prob < lower_threshold\n",
    "            pred_classes[prob_class_1 < lower_threshold] = -1\n",
    "            # Predict +1 if prob > (1 - lower_threshold)\n",
    "            pred_classes[prob_class_1 > (1 - lower_threshold)] = 1\n",
    "\n",
    "        rolling_predictions.append(pred_classes[0])\n",
    "        rolling_targets.append(y_test.item())\n",
    "\n",
    "    rolling_predictions = np.array(rolling_predictions)\n",
    "    rolling_targets = np.array(rolling_targets).astype(int)\n",
    "\n",
    "    # If original labels might be {0,1}, adapt as needed\n",
    "    rolling_targets[rolling_targets == 0] = -1\n",
    "\n",
    "    # Filter out zero predictions\n",
    "    nonzero_mask = (rolling_predictions != 0)\n",
    "    filtered_preds = rolling_predictions[nonzero_mask]\n",
    "    filtered_targets = rolling_targets[nonzero_mask]\n",
    "\n",
    "    if len(filtered_preds) == 0:\n",
    "        accuracy_nonzero = None\n",
    "        print(\"No nonzero predictions, cannot compute thresholded accuracy.\")\n",
    "    else:\n",
    "        accuracy_nonzero = accuracy_score(filtered_targets, filtered_preds)\n",
    "        print(f\"Accuracy on Nonzero Predictions: {accuracy_nonzero:.4f}\")\n",
    "\n",
    "    # -------------------------------------------------\n",
    "    # 3) PLOT (if plots_dir is set and there's test data)\n",
    "    # -------------------------------------------------\n",
    "    if plots_dir is not None:\n",
    "        os.makedirs(plots_dir, exist_ok=True)\n",
    "\n",
    "        # If user didn't provide a filename, create a default\n",
    "        if plot_filename is None:\n",
    "            plot_filename = \"train_test_loss_curve.png\"\n",
    "        plot_path = os.path.join(plots_dir, plot_filename)\n",
    "\n",
    "        # Plot the training and validation (test) loss curves\n",
    "        plt.figure(figsize=(8, 5))\n",
    "        plt.plot(epoch_train_losses, label=\"Train Loss\")\n",
    "        # Only plot test loss if it isn't None\n",
    "        if any(x is not None for x in epoch_test_losses):\n",
    "            plt.plot(epoch_test_losses, label=\"Test Loss\")\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.title(\"Train vs. Test Loss per Epoch\")\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "\n",
    "        plt.savefig(plot_path)\n",
    "        plt.close()\n",
    "        print(f\"Loss curves saved to: {plot_path}\")\n",
    "\n",
    "    # ----------------\n",
    "    # 4) Return results\n",
    "    # ----------------\n",
    "    return {\n",
    "        \"rolling_predictions\": rolling_predictions,\n",
    "        \"rolling_targets\": rolling_targets,\n",
    "        \"filtered_predictions\": filtered_preds,\n",
    "        \"filtered_targets\": filtered_targets,\n",
    "        \"accuracy_nonzero\": accuracy_nonzero,\n",
    "        \"loss_decrease_percentage\": loss_decrease_percentage,\n",
    "        \"final_train_loss\": final_train_loss,\n",
    "        \"train_loss_curve\": epoch_train_losses,\n",
    "        \"test_loss_curve\": epoch_test_losses\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "152fb8d5-715f-4409-8b9f-8b01040b777b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----- WINDOW_SIZE: 2, MODEL_TYPE: lstm -----\n",
      "Training AAPL | LR: 0.01 | Epochs: 100 | Batch: 32 | Threshold: 0.5\n",
      "[Epoch 5/100] Train Loss: 0.6939, Val Loss: 0.6924\n",
      "[Epoch 10/100] Train Loss: 0.6938, Val Loss: 0.6925\n",
      "[Epoch 15/100] Train Loss: 0.6937, Val Loss: 0.6927\n",
      "[Epoch 20/100] Train Loss: 0.6936, Val Loss: 0.6931\n",
      "[Epoch 25/100] Train Loss: 0.6934, Val Loss: 0.6939\n",
      "[Epoch 30/100] Train Loss: 0.6930, Val Loss: 0.6947\n",
      "[Epoch 35/100] Train Loss: 0.6927, Val Loss: 0.6954\n",
      "[Epoch 40/100] Train Loss: 0.6925, Val Loss: 0.6958\n",
      "[Epoch 45/100] Train Loss: 0.6923, Val Loss: 0.6962\n",
      "[Epoch 50/100] Train Loss: 0.6921, Val Loss: 0.6965\n",
      "[Epoch 55/100] Train Loss: 0.6919, Val Loss: 0.6968\n",
      "[Epoch 60/100] Train Loss: 0.6918, Val Loss: 0.6971\n",
      "[Epoch 65/100] Train Loss: 0.6917, Val Loss: 0.6975\n",
      "[Epoch 70/100] Train Loss: 0.6915, Val Loss: 0.6980\n",
      "[Epoch 75/100] Train Loss: 0.6914, Val Loss: 0.6985\n",
      "[Epoch 80/100] Train Loss: 0.6913, Val Loss: 0.6990\n",
      "[Epoch 85/100] Train Loss: 0.6912, Val Loss: 0.6995\n",
      "[Epoch 90/100] Train Loss: 0.6911, Val Loss: 0.7000\n",
      "[Epoch 95/100] Train Loss: 0.6910, Val Loss: 0.7004\n",
      "[Epoch 100/100] Train Loss: 0.6909, Val Loss: 0.7008\n",
      "Accuracy on Nonzero Predictions: 0.5096\n",
      "Loss curves saved to: /home/jupyter-tfg2425paula/prediction_project_v3/03_plots/AAPL_lstm_win2_80_loss.png\n",
      "\n",
      "----- WINDOW_SIZE: 2, MODEL_TYPE: gru -----\n",
      "Training AAPL | LR: 0.01 | Epochs: 100 | Batch: 32 | Threshold: 0.5\n",
      "[Epoch 5/100] Train Loss: 0.6939, Val Loss: 0.6929\n",
      "[Epoch 10/100] Train Loss: 0.6938, Val Loss: 0.6922\n",
      "[Epoch 15/100] Train Loss: 0.6939, Val Loss: 0.6922\n",
      "[Epoch 20/100] Train Loss: 0.6939, Val Loss: 0.6923\n",
      "[Epoch 25/100] Train Loss: 0.6938, Val Loss: 0.6923\n",
      "[Epoch 30/100] Train Loss: 0.6939, Val Loss: 0.6923\n",
      "[Epoch 35/100] Train Loss: 0.6938, Val Loss: 0.6923\n",
      "[Epoch 40/100] Train Loss: 0.6939, Val Loss: 0.6924\n",
      "[Epoch 45/100] Train Loss: 0.6944, Val Loss: 0.6923\n",
      "[Epoch 50/100] Train Loss: 0.6939, Val Loss: 0.6928\n",
      "[Epoch 55/100] Train Loss: 0.6937, Val Loss: 0.6928\n",
      "[Epoch 60/100] Train Loss: 0.6936, Val Loss: 0.6928\n",
      "[Epoch 65/100] Train Loss: 0.6940, Val Loss: 0.6926\n",
      "[Epoch 70/100] Train Loss: 0.6937, Val Loss: 0.6925\n",
      "[Epoch 75/100] Train Loss: 0.6937, Val Loss: 0.6926\n",
      "[Epoch 80/100] Train Loss: 0.6937, Val Loss: 0.6934\n",
      "[Epoch 85/100] Train Loss: 0.6937, Val Loss: 0.6932\n",
      "[Epoch 90/100] Train Loss: 0.6932, Val Loss: 0.6946\n",
      "[Epoch 95/100] Train Loss: 0.6935, Val Loss: 0.6936\n",
      "[Epoch 100/100] Train Loss: 0.6932, Val Loss: 0.6935\n",
      "Accuracy on Nonzero Predictions: 0.5192\n",
      "Loss curves saved to: /home/jupyter-tfg2425paula/prediction_project_v3/03_plots/AAPL_gru_win2_80_loss.png\n",
      "\n",
      "----- WINDOW_SIZE: 3, MODEL_TYPE: lstm -----\n",
      "Training AAPL | LR: 0.01 | Epochs: 100 | Batch: 32 | Threshold: 0.5\n",
      "[Epoch 5/100] Train Loss: 0.6938, Val Loss: 0.6926\n",
      "[Epoch 10/100] Train Loss: 0.6925, Val Loss: 0.6948\n",
      "[Epoch 15/100] Train Loss: 0.6911, Val Loss: 0.6966\n",
      "[Epoch 20/100] Train Loss: 0.6902, Val Loss: 0.6977\n",
      "[Epoch 25/100] Train Loss: 0.6896, Val Loss: 0.6988\n",
      "[Epoch 30/100] Train Loss: 0.6888, Val Loss: 0.6999\n",
      "[Epoch 35/100] Train Loss: 0.6880, Val Loss: 0.7010\n",
      "[Epoch 40/100] Train Loss: 0.6870, Val Loss: 0.7021\n",
      "[Epoch 45/100] Train Loss: 0.6861, Val Loss: 0.7028\n",
      "[Epoch 50/100] Train Loss: 0.6852, Val Loss: 0.7033\n",
      "[Epoch 55/100] Train Loss: 0.6842, Val Loss: 0.7036\n",
      "[Epoch 60/100] Train Loss: 0.6833, Val Loss: 0.7045\n",
      "[Epoch 65/100] Train Loss: 0.6823, Val Loss: 0.7056\n",
      "[Epoch 70/100] Train Loss: 0.6814, Val Loss: 0.7072\n",
      "[Epoch 75/100] Train Loss: 0.6806, Val Loss: 0.7085\n",
      "[Epoch 80/100] Train Loss: 0.6799, Val Loss: 0.7097\n",
      "[Epoch 85/100] Train Loss: 0.6790, Val Loss: 0.7107\n",
      "[Epoch 90/100] Train Loss: 0.6782, Val Loss: 0.7114\n",
      "[Epoch 95/100] Train Loss: 0.6772, Val Loss: 0.7118\n",
      "[Epoch 100/100] Train Loss: 0.6757, Val Loss: 0.7123\n",
      "Accuracy on Nonzero Predictions: 0.4808\n",
      "Loss curves saved to: /home/jupyter-tfg2425paula/prediction_project_v3/03_plots/AAPL_lstm_win3_80_loss.png\n",
      "\n",
      "----- WINDOW_SIZE: 3, MODEL_TYPE: gru -----\n",
      "Training AAPL | LR: 0.01 | Epochs: 100 | Batch: 32 | Threshold: 0.5\n",
      "[Epoch 5/100] Train Loss: 0.6941, Val Loss: 0.6934\n",
      "[Epoch 10/100] Train Loss: 0.6938, Val Loss: 0.6923\n",
      "[Epoch 15/100] Train Loss: 0.6936, Val Loss: 0.6925\n",
      "[Epoch 20/100] Train Loss: 0.6935, Val Loss: 0.6937\n",
      "[Epoch 25/100] Train Loss: 0.6930, Val Loss: 0.6924\n",
      "[Epoch 30/100] Train Loss: 0.6922, Val Loss: 0.6906\n",
      "[Epoch 35/100] Train Loss: 0.6920, Val Loss: 0.6907\n",
      "[Epoch 40/100] Train Loss: 0.6889, Val Loss: 0.6928\n",
      "[Epoch 45/100] Train Loss: 0.6901, Val Loss: 0.6946\n",
      "[Epoch 50/100] Train Loss: 0.6876, Val Loss: 0.6939\n",
      "[Epoch 55/100] Train Loss: 0.6864, Val Loss: 0.6955\n",
      "[Epoch 60/100] Train Loss: 0.6847, Val Loss: 0.6994\n",
      "[Epoch 65/100] Train Loss: 0.6837, Val Loss: 0.6973\n",
      "[Epoch 70/100] Train Loss: 0.6812, Val Loss: 0.7040\n",
      "[Epoch 75/100] Train Loss: 0.6813, Val Loss: 0.7109\n",
      "[Epoch 80/100] Train Loss: 0.6809, Val Loss: 0.7097\n",
      "[Epoch 85/100] Train Loss: 0.6782, Val Loss: 0.7070\n",
      "[Epoch 90/100] Train Loss: 0.6742, Val Loss: 0.7161\n",
      "[Epoch 95/100] Train Loss: 0.6749, Val Loss: 0.7204\n",
      "[Epoch 100/100] Train Loss: 0.6736, Val Loss: 0.7172\n",
      "Accuracy on Nonzero Predictions: 0.5211\n",
      "Loss curves saved to: /home/jupyter-tfg2425paula/prediction_project_v3/03_plots/AAPL_gru_win3_80_loss.png\n",
      "\n",
      "----- WINDOW_SIZE: 4, MODEL_TYPE: lstm -----\n",
      "Training AAPL | LR: 0.01 | Epochs: 100 | Batch: 32 | Threshold: 0.5\n",
      "[Epoch 5/100] Train Loss: 0.6921, Val Loss: 0.6955\n",
      "[Epoch 10/100] Train Loss: 0.6884, Val Loss: 0.7010\n",
      "[Epoch 15/100] Train Loss: 0.6854, Val Loss: 0.7046\n",
      "[Epoch 20/100] Train Loss: 0.6826, Val Loss: 0.7079\n",
      "[Epoch 25/100] Train Loss: 0.6784, Val Loss: 0.7138\n",
      "[Epoch 30/100] Train Loss: 0.6753, Val Loss: 0.7179\n",
      "[Epoch 35/100] Train Loss: 0.6727, Val Loss: 0.7209\n",
      "[Epoch 40/100] Train Loss: 0.6693, Val Loss: 0.7230\n",
      "[Epoch 45/100] Train Loss: 0.6656, Val Loss: 0.7243\n",
      "[Epoch 50/100] Train Loss: 0.6605, Val Loss: 0.7261\n",
      "[Epoch 55/100] Train Loss: 0.6553, Val Loss: 0.7299\n",
      "[Epoch 60/100] Train Loss: 0.6506, Val Loss: 0.7352\n",
      "[Epoch 65/100] Train Loss: 0.6470, Val Loss: 0.7389\n",
      "[Epoch 70/100] Train Loss: 0.6437, Val Loss: 0.7426\n",
      "[Epoch 75/100] Train Loss: 0.6403, Val Loss: 0.7454\n",
      "[Epoch 80/100] Train Loss: 0.6374, Val Loss: 0.7472\n",
      "[Epoch 85/100] Train Loss: 0.6351, Val Loss: 0.7490\n",
      "[Epoch 90/100] Train Loss: 0.6322, Val Loss: 0.7501\n",
      "[Epoch 95/100] Train Loss: 0.6295, Val Loss: 0.7511\n",
      "[Epoch 100/100] Train Loss: 0.6266, Val Loss: 0.7536\n",
      "Accuracy on Nonzero Predictions: 0.4808\n",
      "Loss curves saved to: /home/jupyter-tfg2425paula/prediction_project_v3/03_plots/AAPL_lstm_win4_80_loss.png\n",
      "\n",
      "----- WINDOW_SIZE: 4, MODEL_TYPE: gru -----\n",
      "Training AAPL | LR: 0.01 | Epochs: 100 | Batch: 32 | Threshold: 0.5\n",
      "[Epoch 5/100] Train Loss: 0.6940, Val Loss: 0.6932\n",
      "[Epoch 10/100] Train Loss: 0.6910, Val Loss: 0.6971\n",
      "[Epoch 15/100] Train Loss: 0.6893, Val Loss: 0.7035\n",
      "[Epoch 20/100] Train Loss: 0.6885, Val Loss: 0.7077\n",
      "[Epoch 25/100] Train Loss: 0.6831, Val Loss: 0.7061\n",
      "[Epoch 30/100] Train Loss: 0.6795, Val Loss: 0.7165\n",
      "[Epoch 35/100] Train Loss: 0.6724, Val Loss: 0.7192\n",
      "[Epoch 40/100] Train Loss: 0.6716, Val Loss: 0.7294\n",
      "[Epoch 45/100] Train Loss: 0.6631, Val Loss: 0.7353\n",
      "[Epoch 50/100] Train Loss: 0.6583, Val Loss: 0.7459\n",
      "[Epoch 55/100] Train Loss: 0.6492, Val Loss: 0.7519\n",
      "[Epoch 60/100] Train Loss: 0.6423, Val Loss: 0.7786\n",
      "[Epoch 65/100] Train Loss: 0.6435, Val Loss: 0.7872\n",
      "[Epoch 70/100] Train Loss: 0.6289, Val Loss: 0.7986\n",
      "[Epoch 75/100] Train Loss: 0.6330, Val Loss: 0.8021\n",
      "[Epoch 80/100] Train Loss: 0.6192, Val Loss: 0.8015\n",
      "[Epoch 85/100] Train Loss: 0.6171, Val Loss: 0.8150\n",
      "[Epoch 90/100] Train Loss: 0.6190, Val Loss: 0.8046\n",
      "[Epoch 95/100] Train Loss: 0.5978, Val Loss: 0.8370\n",
      "[Epoch 100/100] Train Loss: 0.6026, Val Loss: 0.8483\n",
      "Accuracy on Nonzero Predictions: 0.4425\n",
      "Loss curves saved to: /home/jupyter-tfg2425paula/prediction_project_v3/03_plots/AAPL_gru_win4_80_loss.png\n",
      "\n",
      "----- WINDOW_SIZE: 5, MODEL_TYPE: lstm -----\n",
      "Training AAPL | LR: 0.01 | Epochs: 100 | Batch: 32 | Threshold: 0.5\n",
      "[Epoch 5/100] Train Loss: 0.6933, Val Loss: 0.6948\n",
      "[Epoch 10/100] Train Loss: 0.6905, Val Loss: 0.6970\n",
      "[Epoch 15/100] Train Loss: 0.6855, Val Loss: 0.7015\n",
      "[Epoch 20/100] Train Loss: 0.6794, Val Loss: 0.7056\n",
      "[Epoch 25/100] Train Loss: 0.6735, Val Loss: 0.7095\n",
      "[Epoch 30/100] Train Loss: 0.6676, Val Loss: 0.7177\n",
      "[Epoch 35/100] Train Loss: 0.6614, Val Loss: 0.7284\n",
      "[Epoch 40/100] Train Loss: 0.6538, Val Loss: 0.7377\n",
      "[Epoch 45/100] Train Loss: 0.6472, Val Loss: 0.7450\n",
      "[Epoch 50/100] Train Loss: 0.6405, Val Loss: 0.7511\n",
      "[Epoch 55/100] Train Loss: 0.6336, Val Loss: 0.7583\n",
      "[Epoch 60/100] Train Loss: 0.6269, Val Loss: 0.7641\n",
      "[Epoch 65/100] Train Loss: 0.6199, Val Loss: 0.7694\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [23], line 46\u001b[0m\n\u001b[1;32m     41\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mlearning_rate)\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstock\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | LR: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlearning_rate\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | Epochs: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     43\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m| Batch: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbatch_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | Threshold: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprediction_threshold\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 46\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_rolling_unchanged_model_threshold\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_resampled\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_resampled\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlower_threshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[43m    \u001b[49m\u001b[43mplots_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mplots_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m                \u001b[49m\u001b[38;5;66;43;03m# folder where plots are saved\u001b[39;49;00m\n\u001b[1;32m     58\u001b[0m \u001b[43m    \u001b[49m\u001b[43mplot_filename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mAAPL_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mmodel_type\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_win\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mwindow_size\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mpossible_train_size\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_loss.png\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# optional custom filename\u001b[39;49;00m\n\u001b[1;32m     59\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;66;03m# 7) Extract results\u001b[39;00m\n\u001b[1;32m     62\u001b[0m rolling_predictions \u001b[38;5;241m=\u001b[39m result[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrolling_predictions\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "Cell \u001b[0;32mIn [19], line 71\u001b[0m, in \u001b[0;36mevaluate_rolling_unchanged_model_threshold\u001b[0;34m(model, X, y, criterion, optimizer, device, train_size, batch_size, num_epochs, lower_threshold, plots_dir, plot_filename)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# TRAINING PASS\u001b[39;00m\n\u001b[1;32m     70\u001b[0m     epoch_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[0;32m---> 71\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m X_batch, y_batch \u001b[38;5;129;01min\u001b[39;00m trainloader:\n\u001b[1;32m     72\u001b[0m         optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     74\u001b[0m         \u001b[38;5;66;03m# Forward\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    707\u001b[0m ):\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py:757\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    755\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    756\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 757\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    758\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    759\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/utils/data/dataset.py:211\u001b[0m, in \u001b[0;36mTensorDataset.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, index):\n\u001b[0;32m--> 211\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m[\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/utils/data/dataset.py:211\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, index):\n\u001b[0;32m--> 211\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(\u001b[43mtensor\u001b[49m\u001b[43m[\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m tensor \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtensors)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "processing = \"clean\"\n",
    "security_type = \"single_name\"\n",
    "window_sizes = [2, 3, 4, 5, 10]\n",
    "possible_train_size = 80\n",
    "\n",
    "results_list = []\n",
    "\n",
    "# Main loop\n",
    "for window_size in window_sizes:\n",
    "    initial_data_dir = os.path.join(project_dir, f\"00_data/{processing}\") \n",
    "    for model_type in model_types:\n",
    "\n",
    "        # 1) Load original data (info only)\n",
    "        filename = f\"{security_type}/{stock}/{period}_data.csv\"\n",
    "        original_input_filepath = os.path.join(initial_data_dir, filename)\n",
    "        original_data = pd.read_csv(original_input_filepath)\n",
    "\n",
    "        print(f\"\\n----- WINDOW_SIZE: {window_size}, MODEL_TYPE: {model_type} -----\")\n",
    "\n",
    "        # 2) Load the preprocessed data\n",
    "        pkl_filename = f\"{processing}/{security_type}/{stock}/{period}_{window_size}_data.pkl\"\n",
    "        input_filepath = os.path.join(horizontal_data_dir, pkl_filename)\n",
    "        input_df = pd.read_pickle(input_filepath)\n",
    "\n",
    "        # 3) Reshape\n",
    "        X_resampled, y_resampled = reshape_remove_characters(input_df)\n",
    "\n",
    "        input_size = X_resampled.shape[2]\n",
    "        train_size = int(X_resampled.shape[0] * possible_train_size / 100)\n",
    "        test_size = X_resampled.shape[0] - train_size\n",
    "\n",
    "        # 4) Initialize the model\n",
    "        if model_type == \"gru\":\n",
    "            model = GRU3DClassifier(input_size, hidden_size, output_size, num_layers, dropout)\n",
    "        elif model_type == \"lstm\":\n",
    "            model = StockPriceLSTM(input_size, hidden_size, output_size)\n",
    "\n",
    "        model = model.to(device)\n",
    "\n",
    "        # 5) Set up optimizer\n",
    "        optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "        print(f\"Training {stock} | LR: {learning_rate} | Epochs: {num_epochs} \"\n",
    "              f\"| Batch: {batch_size} | Threshold: {prediction_threshold}\")\n",
    "\n",
    "        \n",
    "        result = evaluate_rolling_unchanged_model_threshold(\n",
    "            model=model,\n",
    "            X=X_resampled,\n",
    "            y=y_resampled,\n",
    "            criterion=criterion,\n",
    "            optimizer=optimizer,\n",
    "            device=device,\n",
    "            train_size=train_size,\n",
    "            batch_size=batch_size,\n",
    "            num_epochs=num_epochs,\n",
    "            lower_threshold=0.5,\n",
    "            plots_dir=plots_dir,                # folder where plots are saved\n",
    "            plot_filename=f\"AAPL_{model_type}_win{window_size}_{possible_train_size}_loss.png\" # optional custom filename\n",
    "        )\n",
    "\n",
    "        # 7) Extract results\n",
    "        rolling_predictions = result[\"rolling_predictions\"]\n",
    "        rolling_targets = result[\"rolling_targets\"]\n",
    "        test_accuracy = result[\"accuracy_nonzero\"]\n",
    "        loss_decrease_percentage = result[\"loss_decrease_percentage\"]\n",
    "        nonzero_preds = np.count_nonzero(rolling_predictions)\n",
    "        final_train_loss = result[\"final_train_loss\"]\n",
    "\n",
    "        # 9) Create a record (dictionary) for this run\n",
    "        run_record = {\n",
    "            \"STOCK\": stock,\n",
    "            \"DATA_TYPE\": security_type,\n",
    "            \"MODEL\": model_type.upper(),\n",
    "            \"PROCESSING\": processing,\n",
    "            \"ACCURACY\": test_accuracy,\n",
    "            \"TRAIN_PCT_DECREASE\": loss_decrease_percentage,\n",
    "            \"FINAL_TRAIN_LOSS\": final_train_loss\n",
    "        }\n",
    "\n",
    "        # 10) Append to the results_list\n",
    "        results_list.append(run_record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d01c33e-20c2-4d79-9860-260ca61c8566",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'epoch_train_losses' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [54], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mepoch_train_losses\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'epoch_train_losses' is not defined"
     ]
    }
   ],
   "source": [
    "epoch_train_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a1abaee6-0df1-44b7-b371-b87b92bff765",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_csv_path = os.path.join(output_folder, f\"02_{stock}_window_sizes.csv\")\n",
    "\n",
    "if len(results_list) > 0:\n",
    "    df = pd.DataFrame(results_list)\n",
    "\n",
    "    if os.path.exists(results_csv_path):\n",
    "        # Append without header\n",
    "        df.to_csv(results_csv_path, mode='a', header=False, index=False)\n",
    "    else:\n",
    "        # Write new file with header\n",
    "        df.to_csv(results_csv_path, index=False)\n",
    "\n",
    "    # Clear the list before next iteration\n",
    "    results_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "796b209d-9eed-4763-9567-5e143e2577b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/jupyter-tfg2425paula/prediction_project_v3/02_results/inidividual_trials/01_AAPL_proc_model_type.csv'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_csv_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1d49af4d-56cf-43c2-9c78-1d5c472140ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>STOCK</th>\n",
       "      <th>DATA_TYPE</th>\n",
       "      <th>MODEL</th>\n",
       "      <th>PROCESSING</th>\n",
       "      <th>ACCURACY</th>\n",
       "      <th>TRAIN_PCT_DECREASE</th>\n",
       "      <th>FINAL_TRAIN_LOSS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>single_name</td>\n",
       "      <td>LSTM</td>\n",
       "      <td>clean</td>\n",
       "      <td>0.511450</td>\n",
       "      <td>-13.434468</td>\n",
       "      <td>0.602222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>single_name</td>\n",
       "      <td>GRU</td>\n",
       "      <td>clean</td>\n",
       "      <td>0.427481</td>\n",
       "      <td>-17.854286</td>\n",
       "      <td>0.571629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>single_name</td>\n",
       "      <td>LSTM</td>\n",
       "      <td>clean</td>\n",
       "      <td>0.427481</td>\n",
       "      <td>-92.449701</td>\n",
       "      <td>0.052604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>single_name</td>\n",
       "      <td>GRU</td>\n",
       "      <td>clean</td>\n",
       "      <td>0.465649</td>\n",
       "      <td>-54.370292</td>\n",
       "      <td>0.317963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>single_name</td>\n",
       "      <td>LSTM</td>\n",
       "      <td>clean</td>\n",
       "      <td>0.496124</td>\n",
       "      <td>-99.997286</td>\n",
       "      <td>0.000019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>single_name</td>\n",
       "      <td>GRU</td>\n",
       "      <td>clean</td>\n",
       "      <td>0.558140</td>\n",
       "      <td>-91.448532</td>\n",
       "      <td>0.059744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>single_name</td>\n",
       "      <td>LSTM</td>\n",
       "      <td>clean</td>\n",
       "      <td>0.531746</td>\n",
       "      <td>-99.998481</td>\n",
       "      <td>0.000011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>single_name</td>\n",
       "      <td>GRU</td>\n",
       "      <td>clean</td>\n",
       "      <td>0.476190</td>\n",
       "      <td>-96.843120</td>\n",
       "      <td>0.022098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>single_name</td>\n",
       "      <td>LSTM</td>\n",
       "      <td>clean</td>\n",
       "      <td>0.518868</td>\n",
       "      <td>-99.999147</td>\n",
       "      <td>0.000006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>single_name</td>\n",
       "      <td>GRU</td>\n",
       "      <td>clean</td>\n",
       "      <td>0.613208</td>\n",
       "      <td>-96.695056</td>\n",
       "      <td>0.023305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>single_name</td>\n",
       "      <td>LSTM</td>\n",
       "      <td>clean</td>\n",
       "      <td>0.506173</td>\n",
       "      <td>-99.998694</td>\n",
       "      <td>0.000009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>single_name</td>\n",
       "      <td>GRU</td>\n",
       "      <td>clean</td>\n",
       "      <td>0.617284</td>\n",
       "      <td>-97.018625</td>\n",
       "      <td>0.020974</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   STOCK    DATA_TYPE MODEL PROCESSING  ACCURACY  TRAIN_PCT_DECREASE  \\\n",
       "0   AAPL  single_name  LSTM      clean  0.511450          -13.434468   \n",
       "1   AAPL  single_name   GRU      clean  0.427481          -17.854286   \n",
       "2   AAPL  single_name  LSTM      clean  0.427481          -92.449701   \n",
       "3   AAPL  single_name   GRU      clean  0.465649          -54.370292   \n",
       "4   AAPL  single_name  LSTM      clean  0.496124          -99.997286   \n",
       "5   AAPL  single_name   GRU      clean  0.558140          -91.448532   \n",
       "6   AAPL  single_name  LSTM      clean  0.531746          -99.998481   \n",
       "7   AAPL  single_name   GRU      clean  0.476190          -96.843120   \n",
       "8   AAPL  single_name  LSTM      clean  0.518868          -99.999147   \n",
       "9   AAPL  single_name   GRU      clean  0.613208          -96.695056   \n",
       "10  AAPL  single_name  LSTM      clean  0.506173          -99.998694   \n",
       "11  AAPL  single_name   GRU      clean  0.617284          -97.018625   \n",
       "\n",
       "    FINAL_TRAIN_LOSS  \n",
       "0           0.602222  \n",
       "1           0.571629  \n",
       "2           0.052604  \n",
       "3           0.317963  \n",
       "4           0.000019  \n",
       "5           0.059744  \n",
       "6           0.000011  \n",
       "7           0.022098  \n",
       "8           0.000006  \n",
       "9           0.023305  \n",
       "10          0.000009  \n",
       "11          0.020974  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a53216d9-1202-440c-a272-ffb244ed33a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
