{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "940889cf-06e0-4e8c-8efa-1eeacc97ca57",
   "metadata": {},
   "source": [
    "## **This notebook aims to compare some models and store them in a reasonable fashion**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2e46c228-21f4-4e80-a4e8-b62981ab4799",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.metrics import accuracy_score\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7a11a538-40d5-49c3-92e9-e26fdcefe083",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"  # Use GPU 0 in first notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7b259658-5fed-4231-9f06-f9db567e1d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "project_dir = \"/home/jupyter-tfg2425paula/prediction_project_v3\"\n",
    "os.chdir(project_dir)\n",
    "\n",
    "clean_data_dir = os.path.join(project_dir, \"00_data/clean\")\n",
    "horizontal_data_dir = os.path.join(project_dir, \"00_data/horizontal_structure\")\n",
    "results_dir = os.path.join(project_dir, \"02_results\")\n",
    "pca_data_dir = os.path.join(project_dir, \"00_data/pca\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7319029-ba4c-4c90-91b9-a0a02b5be9af",
   "metadata": {},
   "source": [
    "### **GRU Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a7a1485d-99bb-4358-a216-1c1c75e50ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRU3DClassifier(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers, dropout):\n",
    "        super(GRU3DClassifier, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "\n",
    "        out, _ = self.gru(x, h0)\n",
    "        out = self.fc(out[:, -1, :]) \n",
    "        # return self.sigmoid(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3061a6b4-dad3-4129-a2ed-0098e04440e4",
   "metadata": {},
   "source": [
    "### **LSTM Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "95ffa6e2-2c35-48de-8c95-b7ce86fe9dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StockPriceLSTM(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers=1, dropout=0.0):\n",
    "        super(StockPriceLSTM, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers=num_layers, \n",
    "                            batch_first=True, dropout=dropout)\n",
    "    \n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)  # Get the batch size dynamically\n",
    "\n",
    "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_dim).to(x.device)  # (num_layers, batch_size, hidden_dim)\n",
    "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_dim).to(x.device)  # (num_layers, batch_size, hidden_dim)\n",
    "        \n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "\n",
    "        out = self.fc(out[:, -1, :]) \n",
    "        # out = self.sigmoid(out)\n",
    "        return out\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df6d4b78-1492-45ea-a9af-854d8d69b8de",
   "metadata": {},
   "source": [
    "### **Set folders**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e67b679f-204f-4e15-89fe-09879ed7246c",
   "metadata": {},
   "source": [
    "Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2b80ed82-98cd-453d-94e9-b7f42c61040e",
   "metadata": {},
   "outputs": [],
   "source": [
    "processing_types = [\"clean\", \"pca\"]\n",
    "processing_types= [\"clean\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da3781cf-0f0b-4858-bff8-09f839e2da04",
   "metadata": {},
   "source": [
    "Folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a2958856-f0de-4f93-85b5-9fdd169a2a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stocks = ['AAPL', 'MSFT', 'AMZN', 'NVDA', 'SPX']\n",
    "stocks = ['AAPL']\n",
    "# types_securities = [\"single_name\", \"options\", \"technical\"]\n",
    "types_securities = [\"options\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2e929df-0399-4e41-9b2b-1863f8c215b5",
   "metadata": {},
   "source": [
    "Different files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9b853df6-38b4-4bd7-85e5-6a232f17905e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# years = [\"15y\", \"10y\", \"5y\", \"2y\"]\n",
    "years = [\"10y\"]\n",
    "# window_sizes = [5, 10, 50, 100]\n",
    "window_sizes = [5]\n",
    "# train_sizes = [80, 90, 95]\n",
    "train_sizes = [95]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f9fb2cc-d8de-4e4f-be9d-f8cb7aac8e16",
   "metadata": {},
   "source": [
    "Same file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "145e5ba2-35c7-4a71-b100-624ca6ed1ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds = [0.3, 0.35, 0.4, 0.45, 0.5]\n",
    "thresholds = [0.5]\n",
    "learning_rates = [0.005, 0.008, 0.009, 0.01]\n",
    "learning_rates = [0.01]\n",
    "num_epochs_list = [100, 200]\n",
    "num_epochs_list = [100]\n",
    "batch_sizes = [16, 32]\n",
    "batch_sizes = [16]\n",
    "prediction_thresholds = [0.35, 0.4, 0.45, 0.5]\n",
    "prediction_thresholds = [0.5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbbee35f-0147-48b0-93ef-d9687984bbd0",
   "metadata": {},
   "source": [
    "#### **Model and Hyperparameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "71861bc1-7067-47f8-b3dd-a5cef2bc0f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "hidden_size = 64  \n",
    "output_size = 2  \n",
    "num_layers = 2\n",
    "dropout = 0.2\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "60900dea-a72e-4b53-979b-aded1e725489",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_types = [\"lstm\", \"gru\"]   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d873f70-b583-4e84-9289-bc303ef0446f",
   "metadata": {},
   "source": [
    "#### **Last data modifications**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "85729e3c-f060-4dc2-a4fc-a80573fefd73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_remove_characters(df):\n",
    "\n",
    "    X = np.array([np.stack(row) for row in df.drop(columns=['Target']).values])\n",
    "    y = df['Target'].values\n",
    "\n",
    "    smote = SMOTE(random_state=42)\n",
    "    n_samples, timesteps, n_features = X.shape\n",
    "    X_flat = X.reshape((n_samples, timesteps * n_features))\n",
    "    X_flat = np.where(X_flat == 'ç', 0, X_flat)\n",
    "\n",
    "    X_resampled = X_flat.reshape((-1, timesteps, n_features))\n",
    "    \n",
    "    return X_resampled, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88656719-d0c4-4bf2-aef1-31f00b167d2a",
   "metadata": {},
   "source": [
    "### **Evaluation function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "74c49327-1c0d-4706-9943-54b795cb0687",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_rolling_unchanged_model_threshold(\n",
    "    model, \n",
    "    X, \n",
    "    y, \n",
    "    criterion, \n",
    "    optimizer, \n",
    "    device, \n",
    "    train_size, \n",
    "    batch_size, \n",
    "    num_epochs, \n",
    "    lower_threshold\n",
    "):\n",
    "    \"\"\"\n",
    "    Evaluate a PyTorch model using a rolling prediction approach for time series,\n",
    "    training the model only once on the initial training set. For each time step\n",
    "    after train_size, the model makes a prediction without further parameter updates.\n",
    "    Only predicts +1 or -1 if the probability of class 1 is above/below given thresholds;\n",
    "    otherwise, predicts 0. Accuracy is computed only on nonzero predictions.\n",
    "\n",
    "    Args:\n",
    "        model:          PyTorch model to evaluate.\n",
    "        X:              Feature data (numpy array).\n",
    "        y:              Target data (numpy array).\n",
    "        criterion:      Loss function (e.g., CrossEntropyLoss).\n",
    "        optimizer:      Optimizer (e.g., Adam).\n",
    "        device:         Device for computation (CPU or GPU).\n",
    "        train_size:     Initial size of the training data (int or float).\n",
    "                        If < 1, treated as fraction of total length.\n",
    "        batch_size:     Batch size for training.\n",
    "        num_epochs:     Number of epochs for initial training only.\n",
    "        lower_threshold: Probability threshold below which model predicts -1.\n",
    "        upper_threshold: Probability threshold above which model predicts +1.\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary with the following keys:\n",
    "            - \"rolling_predictions\": All predictions (-1, 0, +1) across the test period.\n",
    "            - \"rolling_targets\": Corresponding true targets in [-1, +1].\n",
    "            - \"filtered_predictions\": Nonzero predictions only.\n",
    "            - \"filtered_targets\": Targets corresponding to nonzero predictions.\n",
    "            - \"accuracy_nonzero\": Accuracy computed only on nonzero predictions.\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert X, y to tensors\n",
    "    X = torch.tensor(X, dtype=torch.float32)\n",
    "    y = torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "    # Determine initial training set size\n",
    "    if train_size < 1.0:\n",
    "        lower_bound = int(train_size * len(X))\n",
    "    else:\n",
    "        lower_bound = train_size\n",
    "\n",
    "    # -------------------------\n",
    "    # 1) SINGLE TRAINING PHASE\n",
    "    # -------------------------\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "    \n",
    "    X_train = X[:lower_bound].to(device)\n",
    "    y_train = y[:lower_bound].to(device)\n",
    "\n",
    "    train_dataset = TensorDataset(X_train, y_train)\n",
    "    trainloader = DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size=batch_size, \n",
    "        shuffle=False,         # Keep False if order matters; True for better generalization\n",
    "        # num_workers=4,         # Adjust based on your CPU cores\n",
    "        # pin_memory=True,       # Speeds up transfer if using GPUs\n",
    "        drop_last=False        # Ensure the last batch is included\n",
    "    )\n",
    "\n",
    "    epoch_train_losses = []\n",
    "    for epoch in range(num_epochs):\n",
    "        # torch.cuda.empty_cache()\n",
    "        epoch_loss = 0.0\n",
    "        for X_batch, y_batch in trainloader:\n",
    "            X_batch = X_batch.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            pred_y = model(X_batch)   # [batch_size, num_classes]\n",
    "            loss = criterion(pred_y, y_batch)\n",
    "            loss.backward()\n",
    "\n",
    "            # Gradient clipping (optional)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "               \n",
    "        if (epoch + 1) % 5 == 0 or epoch == num_epochs - 1:\n",
    "            print(f\"[Train] Epoch {epoch+1}/{num_epochs}, Loss={epoch_loss/len(trainloader):.4f}\")\n",
    "\n",
    "        epoch_train_losses.append(epoch_loss/len(trainloader))\n",
    "        \n",
    "    loss_decrease_percentage = ((epoch_train_losses[-1] - epoch_train_losses[0]) / epoch_train_losses[0]) * 100\n",
    "    # ---------------------------------\n",
    "    # 2) ROLLING PREDICTIONS, NO UPDATE\n",
    "    # ---------------------------------\n",
    "    model.eval()\n",
    "\n",
    "    rolling_predictions = []\n",
    "    rolling_targets     = []\n",
    "\n",
    "    for i in range(lower_bound, len(X)):\n",
    "        # Single-step \"test\" sample\n",
    "        X_test = X[i:i+1].to(device)  # shape: (1, num_features)\n",
    "        y_test = y[i:i+1].to(device)  # shape: (1, )\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # Forward pass\n",
    "            pred_y = model(X_test)  # [1, num_classes]\n",
    "            probabilities = torch.softmax(pred_y, dim=1).cpu().numpy()  # shape: (1, 2)\n",
    "            prob_class_1  = probabilities[:, 1]  # shape: (1,)\n",
    "\n",
    "            # Threshold-based logic\n",
    "            # Initialize all predictions to 0\n",
    "            pred_classes = np.zeros_like(prob_class_1)\n",
    "            # Predict -1 if prob < lower_threshold\n",
    "            pred_classes[prob_class_1 < lower_threshold] = -1\n",
    "            # Predict +1 if prob > upper_threshold\n",
    "            pred_classes[prob_class_1 > 1-lower_threshold] = 1\n",
    "\n",
    "        rolling_predictions.append(pred_classes[0])  # scalar\n",
    "        rolling_targets.append(y_test.item())\n",
    "\n",
    "    rolling_predictions = np.array(rolling_predictions)\n",
    "    rolling_targets = np.array(rolling_targets).astype(int)\n",
    "\n",
    "    # Convert any 0-labeled targets to -1 if your original data is in [-1, +1]\n",
    "    # (Sometimes y might be {0,1} or {-1, +1}; adapt as needed.)\n",
    "    rolling_targets[rolling_targets == 0] = -1\n",
    "\n",
    "    # Filter out zero predictions\n",
    "    nonzero_mask = rolling_predictions != 0\n",
    "    filtered_preds = rolling_predictions[nonzero_mask]\n",
    "    filtered_targets = rolling_targets[nonzero_mask]\n",
    "\n",
    "    if len(filtered_preds) == 0:\n",
    "        accuracy_nonzero = None\n",
    "        print(\"No nonzero predictions, cannot compute thresholded accuracy.\")\n",
    "    else:\n",
    "        accuracy_nonzero = accuracy_score(filtered_targets, filtered_preds)\n",
    "        print(f\"Accuracy on Nonzero Predictions: {accuracy_nonzero:.4f}\")\n",
    "\n",
    "    return {\n",
    "        \"rolling_predictions\": rolling_predictions,\n",
    "        \"rolling_targets\": rolling_targets,\n",
    "        \"filtered_predictions\": filtered_preds,\n",
    "        \"filtered_targets\": filtered_targets,\n",
    "        \"accuracy_nonzero\": accuracy_nonzero,\n",
    "        \"loss_decrease_percentage\": loss_decrease_percentage\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b794e699-83fb-4ca2-8c93-318858018a2e",
   "metadata": {},
   "source": [
    "### **1st Type of comparison:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1ebcf636-0ba3-45b9-b37d-5a075fefc803",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AAPL, options, 15y, 95, 5\n",
      "/home/jupyter-tfg2425paula/prediction_project_v3/00_data/horizontal_structure/clean/options/AAPL/15y_5_data.pkl\n",
      "Training AAPL | LR: 0.01 | Epochs: 100 | Batch: 16 | Prediction Threshold: 0.5\n",
      "[Train] Epoch 5/100, Loss=0.6923\n",
      "[Train] Epoch 10/100, Loss=0.6754\n",
      "[Train] Epoch 15/100, Loss=0.6125\n",
      "[Train] Epoch 20/100, Loss=0.5152\n",
      "[Train] Epoch 25/100, Loss=0.4226\n",
      "[Train] Epoch 30/100, Loss=0.3315\n",
      "[Train] Epoch 35/100, Loss=0.2615\n",
      "[Train] Epoch 40/100, Loss=0.2094\n",
      "[Train] Epoch 45/100, Loss=0.1780\n",
      "[Train] Epoch 50/100, Loss=0.1432\n",
      "[Train] Epoch 55/100, Loss=0.1050\n",
      "[Train] Epoch 60/100, Loss=0.1082\n",
      "[Train] Epoch 65/100, Loss=0.1127\n",
      "[Train] Epoch 70/100, Loss=0.1077\n",
      "[Train] Epoch 75/100, Loss=0.0826\n",
      "[Train] Epoch 80/100, Loss=0.0970\n",
      "[Train] Epoch 85/100, Loss=0.0864\n",
      "[Train] Epoch 90/100, Loss=0.0659\n",
      "[Train] Epoch 95/100, Loss=0.0648\n",
      "[Train] Epoch 100/100, Loss=0.1268\n",
      "Accuracy on Nonzero Predictions: 0.5408\n"
     ]
    }
   ],
   "source": [
    "# folder\n",
    "results_list = []\n",
    "stock = \"AAPL\"\n",
    "period = \"10y\"\n",
    "possible_train_size = 95\n",
    "for processing in processing_types:\n",
    "    initial_data_dir = os.path.join(project_dir, f\"00_data/{processing}\")\n",
    "    for model_type in model_types:\n",
    "        for security_type in types_securities:\n",
    "            output_folder = os.path.join(results_dir, f\"{model_type}/{stock}/{security_type}\") \n",
    "            os.makedirs(output_folder, exist_ok=True)\n",
    "            # files\n",
    "            # load original data as well (for info purposes)\n",
    "            \n",
    "            filename = f\"{security_type}/{stock}/{period}_data.csv\"\n",
    "            original_input_filepath = os.path.join(initial_data_dir, filename)\n",
    "            original_data = pd.read_csv(original_input_filepath)\n",
    "            start_date = original_data.loc[0, \"Date\"]\n",
    "            end_date = original_data.iloc[-1][\"Date\"]\n",
    "            results_csv_path = os.path.join(output_folder, f\"{period}_{possible_train_size}.csv\")\n",
    "\n",
    "            # columns, same file\n",
    "            for window_size in window_sizes:\n",
    "                print(f\"{stock}, {security_type}, {period}, {possible_train_size}, {window_size}\")\n",
    "\n",
    "                # load data\n",
    "                pkl_filename = f\"clean/{security_type}/{stock}/{period}_{window_size}_data.pkl\"\n",
    "                input_filepath = os.path.join(horizontal_data_dir, pkl_filename)\n",
    "                print(input_filepath)\n",
    "                input_df = pd.read_pickle(input_filepath)\n",
    "\n",
    "                X_resampled, y_resampled = reshape_remove_characters(input_df)\n",
    "\n",
    "                input_size = X_resampled.shape[2]\n",
    "                train_size = int(X_resampled.shape[0]*possible_train_size/100)\n",
    "                test_size = X_resampled.shape[0] - train_size\n",
    "\n",
    "                # generate model\n",
    "                if model_type == \"gru\":\n",
    "                    model = GRU3DClassifier(input_size, hidden_size, output_size, num_layers, dropout)\n",
    "                elif model_type == \"lstm\":\n",
    "                    model = StockPriceLSTM(input_size, hidden_size, output_size)\n",
    "\n",
    "                model = model.to(device)\n",
    "\n",
    "                for learning_rate in learning_rates:\n",
    "\n",
    "                    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "                    for num_epochs in num_epochs_list:\n",
    "                        for prediction_threshold in prediction_thresholds:\n",
    "                            for batch_size in batch_sizes:\n",
    "\n",
    "                                print(f\"Training {stock} | LR: {learning_rate} | Epochs: {num_epochs} | Batch: {batch_size} | Prediction Threshold: {prediction_threshold}\")\n",
    "\n",
    "                                start_time = time.time()\n",
    "\n",
    "                                result = evaluate_rolling_unchanged_model_threshold(\n",
    "                                    model, X_resampled, y_resampled, criterion, \n",
    "                                                               optimizer, device, train_size, batch_size, num_epochs, lower_threshold=prediction_threshold)     \n",
    "\n",
    "                                rolling_predictions = result[\"rolling_predictions\"]\n",
    "                                rolling_targets = result[\"rolling_targets\"]\n",
    "                                test_accuracy = result[\"accuracy_nonzero\"]\n",
    "                                loss_decrease_percentage = result[\"loss_decrease_percentage\"]\n",
    "                                nonzero_preds = np.count_nonzero(result[\"rolling_predictions\"])\n",
    "\n",
    "\n",
    "                                end_time = time.time()    \n",
    "                                execution_time = end_time - start_time\n",
    "\n",
    "                                # --------------------------------------------\n",
    "                                # 1) Create a record (dictionary) for this run\n",
    "                                # --------------------------------------------\n",
    "                                run_record = {\n",
    "                                    \"start_date\": start_date,\n",
    "                                    \"end_date\": end_date,\n",
    "                                    \"execution_time\": execution_time,\n",
    "                                    \"test_size\": test_size,\n",
    "                                    \"nonzero_preds\": nonzero_preds,\n",
    "                                    \"accuracy\": test_accuracy,\n",
    "                                    \"prediction_threshold\": prediction_threshold,\n",
    "\n",
    "                                    \"window_size\": window_size,\n",
    "                                    \"learning_rate\": learning_rate,\n",
    "                                    \"num_epochs\": num_epochs,\n",
    "                                    \"train_loss_change_pctg\": loss_decrease_percentage,\n",
    "\n",
    "                                    \"batch_size\": batch_size,\n",
    "\n",
    "                                    \"output_size\": output_size,\n",
    "                                    \"hidden_size\": hidden_size,\n",
    "                                    \"num_layers\": num_layers,\n",
    "                                    \"dropout_rate\": dropout,\n",
    "                                    \"optimizer\": optimizer.__class__.__name__,\n",
    "                                    \"criterion\": criterion\n",
    "\n",
    "                                }\n",
    "\n",
    "                                # --------------------------------------------\n",
    "                                # 2) Append the dictionary to the results list\n",
    "                                # --------------------------------------------\n",
    "                                results_list.append(run_record)\n",
    "\n",
    "            # ----------------------------------------------------------------\n",
    "            # 3) Write to CSV *once* after all window_sizes for this setup\n",
    "            # ----------------------------------------------------------------\n",
    "            if len(results_list) > 0:\n",
    "                df = pd.DataFrame(results_list)\n",
    "\n",
    "                # If CSV already exists, append without header\n",
    "                if os.path.exists(results_csv_path):\n",
    "                    df.to_csv(results_csv_path, mode='a', header=False, index=False)\n",
    "                else:\n",
    "                    df.to_csv(results_csv_path, index=False)\n",
    "\n",
    "                # Clear results_list to avoid duplication\n",
    "                results_list = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "672857df-0a56-49c8-a1f9-8972057d026b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clean, lstm, technical\n",
      "/home/jupyter-tfg2425paula/prediction_project_v3/00_data/horizontal_structure/clean/technical/AAPL/10y_5_data.pkl\n",
      "Training AAPL | LR: 0.009 | Epochs: 100 | Batch: 32 | Prediction Threshold: 0.5\n",
      "[Train] Epoch 5/100, Loss=0.6927\n",
      "[Train] Epoch 10/100, Loss=0.6855\n",
      "[Train] Epoch 15/100, Loss=0.6514\n",
      "[Train] Epoch 20/100, Loss=0.5757\n",
      "[Train] Epoch 25/100, Loss=0.4530\n",
      "[Train] Epoch 30/100, Loss=0.3551\n",
      "[Train] Epoch 35/100, Loss=0.2736\n",
      "[Train] Epoch 40/100, Loss=0.1954\n",
      "[Train] Epoch 45/100, Loss=0.1779\n",
      "[Train] Epoch 50/100, Loss=0.1583\n",
      "[Train] Epoch 55/100, Loss=0.0985\n",
      "[Train] Epoch 60/100, Loss=0.1181\n",
      "[Train] Epoch 65/100, Loss=0.0916\n",
      "[Train] Epoch 70/100, Loss=0.0997\n",
      "[Train] Epoch 75/100, Loss=0.0652\n",
      "[Train] Epoch 80/100, Loss=0.0588\n",
      "[Train] Epoch 85/100, Loss=0.0792\n",
      "[Train] Epoch 90/100, Loss=0.0646\n",
      "[Train] Epoch 95/100, Loss=0.1045\n",
      "[Train] Epoch 100/100, Loss=0.0739\n",
      "Accuracy on Nonzero Predictions: 0.4683\n",
      "Training AAPL | LR: 0.01 | Epochs: 100 | Batch: 32 | Prediction Threshold: 0.5\n",
      "[Train] Epoch 5/100, Loss=0.0997\n",
      "[Train] Epoch 10/100, Loss=0.1140\n",
      "[Train] Epoch 15/100, Loss=0.0818\n",
      "[Train] Epoch 20/100, Loss=0.0577\n",
      "[Train] Epoch 25/100, Loss=0.1668\n",
      "[Train] Epoch 30/100, Loss=0.0332\n",
      "[Train] Epoch 35/100, Loss=0.0629\n",
      "[Train] Epoch 40/100, Loss=0.1340\n",
      "[Train] Epoch 45/100, Loss=0.0678\n",
      "[Train] Epoch 50/100, Loss=0.0454\n",
      "[Train] Epoch 55/100, Loss=0.0953\n",
      "[Train] Epoch 60/100, Loss=0.0616\n",
      "[Train] Epoch 65/100, Loss=0.0603\n",
      "[Train] Epoch 70/100, Loss=0.0991\n",
      "[Train] Epoch 75/100, Loss=0.0533\n",
      "[Train] Epoch 80/100, Loss=0.0703\n",
      "[Train] Epoch 85/100, Loss=0.1048\n",
      "[Train] Epoch 90/100, Loss=0.0769\n",
      "[Train] Epoch 95/100, Loss=0.1283\n",
      "[Train] Epoch 100/100, Loss=0.0995\n",
      "Accuracy on Nonzero Predictions: 0.5159\n",
      "Training AAPL | LR: 0.011 | Epochs: 100 | Batch: 32 | Prediction Threshold: 0.5\n",
      "[Train] Epoch 5/100, Loss=0.1012\n",
      "[Train] Epoch 10/100, Loss=0.1319\n",
      "[Train] Epoch 15/100, Loss=0.0959\n",
      "[Train] Epoch 20/100, Loss=0.0996\n",
      "[Train] Epoch 25/100, Loss=0.1844\n",
      "[Train] Epoch 30/100, Loss=0.0758\n",
      "[Train] Epoch 35/100, Loss=0.1089\n",
      "[Train] Epoch 40/100, Loss=0.0702\n",
      "[Train] Epoch 45/100, Loss=0.0908\n",
      "[Train] Epoch 50/100, Loss=0.1220\n",
      "[Train] Epoch 55/100, Loss=0.1014\n",
      "[Train] Epoch 60/100, Loss=0.0549\n",
      "[Train] Epoch 65/100, Loss=0.0808\n",
      "[Train] Epoch 70/100, Loss=0.0651\n",
      "[Train] Epoch 75/100, Loss=0.0923\n",
      "[Train] Epoch 80/100, Loss=0.0817\n",
      "[Train] Epoch 85/100, Loss=0.0953\n",
      "[Train] Epoch 90/100, Loss=0.0932\n",
      "[Train] Epoch 95/100, Loss=0.0953\n",
      "[Train] Epoch 100/100, Loss=0.1282\n",
      "Accuracy on Nonzero Predictions: 0.5556\n",
      "[0.46825396825396826, 0.5158730158730159, 0.5555555555555556]\n",
      "[]\n",
      "clean, lstm, options\n",
      "/home/jupyter-tfg2425paula/prediction_project_v3/00_data/horizontal_structure/clean/options/AAPL/10y_5_data.pkl\n",
      "Training AAPL | LR: 0.009 | Epochs: 100 | Batch: 32 | Prediction Threshold: 0.5\n",
      "[Train] Epoch 5/100, Loss=0.6952\n",
      "[Train] Epoch 10/100, Loss=0.6784\n",
      "[Train] Epoch 15/100, Loss=0.6424\n",
      "[Train] Epoch 20/100, Loss=0.5421\n",
      "[Train] Epoch 25/100, Loss=0.4415\n",
      "[Train] Epoch 30/100, Loss=0.3232\n",
      "[Train] Epoch 35/100, Loss=0.2392\n",
      "[Train] Epoch 40/100, Loss=0.1588\n",
      "[Train] Epoch 45/100, Loss=0.1027\n",
      "[Train] Epoch 50/100, Loss=0.0954\n",
      "[Train] Epoch 55/100, Loss=0.0543\n",
      "[Train] Epoch 60/100, Loss=0.0712\n",
      "[Train] Epoch 65/100, Loss=0.0779\n",
      "[Train] Epoch 70/100, Loss=0.0648\n",
      "[Train] Epoch 75/100, Loss=0.0532\n",
      "[Train] Epoch 80/100, Loss=0.0503\n",
      "[Train] Epoch 85/100, Loss=0.0487\n",
      "[Train] Epoch 90/100, Loss=0.0261\n",
      "[Train] Epoch 95/100, Loss=0.0387\n",
      "[Train] Epoch 100/100, Loss=0.0504\n",
      "Accuracy on Nonzero Predictions: 0.5573\n",
      "Training AAPL | LR: 0.01 | Epochs: 100 | Batch: 32 | Prediction Threshold: 0.5\n",
      "[Train] Epoch 5/100, Loss=0.0325\n",
      "[Train] Epoch 10/100, Loss=0.0436\n",
      "[Train] Epoch 15/100, Loss=0.0634\n",
      "[Train] Epoch 20/100, Loss=0.0390\n",
      "[Train] Epoch 25/100, Loss=0.0368\n",
      "[Train] Epoch 30/100, Loss=0.0381\n",
      "[Train] Epoch 35/100, Loss=0.0545\n",
      "[Train] Epoch 40/100, Loss=0.0178\n",
      "[Train] Epoch 45/100, Loss=0.0286\n",
      "[Train] Epoch 50/100, Loss=0.0697\n",
      "[Train] Epoch 55/100, Loss=0.0399\n",
      "[Train] Epoch 60/100, Loss=0.0142\n",
      "[Train] Epoch 65/100, Loss=0.0247\n",
      "[Train] Epoch 70/100, Loss=0.0568\n",
      "[Train] Epoch 75/100, Loss=0.0318\n",
      "[Train] Epoch 80/100, Loss=0.0295\n",
      "[Train] Epoch 85/100, Loss=0.0051\n",
      "[Train] Epoch 90/100, Loss=0.0005\n",
      "[Train] Epoch 95/100, Loss=0.0003\n",
      "[Train] Epoch 100/100, Loss=0.0002\n",
      "Accuracy on Nonzero Predictions: 0.5802\n",
      "Training AAPL | LR: 0.011 | Epochs: 100 | Batch: 32 | Prediction Threshold: 0.5\n",
      "[Train] Epoch 5/100, Loss=0.0420\n",
      "[Train] Epoch 10/100, Loss=0.0323\n",
      "[Train] Epoch 15/100, Loss=0.0788\n",
      "[Train] Epoch 20/100, Loss=0.0572\n",
      "[Train] Epoch 25/100, Loss=0.0235\n",
      "[Train] Epoch 30/100, Loss=0.0276\n",
      "[Train] Epoch 35/100, Loss=0.0732\n",
      "[Train] Epoch 40/100, Loss=0.0373\n",
      "[Train] Epoch 45/100, Loss=0.0156\n",
      "[Train] Epoch 50/100, Loss=0.0996\n",
      "[Train] Epoch 55/100, Loss=0.0537\n",
      "[Train] Epoch 60/100, Loss=0.0235\n",
      "[Train] Epoch 65/100, Loss=0.0200\n",
      "[Train] Epoch 70/100, Loss=0.0220\n",
      "[Train] Epoch 75/100, Loss=0.0876\n",
      "[Train] Epoch 80/100, Loss=0.0437\n",
      "[Train] Epoch 85/100, Loss=0.0402\n",
      "[Train] Epoch 90/100, Loss=0.0370\n",
      "[Train] Epoch 95/100, Loss=0.0465\n",
      "[Train] Epoch 100/100, Loss=0.0263\n",
      "Accuracy on Nonzero Predictions: 0.5573\n",
      "[0.5572519083969466, 0.5801526717557252, 0.5572519083969466]\n",
      "[]\n",
      "clean, gru, technical\n",
      "/home/jupyter-tfg2425paula/prediction_project_v3/00_data/horizontal_structure/clean/technical/AAPL/10y_5_data.pkl\n",
      "Training AAPL | LR: 0.009 | Epochs: 100 | Batch: 32 | Prediction Threshold: 0.5\n",
      "[Train] Epoch 5/100, Loss=0.7065\n",
      "[Train] Epoch 10/100, Loss=0.7031\n",
      "[Train] Epoch 15/100, Loss=0.7010\n",
      "[Train] Epoch 20/100, Loss=0.7366\n",
      "[Train] Epoch 25/100, Loss=0.7204\n",
      "[Train] Epoch 30/100, Loss=0.7042\n",
      "[Train] Epoch 35/100, Loss=0.7031\n",
      "[Train] Epoch 40/100, Loss=0.6993\n",
      "[Train] Epoch 45/100, Loss=0.7017\n",
      "[Train] Epoch 50/100, Loss=0.7004\n",
      "[Train] Epoch 55/100, Loss=0.6981\n",
      "[Train] Epoch 60/100, Loss=0.7016\n",
      "[Train] Epoch 65/100, Loss=0.7048\n",
      "[Train] Epoch 70/100, Loss=0.6984\n",
      "[Train] Epoch 75/100, Loss=0.6995\n",
      "[Train] Epoch 80/100, Loss=0.6974\n",
      "[Train] Epoch 85/100, Loss=0.7016\n",
      "[Train] Epoch 90/100, Loss=0.6975\n",
      "[Train] Epoch 95/100, Loss=0.6929\n",
      "[Train] Epoch 100/100, Loss=0.6939\n",
      "Accuracy on Nonzero Predictions: 0.5159\n",
      "Training AAPL | LR: 0.01 | Epochs: 100 | Batch: 32 | Prediction Threshold: 0.5\n",
      "[Train] Epoch 5/100, Loss=0.6975\n",
      "[Train] Epoch 10/100, Loss=0.6893\n",
      "[Train] Epoch 15/100, Loss=0.6926\n",
      "[Train] Epoch 20/100, Loss=0.6946\n",
      "[Train] Epoch 25/100, Loss=0.6917\n",
      "[Train] Epoch 30/100, Loss=0.6987\n",
      "[Train] Epoch 35/100, Loss=0.6929\n",
      "[Train] Epoch 40/100, Loss=0.6876\n",
      "[Train] Epoch 45/100, Loss=0.6940\n",
      "[Train] Epoch 50/100, Loss=0.6898\n",
      "[Train] Epoch 55/100, Loss=0.6872\n",
      "[Train] Epoch 60/100, Loss=0.6866\n",
      "[Train] Epoch 65/100, Loss=0.6886\n",
      "[Train] Epoch 70/100, Loss=0.6919\n",
      "[Train] Epoch 75/100, Loss=0.6875\n",
      "[Train] Epoch 80/100, Loss=0.6885\n",
      "[Train] Epoch 85/100, Loss=0.6906\n",
      "[Train] Epoch 90/100, Loss=0.6968\n",
      "[Train] Epoch 95/100, Loss=0.6853\n",
      "[Train] Epoch 100/100, Loss=0.6843\n",
      "Accuracy on Nonzero Predictions: 0.5556\n",
      "Training AAPL | LR: 0.011 | Epochs: 100 | Batch: 32 | Prediction Threshold: 0.5\n",
      "[Train] Epoch 5/100, Loss=0.6911\n",
      "[Train] Epoch 10/100, Loss=0.6858\n",
      "[Train] Epoch 15/100, Loss=0.6902\n",
      "[Train] Epoch 20/100, Loss=0.6826\n",
      "[Train] Epoch 25/100, Loss=0.6902\n",
      "[Train] Epoch 30/100, Loss=0.6862\n",
      "[Train] Epoch 35/100, Loss=0.6867\n",
      "[Train] Epoch 40/100, Loss=0.6860\n",
      "[Train] Epoch 45/100, Loss=0.6817\n",
      "[Train] Epoch 50/100, Loss=0.6803\n",
      "[Train] Epoch 55/100, Loss=0.6844\n",
      "[Train] Epoch 60/100, Loss=0.6777\n",
      "[Train] Epoch 65/100, Loss=0.6818\n",
      "[Train] Epoch 70/100, Loss=0.6828\n",
      "[Train] Epoch 75/100, Loss=0.6752\n",
      "[Train] Epoch 80/100, Loss=0.6831\n",
      "[Train] Epoch 85/100, Loss=0.6748\n",
      "[Train] Epoch 90/100, Loss=0.6768\n",
      "[Train] Epoch 95/100, Loss=0.6756\n",
      "[Train] Epoch 100/100, Loss=0.6819\n",
      "Accuracy on Nonzero Predictions: 0.5873\n",
      "[0.5158730158730159, 0.5555555555555556, 0.5873015873015873]\n",
      "[]\n",
      "clean, gru, options\n",
      "/home/jupyter-tfg2425paula/prediction_project_v3/00_data/horizontal_structure/clean/options/AAPL/10y_5_data.pkl\n",
      "Training AAPL | LR: 0.009 | Epochs: 100 | Batch: 32 | Prediction Threshold: 0.5\n",
      "[Train] Epoch 5/100, Loss=0.6939\n",
      "[Train] Epoch 10/100, Loss=0.6880\n",
      "[Train] Epoch 15/100, Loss=0.6841\n",
      "[Train] Epoch 20/100, Loss=0.6781\n",
      "[Train] Epoch 25/100, Loss=0.6618\n",
      "[Train] Epoch 30/100, Loss=0.6369\n",
      "[Train] Epoch 35/100, Loss=0.6254\n",
      "[Train] Epoch 40/100, Loss=0.5681\n",
      "[Train] Epoch 45/100, Loss=0.5205\n",
      "[Train] Epoch 50/100, Loss=0.5111\n",
      "[Train] Epoch 55/100, Loss=0.4576\n",
      "[Train] Epoch 60/100, Loss=0.4410\n",
      "[Train] Epoch 65/100, Loss=0.4207\n",
      "[Train] Epoch 70/100, Loss=0.3912\n",
      "[Train] Epoch 75/100, Loss=0.3642\n",
      "[Train] Epoch 80/100, Loss=0.3666\n",
      "[Train] Epoch 85/100, Loss=0.3490\n",
      "[Train] Epoch 90/100, Loss=0.3208\n",
      "[Train] Epoch 95/100, Loss=0.3518\n",
      "[Train] Epoch 100/100, Loss=0.3136\n",
      "Accuracy on Nonzero Predictions: 0.5725\n",
      "Training AAPL | LR: 0.01 | Epochs: 100 | Batch: 32 | Prediction Threshold: 0.5\n",
      "[Train] Epoch 5/100, Loss=0.3287\n",
      "[Train] Epoch 10/100, Loss=0.3462\n",
      "[Train] Epoch 15/100, Loss=0.3599\n",
      "[Train] Epoch 20/100, Loss=0.3736\n",
      "[Train] Epoch 25/100, Loss=0.3440\n",
      "[Train] Epoch 30/100, Loss=0.3406\n",
      "[Train] Epoch 35/100, Loss=0.3782\n",
      "[Train] Epoch 40/100, Loss=0.3605\n",
      "[Train] Epoch 45/100, Loss=0.3723\n",
      "[Train] Epoch 50/100, Loss=0.3919\n",
      "[Train] Epoch 55/100, Loss=0.3447\n",
      "[Train] Epoch 60/100, Loss=0.3950\n",
      "[Train] Epoch 65/100, Loss=0.3621\n",
      "[Train] Epoch 70/100, Loss=0.3661\n",
      "[Train] Epoch 75/100, Loss=0.3826\n",
      "[Train] Epoch 80/100, Loss=0.3508\n",
      "[Train] Epoch 85/100, Loss=0.3784\n",
      "[Train] Epoch 90/100, Loss=0.3720\n",
      "[Train] Epoch 95/100, Loss=0.3729\n",
      "[Train] Epoch 100/100, Loss=0.3736\n",
      "Accuracy on Nonzero Predictions: 0.6260\n",
      "Training AAPL | LR: 0.011 | Epochs: 100 | Batch: 32 | Prediction Threshold: 0.5\n",
      "[Train] Epoch 5/100, Loss=0.3861\n",
      "[Train] Epoch 10/100, Loss=0.3939\n",
      "[Train] Epoch 15/100, Loss=0.4668\n",
      "[Train] Epoch 20/100, Loss=0.4359\n",
      "[Train] Epoch 25/100, Loss=0.5052\n",
      "[Train] Epoch 30/100, Loss=0.4946\n",
      "[Train] Epoch 35/100, Loss=0.5265\n",
      "[Train] Epoch 40/100, Loss=0.5376\n",
      "[Train] Epoch 45/100, Loss=0.5834\n",
      "[Train] Epoch 50/100, Loss=0.5642\n",
      "[Train] Epoch 55/100, Loss=0.6175\n",
      "[Train] Epoch 60/100, Loss=0.6094\n",
      "[Train] Epoch 65/100, Loss=0.6386\n",
      "[Train] Epoch 70/100, Loss=0.6251\n",
      "[Train] Epoch 75/100, Loss=0.6211\n",
      "[Train] Epoch 80/100, Loss=0.6106\n",
      "[Train] Epoch 85/100, Loss=0.6139\n",
      "[Train] Epoch 90/100, Loss=0.6207\n",
      "[Train] Epoch 95/100, Loss=0.6292\n",
      "[Train] Epoch 100/100, Loss=0.6338\n",
      "Accuracy on Nonzero Predictions: 0.5344\n",
      "[0.5725190839694656, 0.6259541984732825, 0.5343511450381679]\n",
      "[]\n",
      "pca, lstm, technical\n",
      "/home/jupyter-tfg2425paula/prediction_project_v3/00_data/horizontal_structure/pca/technical/AAPL/10y_5_data.pkl\n",
      "Training AAPL | LR: 0.009 | Epochs: 100 | Batch: 32 | Prediction Threshold: 0.5\n",
      "[Train] Epoch 5/100, Loss=0.6916\n",
      "[Train] Epoch 10/100, Loss=0.6816\n",
      "[Train] Epoch 15/100, Loss=0.6618\n",
      "[Train] Epoch 20/100, Loss=0.6163\n",
      "[Train] Epoch 25/100, Loss=0.5162\n",
      "[Train] Epoch 30/100, Loss=0.3999\n",
      "[Train] Epoch 35/100, Loss=0.2992\n",
      "[Train] Epoch 40/100, Loss=0.1949\n",
      "[Train] Epoch 45/100, Loss=0.1645\n",
      "[Train] Epoch 50/100, Loss=0.1008\n",
      "[Train] Epoch 55/100, Loss=0.1068\n",
      "[Train] Epoch 60/100, Loss=0.0692\n",
      "[Train] Epoch 65/100, Loss=0.0496\n",
      "[Train] Epoch 70/100, Loss=0.1134\n",
      "[Train] Epoch 75/100, Loss=0.0553\n",
      "[Train] Epoch 80/100, Loss=0.0356\n",
      "[Train] Epoch 85/100, Loss=0.0919\n",
      "[Train] Epoch 90/100, Loss=0.0404\n",
      "[Train] Epoch 95/100, Loss=0.0913\n",
      "[Train] Epoch 100/100, Loss=0.0519\n",
      "Accuracy on Nonzero Predictions: 0.6270\n",
      "Training AAPL | LR: 0.01 | Epochs: 100 | Batch: 32 | Prediction Threshold: 0.5\n",
      "[Train] Epoch 5/100, Loss=0.0805\n",
      "[Train] Epoch 10/100, Loss=0.0709\n",
      "[Train] Epoch 15/100, Loss=0.0524\n",
      "[Train] Epoch 20/100, Loss=0.0796\n",
      "[Train] Epoch 25/100, Loss=0.0677\n",
      "[Train] Epoch 30/100, Loss=0.0500\n",
      "[Train] Epoch 35/100, Loss=0.0718\n",
      "[Train] Epoch 40/100, Loss=0.0774\n",
      "[Train] Epoch 45/100, Loss=0.0610\n",
      "[Train] Epoch 50/100, Loss=0.0796\n",
      "[Train] Epoch 55/100, Loss=0.0544\n",
      "[Train] Epoch 60/100, Loss=0.0818\n",
      "[Train] Epoch 65/100, Loss=0.0714\n",
      "[Train] Epoch 70/100, Loss=0.0449\n",
      "[Train] Epoch 75/100, Loss=0.0466\n",
      "[Train] Epoch 80/100, Loss=0.0599\n",
      "[Train] Epoch 85/100, Loss=0.0450\n",
      "[Train] Epoch 90/100, Loss=0.0516\n",
      "[Train] Epoch 95/100, Loss=0.1258\n",
      "[Train] Epoch 100/100, Loss=0.0517\n",
      "Accuracy on Nonzero Predictions: 0.5714\n",
      "Training AAPL | LR: 0.011 | Epochs: 100 | Batch: 32 | Prediction Threshold: 0.5\n",
      "[Train] Epoch 5/100, Loss=0.0635\n",
      "[Train] Epoch 10/100, Loss=0.0677\n",
      "[Train] Epoch 15/100, Loss=0.0482\n",
      "[Train] Epoch 20/100, Loss=0.1010\n",
      "[Train] Epoch 25/100, Loss=0.0567\n",
      "[Train] Epoch 30/100, Loss=0.0637\n",
      "[Train] Epoch 35/100, Loss=0.0529\n",
      "[Train] Epoch 40/100, Loss=0.1335\n",
      "[Train] Epoch 45/100, Loss=0.0781\n",
      "[Train] Epoch 50/100, Loss=0.0205\n",
      "[Train] Epoch 55/100, Loss=0.1051\n",
      "[Train] Epoch 60/100, Loss=0.0982\n",
      "[Train] Epoch 65/100, Loss=0.0553\n",
      "[Train] Epoch 70/100, Loss=0.0726\n",
      "[Train] Epoch 75/100, Loss=0.0485\n",
      "[Train] Epoch 80/100, Loss=0.0948\n",
      "[Train] Epoch 85/100, Loss=0.0916\n",
      "[Train] Epoch 90/100, Loss=0.0435\n",
      "[Train] Epoch 95/100, Loss=0.0640\n",
      "[Train] Epoch 100/100, Loss=0.0651\n",
      "Accuracy on Nonzero Predictions: 0.5556\n",
      "[0.626984126984127, 0.5714285714285714, 0.5555555555555556]\n",
      "[]\n",
      "pca, lstm, options\n",
      "/home/jupyter-tfg2425paula/prediction_project_v3/00_data/horizontal_structure/pca/options/AAPL/10y_5_data.pkl\n",
      "Training AAPL | LR: 0.009 | Epochs: 100 | Batch: 32 | Prediction Threshold: 0.5\n",
      "[Train] Epoch 5/100, Loss=0.6937\n",
      "[Train] Epoch 10/100, Loss=0.6877\n",
      "[Train] Epoch 15/100, Loss=0.6439\n",
      "[Train] Epoch 20/100, Loss=0.5162\n",
      "[Train] Epoch 25/100, Loss=0.3398\n",
      "[Train] Epoch 30/100, Loss=0.2121\n",
      "[Train] Epoch 35/100, Loss=0.1095\n",
      "[Train] Epoch 40/100, Loss=0.0818\n",
      "[Train] Epoch 45/100, Loss=0.0398\n",
      "[Train] Epoch 50/100, Loss=0.0562\n",
      "[Train] Epoch 55/100, Loss=0.0328\n",
      "[Train] Epoch 60/100, Loss=0.0081\n",
      "[Train] Epoch 65/100, Loss=0.0196\n",
      "[Train] Epoch 70/100, Loss=0.0871\n",
      "[Train] Epoch 75/100, Loss=0.0222\n",
      "[Train] Epoch 80/100, Loss=0.0158\n",
      "[Train] Epoch 85/100, Loss=0.0018\n",
      "[Train] Epoch 90/100, Loss=0.0004\n",
      "[Train] Epoch 95/100, Loss=0.0003\n",
      "[Train] Epoch 100/100, Loss=0.0002\n",
      "Accuracy on Nonzero Predictions: 0.4885\n",
      "Training AAPL | LR: 0.01 | Epochs: 100 | Batch: 32 | Prediction Threshold: 0.5\n",
      "[Train] Epoch 5/100, Loss=0.0192\n",
      "[Train] Epoch 10/100, Loss=0.0398\n",
      "[Train] Epoch 15/100, Loss=0.0213\n",
      "[Train] Epoch 20/100, Loss=0.0123\n",
      "[Train] Epoch 25/100, Loss=0.0149\n",
      "[Train] Epoch 30/100, Loss=0.0953\n",
      "[Train] Epoch 35/100, Loss=0.0324\n",
      "[Train] Epoch 40/100, Loss=0.0005\n",
      "[Train] Epoch 45/100, Loss=0.0003\n",
      "[Train] Epoch 50/100, Loss=0.0002\n",
      "[Train] Epoch 55/100, Loss=0.0001\n",
      "[Train] Epoch 60/100, Loss=0.0001\n",
      "[Train] Epoch 65/100, Loss=0.0001\n",
      "[Train] Epoch 70/100, Loss=0.0001\n",
      "[Train] Epoch 75/100, Loss=0.0001\n",
      "[Train] Epoch 80/100, Loss=0.0000\n",
      "[Train] Epoch 85/100, Loss=0.0000\n",
      "[Train] Epoch 90/100, Loss=0.0000\n",
      "[Train] Epoch 95/100, Loss=0.0000\n",
      "[Train] Epoch 100/100, Loss=0.0000\n",
      "Accuracy on Nonzero Predictions: 0.5115\n",
      "Training AAPL | LR: 0.011 | Epochs: 100 | Batch: 32 | Prediction Threshold: 0.5\n",
      "[Train] Epoch 5/100, Loss=0.0295\n",
      "[Train] Epoch 10/100, Loss=0.0047\n",
      "[Train] Epoch 15/100, Loss=0.0865\n",
      "[Train] Epoch 20/100, Loss=0.0305\n",
      "[Train] Epoch 25/100, Loss=0.0275\n",
      "[Train] Epoch 30/100, Loss=0.0207\n",
      "[Train] Epoch 35/100, Loss=0.0124\n",
      "[Train] Epoch 40/100, Loss=0.0277\n",
      "[Train] Epoch 45/100, Loss=0.0489\n",
      "[Train] Epoch 50/100, Loss=0.0143\n",
      "[Train] Epoch 55/100, Loss=0.0003\n",
      "[Train] Epoch 60/100, Loss=0.0002\n",
      "[Train] Epoch 65/100, Loss=0.0001\n",
      "[Train] Epoch 70/100, Loss=0.0001\n",
      "[Train] Epoch 75/100, Loss=0.0001\n",
      "[Train] Epoch 80/100, Loss=0.0001\n",
      "[Train] Epoch 85/100, Loss=0.0000\n",
      "[Train] Epoch 90/100, Loss=0.0000\n",
      "[Train] Epoch 95/100, Loss=0.0000\n",
      "[Train] Epoch 100/100, Loss=0.0000\n",
      "Accuracy on Nonzero Predictions: 0.5420\n",
      "[0.48854961832061067, 0.5114503816793893, 0.5419847328244275]\n",
      "[]\n",
      "pca, gru, technical\n",
      "/home/jupyter-tfg2425paula/prediction_project_v3/00_data/horizontal_structure/pca/technical/AAPL/10y_5_data.pkl\n",
      "Training AAPL | LR: 0.009 | Epochs: 100 | Batch: 32 | Prediction Threshold: 0.5\n",
      "[Train] Epoch 5/100, Loss=0.6938\n",
      "[Train] Epoch 10/100, Loss=0.6953\n",
      "[Train] Epoch 15/100, Loss=0.7038\n",
      "[Train] Epoch 20/100, Loss=0.6976\n",
      "[Train] Epoch 25/100, Loss=0.6986\n",
      "[Train] Epoch 30/100, Loss=0.6961\n",
      "[Train] Epoch 35/100, Loss=0.6874\n",
      "[Train] Epoch 40/100, Loss=0.6898\n",
      "[Train] Epoch 45/100, Loss=0.6931\n",
      "[Train] Epoch 50/100, Loss=0.6865\n",
      "[Train] Epoch 55/100, Loss=0.6776\n",
      "[Train] Epoch 60/100, Loss=0.6842\n",
      "[Train] Epoch 65/100, Loss=0.6756\n",
      "[Train] Epoch 70/100, Loss=0.6807\n",
      "[Train] Epoch 75/100, Loss=0.6733\n",
      "[Train] Epoch 80/100, Loss=0.6790\n",
      "[Train] Epoch 85/100, Loss=0.6804\n",
      "[Train] Epoch 90/100, Loss=0.6753\n",
      "[Train] Epoch 95/100, Loss=0.6708\n",
      "[Train] Epoch 100/100, Loss=0.6829\n",
      "Accuracy on Nonzero Predictions: 0.5635\n",
      "Training AAPL | LR: 0.01 | Epochs: 100 | Batch: 32 | Prediction Threshold: 0.5\n",
      "[Train] Epoch 5/100, Loss=0.6887\n",
      "[Train] Epoch 10/100, Loss=0.6883\n",
      "[Train] Epoch 15/100, Loss=0.6892\n",
      "[Train] Epoch 20/100, Loss=0.6857\n",
      "[Train] Epoch 25/100, Loss=0.6865\n",
      "[Train] Epoch 30/100, Loss=0.6857\n",
      "[Train] Epoch 35/100, Loss=0.6873\n",
      "[Train] Epoch 40/100, Loss=0.6849\n",
      "[Train] Epoch 45/100, Loss=0.6870\n",
      "[Train] Epoch 50/100, Loss=0.6865\n",
      "[Train] Epoch 55/100, Loss=0.6807\n",
      "[Train] Epoch 60/100, Loss=0.6833\n",
      "[Train] Epoch 65/100, Loss=0.6832\n",
      "[Train] Epoch 70/100, Loss=0.6792\n",
      "[Train] Epoch 75/100, Loss=0.6897\n",
      "[Train] Epoch 80/100, Loss=0.6898\n",
      "[Train] Epoch 85/100, Loss=0.6917\n",
      "[Train] Epoch 90/100, Loss=0.6949\n",
      "[Train] Epoch 95/100, Loss=0.6890\n",
      "[Train] Epoch 100/100, Loss=0.6844\n",
      "Accuracy on Nonzero Predictions: 0.5873\n",
      "Training AAPL | LR: 0.011 | Epochs: 100 | Batch: 32 | Prediction Threshold: 0.5\n",
      "[Train] Epoch 5/100, Loss=0.6883\n",
      "[Train] Epoch 10/100, Loss=0.6875\n",
      "[Train] Epoch 15/100, Loss=0.6813\n",
      "[Train] Epoch 20/100, Loss=0.6834\n",
      "[Train] Epoch 25/100, Loss=0.6821\n",
      "[Train] Epoch 30/100, Loss=0.6779\n",
      "[Train] Epoch 35/100, Loss=0.6752\n",
      "[Train] Epoch 40/100, Loss=0.6829\n",
      "[Train] Epoch 45/100, Loss=0.6729\n",
      "[Train] Epoch 50/100, Loss=0.6747\n",
      "[Train] Epoch 55/100, Loss=0.6726\n",
      "[Train] Epoch 60/100, Loss=0.6759\n",
      "[Train] Epoch 65/100, Loss=0.6841\n",
      "[Train] Epoch 70/100, Loss=0.6780\n",
      "[Train] Epoch 75/100, Loss=0.6795\n",
      "[Train] Epoch 80/100, Loss=0.6749\n",
      "[Train] Epoch 85/100, Loss=0.6833\n",
      "[Train] Epoch 90/100, Loss=0.6757\n",
      "[Train] Epoch 95/100, Loss=0.6809\n",
      "[Train] Epoch 100/100, Loss=0.6772\n",
      "Accuracy on Nonzero Predictions: 0.5476\n",
      "[0.5634920634920635, 0.5873015873015873, 0.5476190476190477]\n",
      "[]\n",
      "pca, gru, options\n",
      "/home/jupyter-tfg2425paula/prediction_project_v3/00_data/horizontal_structure/pca/options/AAPL/10y_5_data.pkl\n",
      "Training AAPL | LR: 0.009 | Epochs: 100 | Batch: 32 | Prediction Threshold: 0.5\n",
      "[Train] Epoch 5/100, Loss=0.6942\n",
      "[Train] Epoch 10/100, Loss=0.6891\n",
      "[Train] Epoch 15/100, Loss=0.6850\n",
      "[Train] Epoch 20/100, Loss=0.6770\n",
      "[Train] Epoch 25/100, Loss=0.6717\n",
      "[Train] Epoch 30/100, Loss=0.6662\n",
      "[Train] Epoch 35/100, Loss=0.6483\n",
      "[Train] Epoch 40/100, Loss=0.6296\n",
      "[Train] Epoch 45/100, Loss=0.5988\n",
      "[Train] Epoch 50/100, Loss=0.5940\n",
      "[Train] Epoch 55/100, Loss=0.5425\n",
      "[Train] Epoch 60/100, Loss=0.5318\n",
      "[Train] Epoch 65/100, Loss=0.4919\n",
      "[Train] Epoch 70/100, Loss=0.4744\n",
      "[Train] Epoch 75/100, Loss=0.4331\n",
      "[Train] Epoch 80/100, Loss=0.4228\n",
      "[Train] Epoch 85/100, Loss=0.4039\n",
      "[Train] Epoch 90/100, Loss=0.3988\n",
      "[Train] Epoch 95/100, Loss=0.3454\n",
      "[Train] Epoch 100/100, Loss=0.3476\n",
      "Accuracy on Nonzero Predictions: 0.4809\n",
      "Training AAPL | LR: 0.01 | Epochs: 100 | Batch: 32 | Prediction Threshold: 0.5\n",
      "[Train] Epoch 5/100, Loss=0.3454\n",
      "[Train] Epoch 10/100, Loss=0.3298\n",
      "[Train] Epoch 15/100, Loss=0.3098\n",
      "[Train] Epoch 20/100, Loss=0.3172\n",
      "[Train] Epoch 25/100, Loss=0.3032\n",
      "[Train] Epoch 30/100, Loss=0.3081\n",
      "[Train] Epoch 35/100, Loss=0.2707\n",
      "[Train] Epoch 40/100, Loss=0.2983\n",
      "[Train] Epoch 45/100, Loss=0.3173\n",
      "[Train] Epoch 50/100, Loss=0.2447\n",
      "[Train] Epoch 55/100, Loss=0.2810\n",
      "[Train] Epoch 60/100, Loss=0.2559\n",
      "[Train] Epoch 65/100, Loss=0.2443\n",
      "[Train] Epoch 70/100, Loss=0.2625\n",
      "[Train] Epoch 75/100, Loss=0.2561\n",
      "[Train] Epoch 80/100, Loss=0.2642\n",
      "[Train] Epoch 85/100, Loss=0.2666\n",
      "[Train] Epoch 90/100, Loss=0.2278\n",
      "[Train] Epoch 95/100, Loss=0.2269\n",
      "[Train] Epoch 100/100, Loss=0.2205\n",
      "Accuracy on Nonzero Predictions: 0.4962\n",
      "Training AAPL | LR: 0.011 | Epochs: 100 | Batch: 32 | Prediction Threshold: 0.5\n",
      "[Train] Epoch 5/100, Loss=0.2546\n",
      "[Train] Epoch 10/100, Loss=0.2538\n",
      "[Train] Epoch 15/100, Loss=0.2337\n",
      "[Train] Epoch 20/100, Loss=0.2744\n",
      "[Train] Epoch 25/100, Loss=0.2771\n",
      "[Train] Epoch 30/100, Loss=0.2370\n",
      "[Train] Epoch 35/100, Loss=0.2486\n",
      "[Train] Epoch 40/100, Loss=0.2185\n",
      "[Train] Epoch 45/100, Loss=0.2492\n",
      "[Train] Epoch 50/100, Loss=0.2173\n",
      "[Train] Epoch 55/100, Loss=0.2446\n",
      "[Train] Epoch 60/100, Loss=0.2262\n",
      "[Train] Epoch 65/100, Loss=0.2760\n",
      "[Train] Epoch 70/100, Loss=0.2344\n",
      "[Train] Epoch 75/100, Loss=0.2232\n",
      "[Train] Epoch 80/100, Loss=0.2410\n",
      "[Train] Epoch 85/100, Loss=0.1958\n",
      "[Train] Epoch 90/100, Loss=0.2320\n",
      "[Train] Epoch 95/100, Loss=0.2024\n",
      "[Train] Epoch 100/100, Loss=0.2048\n",
      "Accuracy on Nonzero Predictions: 0.4809\n",
      "[0.48091603053435117, 0.4961832061068702, 0.48091603053435117]\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "results_list = []\n",
    "stock = \"AAPL\"\n",
    "period = \"10y\"\n",
    "possible_train_size = 95\n",
    "window_size = 5\n",
    "batch_size = 32\n",
    "num_epochs = 100\n",
    "\n",
    "prediction_threshold = 0.5\n",
    "\n",
    "output_folder = os.path.join(results_dir, f\"inidividual_trials\") \n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "results_csv_path = os.path.join(output_folder, f\"01_{stock}_proc_model_type.csv\")\n",
    "processing_types = [\"clean\", \"pca\"]\n",
    "types_securities = [\"technical\", \"options\"]\n",
    "\n",
    "results_list = []\n",
    "\n",
    "learning_rates = [0.009, 0.0099, 0.00999, 0.01, 0.011]\n",
    "\n",
    "for processing in processing_types:  # e.g. [\"clean\", \"pca\"]\n",
    "    initial_data_dir = os.path.join(project_dir, f\"00_data/{processing}\") \n",
    "\n",
    "    for model_type in model_types:\n",
    "        for security_type in types_securities:\n",
    "            \n",
    "            test_accuracy_list = []\n",
    "            pct_decrease_list = []\n",
    "            \n",
    "            # Load original data (info only)\n",
    "            filename = f\"{security_type}/{stock}/{period}_data.csv\"\n",
    "            original_input_filepath = os.path.join(initial_data_dir, filename)\n",
    "            original_data = pd.read_csv(original_input_filepath)\n",
    "\n",
    "            # Iterate over window sizes\n",
    "            print(f\"{processing}, {model_type}, {security_type}\")\n",
    "\n",
    "            # Load data using the 'processing' variable in path\n",
    "            pkl_filename = f\"{processing}/{security_type}/{stock}/{period}_{window_size}_data.pkl\"\n",
    "            input_filepath = os.path.join(horizontal_data_dir, pkl_filename)\n",
    "            print(input_filepath)\n",
    "            input_df = pd.read_pickle(input_filepath)\n",
    "\n",
    "            X_resampled, y_resampled = reshape_remove_characters(input_df)\n",
    "\n",
    "            input_size = X_resampled.shape[2]\n",
    "            train_size = int(X_resampled.shape[0] * possible_train_size / 100)\n",
    "            test_size = X_resampled.shape[0] - train_size\n",
    "\n",
    "            # Generate model\n",
    "            if model_type == \"gru\":\n",
    "                model = GRU3DClassifier(input_size, hidden_size, output_size, num_layers, dropout)\n",
    "            elif model_type == \"lstm\":\n",
    "                model = StockPriceLSTM(input_size, hidden_size, output_size)\n",
    "\n",
    "            model = model.to(device)\n",
    "            \n",
    "            for learning_rate in learning_rates:\n",
    "                optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "                print(f\"Training {stock} | LR: {learning_rate} | Epochs: {num_epochs} \"\n",
    "                      f\"| Batch: {batch_size} | Prediction Threshold: {prediction_threshold}\")\n",
    "\n",
    "                result = evaluate_rolling_unchanged_model_threshold(\n",
    "                    model, \n",
    "                    X_resampled, \n",
    "                    y_resampled, \n",
    "                    criterion, \n",
    "                    optimizer, \n",
    "                    device, \n",
    "                    train_size, \n",
    "                    batch_size, \n",
    "                    num_epochs, \n",
    "                    lower_threshold=prediction_threshold\n",
    "                )     \n",
    "\n",
    "                rolling_predictions = result[\"rolling_predictions\"]\n",
    "                rolling_targets = result[\"rolling_targets\"]\n",
    "                test_accuracy = result[\"accuracy_nonzero\"]\n",
    "                loss_decrease_percentage = result[\"loss_decrease_percentage\"]\n",
    "                nonzero_preds = np.count_nonzero(rolling_predictions)\n",
    "                \n",
    "                test_accuracy_list.append(test_accuracy)\n",
    "\n",
    "            print(test_accuracy_list)\n",
    "            print(pct_decrease_list)\n",
    "            # 1) Create a record (dictionary) for this run\n",
    "            run_record = {\"STOCK\": stock,\n",
    "                \"DATA_TYPE\": security_type,\n",
    "                \"MODEL\": model_type.upper(),  # Convert to uppercase for consistency\n",
    "                \"PROCESSING\": processing,\n",
    "                \"MEAN_ACCURACY\": np.mean(test_accuracy_list),\n",
    "                \"MAX_ACCURACY\": np.max(test_accuracy_list),\n",
    "                \"MIN_ACCURACY\": np.min(test_accuracy_list)}\n",
    "\n",
    "            # 2) Append to list\n",
    "            results_list.append(run_record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a1abaee6-0df1-44b7-b371-b87b92bff765",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) Write to CSV once for this batch of window_sizes (avoid partial duplication)\n",
    "if len(results_list) > 0:\n",
    "    df = pd.DataFrame(results_list)\n",
    "\n",
    "    if os.path.exists(results_csv_path):\n",
    "        # Append without header\n",
    "        df.to_csv(results_csv_path, mode='a', header=False, index=False)\n",
    "    else:\n",
    "        # Write new file with header\n",
    "        df.to_csv(results_csv_path, index=False)\n",
    "\n",
    "    # Clear the list before next iteration\n",
    "    results_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "796b209d-9eed-4763-9567-5e143e2577b6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
