{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "940889cf-06e0-4e8c-8efa-1eeacc97ca57",
   "metadata": {},
   "source": [
    "## **This notebook aims to find the best hyperparameters for AAPL, 10y**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e46c228-21f4-4e80-a4e8-b62981ab4799",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.metrics import accuracy_score\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a11a538-40d5-49c3-92e9-e26fdcefe083",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"  # Use GPU 0 in first notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b259658-5fed-4231-9f06-f9db567e1d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "project_dir = \"/home/jupyter-tfg2425paula/prediction_project_v3\"\n",
    "os.chdir(project_dir)\n",
    "\n",
    "clean_data_dir = os.path.join(project_dir, \"00_data/clean\")\n",
    "horizontal_data_dir = os.path.join(project_dir, \"00_data/horizontal_structure\")\n",
    "results_dir = os.path.join(project_dir, \"02_results\")\n",
    "plots_dir = os.path.join(project_dir, \"03_plots\")\n",
    "pca_data_dir = os.path.join(project_dir, \"00_data/pca\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7319029-ba4c-4c90-91b9-a0a02b5be9af",
   "metadata": {},
   "source": [
    "### **GRU Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a7a1485d-99bb-4358-a216-1c1c75e50ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRU3DClassifier(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers, dropout):\n",
    "        super(GRU3DClassifier, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "\n",
    "        out, _ = self.gru(x, h0)\n",
    "        out = self.fc(out[:, -1, :]) \n",
    "        # return self.sigmoid(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3061a6b4-dad3-4129-a2ed-0098e04440e4",
   "metadata": {},
   "source": [
    "### **LSTM Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "95ffa6e2-2c35-48de-8c95-b7ce86fe9dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StockPriceLSTM(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers=1, dropout=0.0):\n",
    "        super(StockPriceLSTM, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers=num_layers, \n",
    "                            batch_first=True, dropout=dropout)\n",
    "    \n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)  # Get the batch size dynamically\n",
    "\n",
    "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_dim).to(x.device)  # (num_layers, batch_size, hidden_dim)\n",
    "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_dim).to(x.device)  # (num_layers, batch_size, hidden_dim)\n",
    "        \n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "\n",
    "        out = self.fc(out[:, -1, :]) \n",
    "        # out = self.sigmoid(out)\n",
    "        return out\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df6d4b78-1492-45ea-a9af-854d8d69b8de",
   "metadata": {},
   "source": [
    "### **Choose data types**\n",
    "Okay, we know what suits better AAPL 10y data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f9fb2cc-d8de-4e4f-be9d-f8cb7aac8e16",
   "metadata": {},
   "source": [
    "Parameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "145e5ba2-35c7-4a71-b100-624ca6ed1ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds = [0.3, 0.35, 0.4, 0.45, 0.5]\n",
    "thresholds = [0.5]\n",
    "learning_rates = [0.005, 0.008, 0.009, 0.01]\n",
    "learning_rates = [0.01]\n",
    "num_epochs_list = [100, 200]\n",
    "num_epochs_list = [100]\n",
    "batch_sizes = [16, 32]\n",
    "batch_sizes = [16]\n",
    "prediction_thresholds = [0.35, 0.4, 0.45, 0.5]\n",
    "prediction_thresholds = [0.5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbbee35f-0147-48b0-93ef-d9687984bbd0",
   "metadata": {},
   "source": [
    "#### **Model and Hyperparameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "71861bc1-7067-47f8-b3dd-a5cef2bc0f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "hidden_size = 64  \n",
    "output_size = 2  \n",
    "num_layers = 2\n",
    "dropout = 0.2\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "60900dea-a72e-4b53-979b-aded1e725489",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_types = [\"lstm\", \"gru\"]   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d873f70-b583-4e84-9289-bc303ef0446f",
   "metadata": {},
   "source": [
    "#### **Last data modifications**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "85729e3c-f060-4dc2-a4fc-a80573fefd73",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def reshape_remove_characters(df):\n",
    "\n",
    "    X = np.array([np.stack(row) for row in df.drop(columns=['Target']).values])\n",
    "    y = df['Target'].values\n",
    "\n",
    "    smote = SMOTE(random_state=42)\n",
    "    n_samples, timesteps, n_features = X.shape\n",
    "    X_flat = X.reshape((n_samples, timesteps * n_features))\n",
    "    X_flat = np.where(X_flat == 'ç', 0, X_flat)\n",
    "\n",
    "    X_resampled = X_flat.reshape((-1, timesteps, n_features))\n",
    "    \n",
    "    return X_resampled, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2cc09a85-0e99-4103-9b10-883d7f7cc857",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def combine_stocks_pkl_df(processing, security_type, period, window_size, project_dir, stocks):\n",
    "\n",
    "    X_list = []\n",
    "    y_list = []\n",
    "\n",
    "    for stock in stocks:\n",
    "        initial_data_dir = os.path.join(project_dir, f\"00_data/{processing}\") \n",
    "\n",
    "        # 1) Load original data (info only)\n",
    "        filename = f\"{security_type}/{stock}/{period}_data.csv\"\n",
    "        original_input_filepath = os.path.join(initial_data_dir, filename)\n",
    "        original_data = pd.read_csv(original_input_filepath)\n",
    "\n",
    "        print(f\"----- Appending stock: {stock}, period: {period}, data_type: {security_type} -----\")\n",
    "\n",
    "        # 2) Load the preprocessed data\n",
    "        pkl_filename = f\"{processing}/{security_type}/{stock}/{period}_{window_size}_data.pkl\"\n",
    "        input_filepath = os.path.join(horizontal_data_dir, pkl_filename)\n",
    "        input_df = pd.read_pickle(input_filepath)\n",
    "\n",
    "        # 3) Reshape\n",
    "        X_resampled, y_resampled = reshape_remove_characters(input_df)\n",
    "\n",
    "        # Store in lists\n",
    "        X_list.append(X_resampled)\n",
    "        y_list.append(y_resampled)\n",
    "\n",
    "    # Concatenate all stocks along the sample dimension\n",
    "    X_combined = np.concatenate(X_list, axis=0)  # Shape: (total_samples, num_features, window_size)\n",
    "    y_combined = np.concatenate(y_list, axis=0)  # Shape: (total_samples,)\n",
    "\n",
    "    # Print shapes to verify\n",
    "    print(\"Final X shape:\", X_combined.shape)\n",
    "    print(\"Final y shape:\", y_combined.shape)\n",
    "    \n",
    "    return X_combined, y_combined"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88656719-d0c4-4bf2-aef1-31f00b167d2a",
   "metadata": {},
   "source": [
    "### **Evaluation function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "74c49327-1c0d-4706-9943-54b795cb0687",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evaluate_rolling_unchanged_model_threshold(\n",
    "    model, \n",
    "    X, \n",
    "    y, \n",
    "    criterion, \n",
    "    optimizer, \n",
    "    device, \n",
    "    train_size, \n",
    "    batch_size, \n",
    "    num_epochs, \n",
    "    lower_threshold\n",
    "):\n",
    "    \"\"\"\n",
    "    Evaluate a PyTorch model using a rolling prediction approach for time series,\n",
    "    training the model only once on the initial training set. For each time step\n",
    "    after train_size, the model makes a prediction without further parameter updates.\n",
    "    Only predicts +1 or -1 if the probability of class 1 is above/below given thresholds;\n",
    "    otherwise, predicts 0. Accuracy is computed only on nonzero predictions.\n",
    "\n",
    "    Args:\n",
    "        model:          PyTorch model to evaluate.\n",
    "        X:              Feature data (numpy array).\n",
    "        y:              Target data (numpy array).\n",
    "        criterion:      Loss function (e.g., CrossEntropyLoss).\n",
    "        optimizer:      Optimizer (e.g., Adam).\n",
    "        device:         Device for computation (CPU or GPU).\n",
    "        train_size:     Initial size of the training data (int or float).\n",
    "                        If < 1, treated as fraction of total length.\n",
    "        batch_size:     Batch size for training.\n",
    "        num_epochs:     Number of epochs for initial training only.\n",
    "        lower_threshold: Probability threshold below which model predicts -1.\n",
    "        upper_threshold: Probability threshold above which model predicts +1.\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary with the following keys:\n",
    "            - \"rolling_predictions\": All predictions (-1, 0, +1) across the test period.\n",
    "            - \"rolling_targets\": Corresponding true targets in [-1, +1].\n",
    "            - \"filtered_predictions\": Nonzero predictions only.\n",
    "            - \"filtered_targets\": Targets corresponding to nonzero predictions.\n",
    "            - \"accuracy_nonzero\": Accuracy computed only on nonzero predictions.\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert X, y to tensors\n",
    "    X = torch.tensor(X, dtype=torch.float32)\n",
    "    y = torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "    # Determine initial training set size\n",
    "    if train_size < 1.0:\n",
    "        lower_bound = int(train_size * len(X))\n",
    "    else:\n",
    "        lower_bound = train_size\n",
    "\n",
    "    # -------------------------\n",
    "    # 1) SINGLE TRAINING PHASE\n",
    "    # -------------------------\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "    \n",
    "    X_train = X[:lower_bound].to(device)\n",
    "    y_train = y[:lower_bound].to(device)\n",
    "\n",
    "    train_dataset = TensorDataset(X_train, y_train)\n",
    "    trainloader = DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size=batch_size, \n",
    "        shuffle=False,         # Keep False if order matters; True for better generalization\n",
    "        # num_workers=4,         # Adjust based on your CPU cores\n",
    "        # pin_memory=True,       # Speeds up transfer if using GPUs\n",
    "        drop_last=False        # Ensure the last batch is included\n",
    "    )\n",
    "\n",
    "    epoch_train_losses = []\n",
    "    for epoch in range(num_epochs):\n",
    "        # torch.cuda.empty_cache()\n",
    "        epoch_loss = 0.0\n",
    "        for X_batch, y_batch in trainloader:\n",
    "            X_batch = X_batch.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            pred_y = model(X_batch)   # [batch_size, num_classes]\n",
    "            loss = criterion(pred_y, y_batch)\n",
    "            loss.backward()\n",
    "\n",
    "            # Gradient clipping (optional)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "               \n",
    "        if (epoch + 1) % 5 == 0 or epoch == num_epochs - 1:\n",
    "            print(f\"[Train] Epoch {epoch+1}/{num_epochs}, Loss={epoch_loss/len(trainloader):.4f}\")\n",
    "\n",
    "        epoch_train_losses.append(epoch_loss/len(trainloader))\n",
    "        \n",
    "    loss_decrease_percentage = ((epoch_train_losses[-1] - epoch_train_losses[0]) / epoch_train_losses[0]) * 100\n",
    "    # ---------------------------------\n",
    "    # 2) ROLLING PREDICTIONS, NO UPDATE\n",
    "    # ---------------------------------\n",
    "    model.eval()\n",
    "\n",
    "    rolling_predictions = []\n",
    "    rolling_targets     = []\n",
    "\n",
    "    for i in range(lower_bound, len(X)):\n",
    "        # Single-step \"test\" sample\n",
    "        X_test = X[i:i+1].to(device)  # shape: (1, num_features)\n",
    "        y_test = y[i:i+1].to(device)  # shape: (1, )\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # Forward pass\n",
    "            pred_y = model(X_test)  # [1, num_classes]\n",
    "            probabilities = torch.softmax(pred_y, dim=1).cpu().numpy()  # shape: (1, 2)\n",
    "            prob_class_1  = probabilities[:, 1]  # shape: (1,)\n",
    "\n",
    "            # Threshold-based logic\n",
    "            # Initialize all predictions to 0\n",
    "            pred_classes = np.zeros_like(prob_class_1)\n",
    "            # Predict -1 if prob < lower_threshold\n",
    "            pred_classes[prob_class_1 < lower_threshold] = -1\n",
    "            # Predict +1 if prob > upper_threshold\n",
    "            pred_classes[prob_class_1 > 1-lower_threshold] = 1\n",
    "\n",
    "        rolling_predictions.append(pred_classes[0])  # scalar\n",
    "        rolling_targets.append(y_test.item())\n",
    "\n",
    "    rolling_predictions = np.array(rolling_predictions)\n",
    "    rolling_targets = np.array(rolling_targets).astype(int)\n",
    "\n",
    "    # Convert any 0-labeled targets to -1 if your original data is in [-1, +1]\n",
    "    # (Sometimes y might be {0,1} or {-1, +1}; adapt as needed.)\n",
    "    rolling_targets[rolling_targets == 0] = -1\n",
    "\n",
    "    # Filter out zero predictions\n",
    "    nonzero_mask = rolling_predictions != 0\n",
    "    filtered_preds = rolling_predictions[nonzero_mask]\n",
    "    filtered_targets = rolling_targets[nonzero_mask]\n",
    "\n",
    "    if len(filtered_preds) == 0:\n",
    "        accuracy_nonzero = None\n",
    "        print(\"No nonzero predictions, cannot compute thresholded accuracy.\")\n",
    "    else:\n",
    "        accuracy_nonzero = accuracy_score(filtered_targets, filtered_preds)\n",
    "        print(f\"Accuracy on Nonzero Predictions: {accuracy_nonzero:.4f}\")\n",
    "\n",
    "    return {\n",
    "        \"rolling_predictions\": rolling_predictions,\n",
    "        \"rolling_targets\": rolling_targets,\n",
    "        \"filtered_predictions\": filtered_preds,\n",
    "        \"filtered_targets\": filtered_targets,\n",
    "        \"accuracy_nonzero\": accuracy_nonzero,\n",
    "        \"loss_decrease_percentage\": loss_decrease_percentage,\n",
    "        \"final_train_loss\": epoch_train_losses[-1] \n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b794e699-83fb-4ca2-8c93-318858018a2e",
   "metadata": {},
   "source": [
    "### **4th Type of comparison:**\n",
    "\n",
    "Hyperparameters finetuning, for AAPL 10y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "836045d1-1ffb-4328-90d9-5ec333dfcc1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Appending stock: AAPL, period: 10y, data_type: technical -----\n",
      "----- Appending stock: MSFT, period: 10y, data_type: technical -----\n",
      "----- Appending stock: AMZN, period: 10y, data_type: technical -----\n",
      "----- Appending stock: NVDA, period: 10y, data_type: technical -----\n",
      "----- Appending stock: SPX, period: 10y, data_type: technical -----\n",
      "Final X shape: (12090, 92, 100)\n",
      "Final y shape: (12090,)\n"
     ]
    }
   ],
   "source": [
    "processing = \"clean\"\n",
    "stocks = ['AAPL', 'MSFT', 'AMZN', 'NVDA', 'SPX']\n",
    "security_type = \"technical\"\n",
    "period = \"10y\"\n",
    "\n",
    "possible_train_size = 95\n",
    "batch_size = 32\n",
    "num_epochs = 100\n",
    "window_size = 100\n",
    "\n",
    "# 3) Reshape\n",
    "X_combined, y_combined = combine_stocks_pkl_df(processing, security_type, period, window_size, project_dir, stocks)\n",
    "\n",
    "input_size = X_combined.shape[2]\n",
    "train_size = int(X_combined.shape[0] * possible_train_size / 100)\n",
    "test_size = X_combined.shape[0] - train_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "935b7698-c75e-496b-8850-8053044ce7f9",
   "metadata": {},
   "source": [
    "### **Iterate over hyperparameters using OPTUNA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8953eb19-f0c8-4c66-bdfa-9bd8a8a92b1a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evaluate_rolling_unchanged_model_threshold(\n",
    "    model, \n",
    "    X, \n",
    "    y, \n",
    "    criterion, \n",
    "    optimizer, \n",
    "    device, \n",
    "    train_size, \n",
    "    batch_size, \n",
    "    num_epochs, \n",
    "    lower_threshold\n",
    "):\n",
    "    \"\"\"\n",
    "    Evaluate a PyTorch model using a rolling prediction approach for time series,\n",
    "    training the model only once on the initial training set. For each time step\n",
    "    after train_size, the model makes a prediction without further parameter updates.\n",
    "    Only predicts +1 or -1 if the probability of class 1 is above/below given thresholds;\n",
    "    otherwise, predicts 0. Accuracy is computed only on nonzero predictions.\n",
    "\n",
    "    Args:\n",
    "        model:          PyTorch model to evaluate.\n",
    "        X:              Feature data (numpy array).\n",
    "        y:              Target data (numpy array).\n",
    "        criterion:      Loss function (e.g., CrossEntropyLoss).\n",
    "        optimizer:      Optimizer (e.g., Adam).\n",
    "        device:         Device for computation (CPU or GPU).\n",
    "        train_size:     Initial size of the training data (int or float).\n",
    "                        If < 1, treated as fraction of total length.\n",
    "        batch_size:     Batch size for training.\n",
    "        num_epochs:     Number of epochs for initial training only.\n",
    "        lower_threshold: Probability threshold below which model predicts -1.\n",
    "        upper_threshold: Probability threshold above which model predicts +1.\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary with the following keys:\n",
    "            - \"rolling_predictions\": All predictions (-1, 0, +1) across the test period.\n",
    "            - \"rolling_targets\": Corresponding true targets in [-1, +1].\n",
    "            - \"filtered_predictions\": Nonzero predictions only.\n",
    "            - \"filtered_targets\": Targets corresponding to nonzero predictions.\n",
    "            - \"accuracy_nonzero\": Accuracy computed only on nonzero predictions.\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert X, y to tensors\n",
    "    X = torch.tensor(X, dtype=torch.float32)\n",
    "    y = torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "    # Determine initial training set size\n",
    "    if train_size < 1.0:\n",
    "        lower_bound = int(train_size * len(X))\n",
    "    else:\n",
    "        lower_bound = train_size\n",
    "\n",
    "    # -------------------------\n",
    "    # 1) SINGLE TRAINING PHASE\n",
    "    # -------------------------\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "    \n",
    "    X_train = X[:lower_bound].to(device)\n",
    "    y_train = y[:lower_bound].to(device)\n",
    "\n",
    "    train_dataset = TensorDataset(X_train, y_train)\n",
    "    trainloader = DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size=batch_size, \n",
    "        shuffle=False,         # Keep False if order matters; True for better generalization\n",
    "        # num_workers=4,         # Adjust based on your CPU cores\n",
    "        # pin_memory=True,       # Speeds up transfer if using GPUs\n",
    "        drop_last=False        # Ensure the last batch is included\n",
    "    )\n",
    "\n",
    "    epoch_train_losses = []\n",
    "    for epoch in range(num_epochs):\n",
    "        # torch.cuda.empty_cache()\n",
    "        epoch_loss = 0.0\n",
    "        for X_batch, y_batch in trainloader:\n",
    "            X_batch = X_batch.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            pred_y = model(X_batch)   # [batch_size, num_classes]\n",
    "            loss = criterion(pred_y, y_batch)\n",
    "            loss.backward()\n",
    "\n",
    "            # Gradient clipping (optional)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "               \n",
    "        if (epoch + 1) % 5 == 0 or epoch == num_epochs - 1:\n",
    "            print(f\"[Train] Epoch {epoch+1}/{num_epochs}, Loss={epoch_loss/len(trainloader):.4f}\")\n",
    "\n",
    "        epoch_train_losses.append(epoch_loss/len(trainloader))\n",
    "        \n",
    "    loss_decrease_percentage = ((epoch_train_losses[-1] - epoch_train_losses[0]) / epoch_train_losses[0]) * 100\n",
    "    # ---------------------------------\n",
    "    # 2) ROLLING PREDICTIONS, NO UPDATE\n",
    "    # ---------------------------------\n",
    "    model.eval()\n",
    "\n",
    "    rolling_predictions = []\n",
    "    rolling_targets     = []\n",
    "\n",
    "    for i in range(lower_bound, len(X)):\n",
    "        # Single-step \"test\" sample\n",
    "        X_test = X[i:i+1].to(device)  # shape: (1, num_features)\n",
    "        y_test = y[i:i+1].to(device)  # shape: (1, )\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # Forward pass\n",
    "            pred_y = model(X_test)  # [1, num_classes]\n",
    "            probabilities = torch.softmax(pred_y, dim=1).cpu().numpy()  # shape: (1, 2)\n",
    "            prob_class_1  = probabilities[:, 1]  # shape: (1,)\n",
    "\n",
    "            # Threshold-based logic\n",
    "            # Initialize all predictions to 0\n",
    "            pred_classes = np.zeros_like(prob_class_1)\n",
    "            # Predict -1 if prob < lower_threshold\n",
    "            pred_classes[prob_class_1 < lower_threshold] = -1\n",
    "            # Predict +1 if prob > upper_threshold\n",
    "            pred_classes[prob_class_1 > 1-lower_threshold] = 1\n",
    "\n",
    "        rolling_predictions.append(pred_classes[0])  # scalar\n",
    "        rolling_targets.append(y_test.item())\n",
    "\n",
    "    rolling_predictions = np.array(rolling_predictions)\n",
    "    rolling_targets = np.array(rolling_targets).astype(int)\n",
    "\n",
    "    # Convert any 0-labeled targets to -1 if your original data is in [-1, +1]\n",
    "    # (Sometimes y might be {0,1} or {-1, +1}; adapt as needed.)\n",
    "    rolling_targets[rolling_targets == 0] = -1\n",
    "\n",
    "    # Filter out zero predictions\n",
    "    nonzero_mask = rolling_predictions != 0\n",
    "    filtered_preds = rolling_predictions[nonzero_mask]\n",
    "    filtered_targets = rolling_targets[nonzero_mask]\n",
    "\n",
    "    if len(filtered_preds) == 0:\n",
    "        accuracy_nonzero = None\n",
    "        print(\"No nonzero predictions, cannot compute thresholded accuracy.\")\n",
    "    else:\n",
    "        accuracy_nonzero = accuracy_score(filtered_targets, filtered_preds)\n",
    "        print(f\"Accuracy on Nonzero Predictions: {accuracy_nonzero:.4f}\")\n",
    "\n",
    "    return {\n",
    "        \"rolling_predictions\": rolling_predictions,\n",
    "        \"rolling_targets\": rolling_targets,\n",
    "        \"filtered_predictions\": filtered_preds,\n",
    "        \"filtered_targets\": filtered_targets,\n",
    "        \"accuracy_nonzero\": accuracy_nonzero,\n",
    "        \"loss_decrease_percentage\": loss_decrease_percentage\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "28c102c4-2e65-4e07-9d0d-d9f4ef3c55d0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import optuna\n",
    "from optuna.trial import Trial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c439a97e-d12f-4c6d-916e-57cab1c9ef41",
   "metadata": {},
   "source": [
    "### **Define Optuna objective function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b2b1f4-5019-4bdb-aefd-29e0458affd1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-14 12:19:51,897] A new study created in memory with name: no-name-269751d8-55ce-4496-b3b3-7b3ed6a42498\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 5/100, Loss=0.6168\n",
      "[Train] Epoch 10/100, Loss=0.5311\n",
      "[Train] Epoch 15/100, Loss=0.5539\n",
      "[Train] Epoch 20/100, Loss=0.6062\n",
      "[Train] Epoch 25/100, Loss=0.6015\n",
      "[Train] Epoch 30/100, Loss=0.6092\n",
      "[Train] Epoch 35/100, Loss=0.6482\n",
      "[Train] Epoch 40/100, Loss=0.6644\n",
      "[Train] Epoch 45/100, Loss=0.6957\n",
      "[Train] Epoch 50/100, Loss=0.6964\n",
      "[Train] Epoch 55/100, Loss=0.6907\n",
      "[Train] Epoch 60/100, Loss=0.7111\n",
      "[Train] Epoch 65/100, Loss=0.7286\n",
      "[Train] Epoch 70/100, Loss=0.7361\n",
      "[Train] Epoch 75/100, Loss=0.7219\n",
      "[Train] Epoch 80/100, Loss=0.7242\n",
      "[Train] Epoch 85/100, Loss=0.7320\n",
      "[Train] Epoch 90/100, Loss=0.7208\n",
      "[Train] Epoch 95/100, Loss=0.7354\n",
      "[Train] Epoch 100/100, Loss=0.7385\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-14 12:32:15,952] Trial 0 finished with value: 0.5256198347107438 and parameters: {'model_type': 'gru', 'learning_rate': 0.010246491531485753, 'batch_size': 64, 'num_epochs': 100}. Best is trial 0 with value: 0.5256198347107438.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on Nonzero Predictions: 0.5256\n",
      "[Train] Epoch 5/80, Loss=0.3109\n",
      "[Train] Epoch 10/80, Loss=0.1016\n",
      "[Train] Epoch 15/80, Loss=0.0553\n",
      "[Train] Epoch 20/80, Loss=0.0752\n",
      "[Train] Epoch 25/80, Loss=0.0181\n",
      "[Train] Epoch 30/80, Loss=0.0675\n",
      "[Train] Epoch 35/80, Loss=0.0231\n",
      "[Train] Epoch 40/80, Loss=0.0012\n",
      "[Train] Epoch 45/80, Loss=0.0005\n",
      "[Train] Epoch 50/80, Loss=0.0003\n",
      "[Train] Epoch 55/80, Loss=0.0002\n",
      "[Train] Epoch 60/80, Loss=0.0001\n",
      "[Train] Epoch 65/80, Loss=0.0001\n",
      "[Train] Epoch 70/80, Loss=0.0000\n",
      "[Train] Epoch 75/80, Loss=0.0000\n",
      "[Train] Epoch 80/80, Loss=0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-14 12:44:12,253] Trial 1 finished with value: 0.6479338842975206 and parameters: {'model_type': 'lstm', 'learning_rate': 0.010781551739547245, 'batch_size': 64, 'num_epochs': 80}. Best is trial 1 with value: 0.6479338842975206.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on Nonzero Predictions: 0.6479\n",
      "[Train] Epoch 5/90, Loss=0.2887\n",
      "[Train] Epoch 10/90, Loss=0.1039\n",
      "[Train] Epoch 15/90, Loss=0.0246\n",
      "[Train] Epoch 20/90, Loss=0.0426\n",
      "[Train] Epoch 25/90, Loss=0.0013\n",
      "[Train] Epoch 30/90, Loss=0.0006\n",
      "[Train] Epoch 35/90, Loss=0.0003\n",
      "[Train] Epoch 40/90, Loss=0.0002\n",
      "[Train] Epoch 45/90, Loss=0.0001\n",
      "[Train] Epoch 50/90, Loss=0.0001\n",
      "[Train] Epoch 55/90, Loss=0.0000\n",
      "[Train] Epoch 60/90, Loss=0.0000\n",
      "[Train] Epoch 65/90, Loss=0.0000\n",
      "[Train] Epoch 70/90, Loss=0.0000\n",
      "[Train] Epoch 75/90, Loss=0.0000\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "\n",
    "def objective(trial, X, y, device=\"cpu\"):\n",
    "    \"\"\"\n",
    "    Returns accuracy (to be maximized), so we will call:\n",
    "        study = optuna.create_study(direction=\"maximize\")\n",
    "    \"\"\"\n",
    "    # -----------------------------------------------------------\n",
    "    # 1) Choose Model Type\n",
    "    #    Make sure we match the capitalization \"LSTM\" vs. \"GRU.\"\n",
    "    # -----------------------------------------------------------\n",
    "    model_type = trial.suggest_categorical(\"model_type\", [\"lstm\", \"gru\"])\n",
    "    \n",
    "    # -----------------------------------------------------------\n",
    "    # 2) Sample Hyperparameters around known best values\n",
    "    # -----------------------------------------------------------\n",
    "    # Best found was ~0.01053, so we widen around that a bit:\n",
    "    learning_rate = trial.suggest_float(\"learning_rate\", \n",
    "                                        0.009, 0.011, log=True)\n",
    "    \n",
    "    # Best was 64; we keep 16,32,64 in the search:\n",
    "    batch_size    = trial.suggest_categorical(\"batch_size\", [64])\n",
    "    \n",
    "    # Best was 80 epochs; we allow from 20 to 100:\n",
    "    num_epochs    = trial.suggest_int(\"num_epochs\", 80, 100, step=10)\n",
    "\n",
    "    # You can also tune hidden_size, num_layers, dropout, etc.\n",
    "    hidden_size   = 64\n",
    "    output_size   = 2\n",
    "    \n",
    "    # Some placeholders for whatever your model classes need:\n",
    "    input_size    = X.shape[2]  # e.g. the \"features\" dimension\n",
    "    num_layers    = 1\n",
    "    dropout       = 0.0\n",
    "\n",
    "    # -----------------------------------------------------------\n",
    "    # 3) Build Model\n",
    "    # -----------------------------------------------------------\n",
    "    if model_type == \"lstm\":\n",
    "        model = StockPriceLSTM(input_size, hidden_size, output_size)\n",
    "    else:\n",
    "        model = GRU3DClassifier(input_size, hidden_size, output_size,\n",
    "                                num_layers, dropout)\n",
    "\n",
    "    model = model.to(device)\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # -----------------------------------------------------------\n",
    "    # 4) Train / Evaluate\n",
    "    # -----------------------------------------------------------\n",
    "    # Example of how you might slice training set:\n",
    "    train_size_percent = possible_train_size / 100\n",
    "    if isinstance(train_size_percent, float):\n",
    "        actual_train_size = int(train_size_percent * X.shape[0])\n",
    "    else:\n",
    "        actual_train_size = train_size_percent\n",
    "\n",
    "    result = evaluate_rolling_unchanged_model_threshold(\n",
    "        model=model,\n",
    "        X=X,\n",
    "        y=y,\n",
    "        criterion=criterion,\n",
    "        optimizer=optimizer,\n",
    "        device=device,\n",
    "        train_size=actual_train_size,\n",
    "        batch_size=batch_size,\n",
    "        num_epochs=num_epochs,\n",
    "        lower_threshold=0.5\n",
    "    )\n",
    "\n",
    "    # Extract the accuracy on nonzero predictions\n",
    "    accuracy = result[\"accuracy_nonzero\"]\n",
    "    \n",
    "    # If model never predicts nonzero => heavy penalty\n",
    "    if accuracy is None:\n",
    "        return 0.0  # or return float(\"-inf\")\n",
    "\n",
    "    # Now we just return accuracy so that Optuna will maximize it\n",
    "    return accuracy\n",
    "\n",
    "# Then create your study as follows:\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(lambda trial: objective(trial, X_combined, y_combined), n_trials=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "549c303b-01c1-46c1-a9c3-8107bf29d049",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best (1 - accuracy): 0.6106870229007634\n",
      "Best hyperparameters: {'model_type': 'gru', 'learning_rate': 0.009883203468677031, 'batch_size': 32, 'num_epochs': 80}\n"
     ]
    }
   ],
   "source": [
    "print(\"Best (1 - accuracy):\", study.best_value)\n",
    "print(\"Best hyperparameters:\", study.best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f1a37b-1204-45b5-82ec-d1efafa1be01",
   "metadata": {},
   "source": [
    "### **Save Optuna combinations of hyperparameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9764fdd4-4565-49c3-aa66-b55cbb847d92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to optuna_hyperparameter_results.csv\n"
     ]
    }
   ],
   "source": [
    "df_results = pd.DataFrame([\n",
    "    {\n",
    "        \"trial\": trial.number,\n",
    "        \"model_type\": trial.params[\"model_type\"],\n",
    "        \"learning_rate\": trial.params[\"learning_rate\"],\n",
    "        \"batch_size\": trial.params[\"batch_size\"],\n",
    "        \"num_epochs\": trial.params[\"num_epochs\"],\n",
    "        \"accuracy\": trial.value\n",
    "    }\n",
    "    for trial in study.trials\n",
    "])\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# Display DataFrame\n",
    "# -----------------------------------------------------------\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# (Optional) Save results to a CSV file\n",
    "# -----------------------------------------------------------\n",
    "df_results.to_csv(os.path.join(results_dir, \"optuna_hyperparameter_results.csv\"), index=False)\n",
    "\n",
    "print(\"Results saved to optuna_hyperparameter_results.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5075f582-67b2-4339-bf9b-0ff7d8a1f544",
   "metadata": {
    "tags": []
   },
   "source": [
    "### **Choose Optuna hyperparameter** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3a60d375-1d6c-4840-9044-0d53a494d39c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 5/80, Loss=0.6940\n",
      "[Train] Epoch 10/80, Loss=0.6937\n",
      "[Train] Epoch 15/80, Loss=0.6936\n",
      "[Train] Epoch 20/80, Loss=0.6933\n",
      "[Train] Epoch 25/80, Loss=0.6931\n",
      "[Train] Epoch 30/80, Loss=0.6930\n",
      "[Train] Epoch 35/80, Loss=0.6921\n",
      "[Train] Epoch 40/80, Loss=0.6888\n",
      "[Train] Epoch 45/80, Loss=0.6881\n",
      "[Train] Epoch 50/80, Loss=0.6874\n",
      "[Train] Epoch 55/80, Loss=0.6859\n",
      "[Train] Epoch 60/80, Loss=0.6853\n",
      "[Train] Epoch 65/80, Loss=0.6843\n",
      "[Train] Epoch 70/80, Loss=0.6811\n",
      "[Train] Epoch 75/80, Loss=0.6786\n",
      "[Train] Epoch 80/80, Loss=0.6820\n",
      "Accuracy on Nonzero Predictions: 0.6336\n"
     ]
    }
   ],
   "source": [
    "batch_size= 32\n",
    "num_epochs= 80\n",
    "gru_model = GRU3DClassifier(input_size, hidden_size, output_size, num_layers, dropout)\n",
    "optimizer = optim.Adam(gru_model.parameters(), lr=0.009883203468677031)\n",
    "train_size_percent = possible_train_size/100\n",
    "if isinstance(train_size_percent, float):\n",
    "    actual_train_size = int(train_size_percent * X_resampled.shape[0])\n",
    "else:\n",
    "    actual_train_size = train_size_percent\n",
    "    \n",
    "result = evaluate_rolling_unchanged_model_threshold(\n",
    "    model=gru_model,\n",
    "    X=X_resampled,\n",
    "    y=y_resampled,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    device=device,\n",
    "    train_size=actual_train_size,\n",
    "    batch_size=batch_size,\n",
    "    num_epochs=num_epochs,\n",
    "    lower_threshold=0.5,   # as requested\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "011486f4-bd48-4cd7-8dd3-4c7365acf656",
   "metadata": {},
   "source": [
    "### **Iterate with Optuna hyperparameter**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "cbd281dd-957a-4645-a4f9-6d7fff21976c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----- LEARNING_RATE: 0.009883203468677031, SECURITY_TYPE: single_name, MODEL_TYPE: gru -----\n",
      "0\n",
      "Training AAPL | LR: 0.009883203468677031 | Epochs: 80 | Batch: 32 | Security type: single_name\n",
      "[Train] Epoch 5/80, Loss=0.6938\n",
      "[Train] Epoch 10/80, Loss=0.6937\n",
      "[Train] Epoch 15/80, Loss=0.6932\n",
      "[Train] Epoch 20/80, Loss=0.6924\n",
      "[Train] Epoch 25/80, Loss=0.6915\n",
      "[Train] Epoch 30/80, Loss=0.6909\n",
      "[Train] Epoch 35/80, Loss=0.6887\n",
      "[Train] Epoch 40/80, Loss=0.6897\n",
      "[Train] Epoch 45/80, Loss=0.6882\n",
      "[Train] Epoch 50/80, Loss=0.6886\n",
      "[Train] Epoch 55/80, Loss=0.6886\n",
      "[Train] Epoch 60/80, Loss=0.6849\n",
      "[Train] Epoch 65/80, Loss=0.6852\n",
      "[Train] Epoch 70/80, Loss=0.6831\n",
      "[Train] Epoch 75/80, Loss=0.6831\n",
      "[Train] Epoch 80/80, Loss=0.6798\n",
      "Accuracy on Nonzero Predictions: 0.5420\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'final_train_loss'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [42], line 78\u001b[0m\n\u001b[1;32m     76\u001b[0m loss_decrease_percentage \u001b[38;5;241m=\u001b[39m result[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss_decrease_percentage\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     77\u001b[0m nonzero_preds \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mcount_nonzero(rolling_predictions)\n\u001b[0;32m---> 78\u001b[0m final_train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mresult\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfinal_train_loss\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;66;03m# 9) Create a record (dictionary) for this run\u001b[39;00m\n\u001b[1;32m     81\u001b[0m run_record \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     82\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSTOCK\u001b[39m\u001b[38;5;124m\"\u001b[39m: stock,\n\u001b[1;32m     83\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDATA_TYPE\u001b[39m\u001b[38;5;124m\"\u001b[39m: security_type,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     88\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFINAL_TRAIN_LOSS\u001b[39m\u001b[38;5;124m\"\u001b[39m: final_train_loss\n\u001b[1;32m     89\u001b[0m }\n",
      "\u001b[0;31mKeyError\u001b[0m: 'final_train_loss'"
     ]
    }
   ],
   "source": [
    "results_list = []\n",
    "stock = \"AAPL\"\n",
    "period = \"10y\"\n",
    "possible_train_size = 95\n",
    "batch_size = 32\n",
    "num_epochs = 80\n",
    "window_size = 3\n",
    "\n",
    "prediction_threshold = 0.5\n",
    "\n",
    "output_folder = os.path.join(results_dir, f\"individual_trials\") \n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "processing = \"clean\"\n",
    "security_types = [\"single_name\"]\n",
    "learning_rate = 0.009883203468677031\n",
    "model_type = \"gru\"\n",
    "\n",
    "results_list = []\n",
    "for security_type in security_types:\n",
    "    initial_data_dir = os.path.join(project_dir, f\"00_data/{processing}\") \n",
    "    \n",
    "    # 1) Load original data (info only)\n",
    "    filename = f\"{security_type}/{stock}/{period}_data.csv\"\n",
    "    original_input_filepath = os.path.join(initial_data_dir, filename)\n",
    "    original_data = pd.read_csv(original_input_filepath)\n",
    "\n",
    "    print(f\"\\n----- LEARNING_RATE: {learning_rate}, SECURITY_TYPE: {security_type}, MODEL_TYPE: {model_type} -----\")\n",
    "\n",
    "    # 2) Load the preprocessed data\n",
    "    pkl_filename = f\"{processing}/{security_type}/{stock}/{period}_{window_size}_data.pkl\"\n",
    "    input_filepath = os.path.join(horizontal_data_dir, pkl_filename)\n",
    "    input_df = pd.read_pickle(input_filepath)\n",
    "    \n",
    "    # 3) Reshape\n",
    "    X_resampled, y_resampled = reshape_remove_characters(input_df)\n",
    "\n",
    "    input_size = X_resampled.shape[2]\n",
    "    train_size = int(X_resampled.shape[0] * possible_train_size / 100)\n",
    "    test_size = X_resampled.shape[0] - train_size\n",
    "\n",
    "    for i in range(50):\n",
    "        print(i)\n",
    "        \n",
    "        # for model_type in model_types:\n",
    "            # 4) Initialize the model\n",
    "            # if model_type == \"gru\":\n",
    "            #     \n",
    "            # elif model_type == \"lstm\":\n",
    "        # model = StockPriceLSTM(input_size, hidden_size, output_size)\n",
    "        model = GRU3DClassifier(input_size, hidden_size, output_size, num_layers, dropout)\n",
    "        model = model.to(device)\n",
    "\n",
    "        # 5) Set up optimizer\n",
    "        optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "        print(f\"Training {stock} | LR: {learning_rate} | Epochs: {num_epochs} \"\n",
    "              f\"| Batch: {batch_size} | Security type: {security_type}\")\n",
    "\n",
    "        result = evaluate_rolling_unchanged_model_threshold(\n",
    "            model=model,\n",
    "            X=X_resampled,\n",
    "            y=y_resampled,\n",
    "            criterion=criterion,\n",
    "            optimizer=optimizer,\n",
    "            device=device,\n",
    "            train_size=train_size,\n",
    "            batch_size=batch_size,\n",
    "            num_epochs=num_epochs,\n",
    "            lower_threshold=0.5\n",
    "        )\n",
    "\n",
    "        # 7) Extract results\n",
    "        rolling_predictions = result[\"rolling_predictions\"]\n",
    "        rolling_targets = result[\"rolling_targets\"]\n",
    "        test_accuracy = result[\"accuracy_nonzero\"]\n",
    "        loss_decrease_percentage = result[\"loss_decrease_percentage\"]\n",
    "        nonzero_preds = np.count_nonzero(rolling_predictions)\n",
    "        final_train_loss = result[\"final_train_loss\"]\n",
    "\n",
    "        # 9) Create a record (dictionary) for this run\n",
    "        run_record = {\n",
    "            \"STOCK\": stock,\n",
    "            \"DATA_TYPE\": security_type,\n",
    "            \"MODEL\": model_type.upper(),\n",
    "            \"PROCESSING\": processing,\n",
    "            \"ACCURACY\": test_accuracy,\n",
    "            \"TRAIN_PCT_DECREASE\": loss_decrease_percentage,\n",
    "            \"FINAL_TRAIN_LOSS\": final_train_loss\n",
    "        }\n",
    "\n",
    "        # 10) Append to the results_list\n",
    "        results_list.append(run_record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5ae28c8f-393e-4605-abef-362a06fc935a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A modo de ejemplo, generamos datos aleatorios\n",
    "num_samples = 1000\n",
    "seq_length = 10\n",
    "num_features = 5\n",
    "\n",
    "X = np.random.rand(num_samples, seq_length, num_features).astype(np.float32)\n",
    "y = np.random.choice([0, 1], size=num_samples).astype(int)  \n",
    "# O y = np.random.choice([-1, +1], size=num_samples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2b652af5-13e0-4c21-89cc-f289e3ce1951",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------\n",
    "# 3) Definir la función objetivo para Optuna\n",
    "# ---------------------------------------------------------\n",
    "def objective(trial):\n",
    "    \"\"\"\n",
    "    Función objetivo donde:\n",
    "      - Se definen los hiperparámetros que queremos optimizar\n",
    "      - Se construye y entrena el modelo con esos hiperparámetros\n",
    "      - Se obtienen las métricas de la función evaluate_rolling_unchanged_model_threshold\n",
    "    \"\"\"\n",
    "    \n",
    "    # Hiperparámetros a \"samplear\":\n",
    "    learning_rate = trial.suggest_loguniform('learning_rate',0.009, 0.01)\n",
    "    batch_size = trial.suggest_categorical('batch_size', [16, 32, 64, 128])\n",
    "\n",
    "    # Definir el modelo\n",
    "    input_size = X_resampled.shape[2]\n",
    "    train_size = int(X_resampled.shape[0] * possible_train_size / 100)\n",
    "    test_size = X_resampled.shape[0] - train_size\n",
    "    \n",
    "    \n",
    "    model = StockPriceLSTM(input_size, hidden_size, output_size)\n",
    "\n",
    "    # Definir criterio y optimizador\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Parámetros fijos de tu evaluación\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    train_size = 0.7  # Ejemplo (70% entrenar, 30% test)\n",
    "    num_epochs = 10\n",
    "    lower_threshold = 0.5\n",
    "\n",
    "    # Llamar a tu función de evaluación\n",
    "    results = evaluate_rolling_unchanged_model_threshold(\n",
    "        model=model,\n",
    "        X=X_resampled,\n",
    "        y=y_resampled,\n",
    "        criterion=criterion,\n",
    "        optimizer=optimizer,\n",
    "        device=device,\n",
    "        train_size=train_size,\n",
    "        batch_size=batch_size,\n",
    "        num_epochs=num_epochs,\n",
    "        lower_threshold=lower_threshold\n",
    "        # NOTA: si en tu función original también pasas upper_threshold, agrégalo aquí\n",
    "    )\n",
    "\n",
    "    # Queremos optimizar accuracy_nonzero (o el metric que elijas)\n",
    "    accuracy_nonzero = results[\"accuracy_nonzero\"]\n",
    "\n",
    "    # A veces puede pasar que no haya predicciones != 0; en tal caso, retornamos 0 para \"penalizar\"\n",
    "    if accuracy_nonzero is None:\n",
    "        return 0.0\n",
    "    else:\n",
    "        return accuracy_nonzero\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6d50093c-1ef8-4837-83cc-1bdb32fd305b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-11 12:21:55,381] A new study created in memory with name: no-name-a89a00e1-c46c-4e97-9dc6-ce3c37404898\n",
      "/tmp/ipykernel_39327/2555981884.py:13: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate',0.009, 0.01)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 5/10, Loss=0.6924\n",
      "[Train] Epoch 10/10, Loss=0.6915\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-11 12:21:56,189] Trial 0 finished with value: 0.5044699872286079 and parameters: {'learning_rate': 0.00928854584534339, 'batch_size': 128}. Best is trial 0 with value: 0.5044699872286079.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on Nonzero Predictions: 0.5045\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_39327/2555981884.py:13: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate',0.009, 0.01)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 5/10, Loss=0.6931\n",
      "[Train] Epoch 10/10, Loss=0.6921\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-11 12:21:57,353] Trial 1 finished with value: 0.5031928480204342 and parameters: {'learning_rate': 0.009201191445315782, 'batch_size': 64}. Best is trial 0 with value: 0.5044699872286079.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on Nonzero Predictions: 0.5032\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_39327/2555981884.py:13: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate',0.009, 0.01)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 5/10, Loss=0.6940\n",
      "[Train] Epoch 10/10, Loss=0.6933\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-11 12:22:00,368] Trial 2 finished with value: 0.4929757343550447 and parameters: {'learning_rate': 0.009246580847534566, 'batch_size': 16}. Best is trial 0 with value: 0.5044699872286079.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on Nonzero Predictions: 0.4930\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_39327/2555981884.py:13: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate',0.009, 0.01)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 5/10, Loss=0.6941\n",
      "[Train] Epoch 10/10, Loss=0.6934\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-11 12:22:03,227] Trial 3 finished with value: 0.49808429118773945 and parameters: {'learning_rate': 0.009079412830845061, 'batch_size': 16}. Best is trial 0 with value: 0.5044699872286079.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on Nonzero Predictions: 0.4981\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_39327/2555981884.py:13: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate',0.009, 0.01)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 5/10, Loss=0.6932\n",
      "[Train] Epoch 10/10, Loss=0.6930\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-11 12:22:04,928] Trial 4 finished with value: 0.5006385696040868 and parameters: {'learning_rate': 0.009309370037926864, 'batch_size': 32}. Best is trial 0 with value: 0.5044699872286079.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on Nonzero Predictions: 0.5006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_39327/2555981884.py:13: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate',0.009, 0.01)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 5/10, Loss=0.6932\n",
      "[Train] Epoch 10/10, Loss=0.6930\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-11 12:22:06,551] Trial 5 finished with value: 0.4955300127713921 and parameters: {'learning_rate': 0.00960888270777087, 'batch_size': 32}. Best is trial 0 with value: 0.5044699872286079.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on Nonzero Predictions: 0.4955\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_39327/2555981884.py:13: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate',0.009, 0.01)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 5/10, Loss=0.6932\n",
      "[Train] Epoch 10/10, Loss=0.6929\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-11 12:22:08,207] Trial 6 finished with value: 0.5019157088122606 and parameters: {'learning_rate': 0.009598229802676621, 'batch_size': 32}. Best is trial 0 with value: 0.5044699872286079.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on Nonzero Predictions: 0.5019\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_39327/2555981884.py:13: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate',0.009, 0.01)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 5/10, Loss=0.6932\n",
      "[Train] Epoch 10/10, Loss=0.6929\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-11 12:22:09,870] Trial 7 finished with value: 0.5006385696040868 and parameters: {'learning_rate': 0.009508223317951441, 'batch_size': 32}. Best is trial 0 with value: 0.5044699872286079.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on Nonzero Predictions: 0.5006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_39327/2555981884.py:13: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate',0.009, 0.01)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 5/10, Loss=0.6931\n",
      "[Train] Epoch 10/10, Loss=0.6922\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-11 12:22:10,913] Trial 8 finished with value: 0.4942528735632184 and parameters: {'learning_rate': 0.009385165960245917, 'batch_size': 64}. Best is trial 0 with value: 0.5044699872286079.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on Nonzero Predictions: 0.4943\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_39327/2555981884.py:13: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate',0.009, 0.01)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 5/10, Loss=0.6941\n",
      "[Train] Epoch 10/10, Loss=0.6936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-11 12:22:13,674] Trial 9 finished with value: 0.5031928480204342 and parameters: {'learning_rate': 0.009310136340673276, 'batch_size': 16}. Best is trial 0 with value: 0.5044699872286079.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on Nonzero Predictions: 0.5032\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_39327/2555981884.py:13: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate',0.009, 0.01)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 5/10, Loss=0.6922\n",
      "[Train] Epoch 10/10, Loss=0.6912\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-11 12:22:14,483] Trial 10 finished with value: 0.508301404853129 and parameters: {'learning_rate': 0.009898054853584021, 'batch_size': 128}. Best is trial 10 with value: 0.508301404853129.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on Nonzero Predictions: 0.5083\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_39327/2555981884.py:13: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate',0.009, 0.01)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 5/10, Loss=0.6922\n",
      "[Train] Epoch 10/10, Loss=0.6912\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-11 12:22:15,270] Trial 11 finished with value: 0.5006385696040868 and parameters: {'learning_rate': 0.009981379110880428, 'batch_size': 128}. Best is trial 10 with value: 0.508301404853129.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on Nonzero Predictions: 0.5006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_39327/2555981884.py:13: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate',0.009, 0.01)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 5/10, Loss=0.6922\n",
      "[Train] Epoch 10/10, Loss=0.6911\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-11 12:22:16,062] Trial 12 finished with value: 0.49808429118773945 and parameters: {'learning_rate': 0.009991564957482792, 'batch_size': 128}. Best is trial 10 with value: 0.508301404853129.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on Nonzero Predictions: 0.4981\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_39327/2555981884.py:13: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate',0.009, 0.01)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 5/10, Loss=0.6924\n",
      "[Train] Epoch 10/10, Loss=0.6915\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-11 12:22:16,856] Trial 13 finished with value: 0.508301404853129 and parameters: {'learning_rate': 0.00976753773305084, 'batch_size': 128}. Best is trial 10 with value: 0.508301404853129.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on Nonzero Predictions: 0.5083\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_39327/2555981884.py:13: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate',0.009, 0.01)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 5/10, Loss=0.6923\n",
      "[Train] Epoch 10/10, Loss=0.6915\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-11 12:22:17,663] Trial 14 finished with value: 0.5044699872286079 and parameters: {'learning_rate': 0.00980970610962458, 'batch_size': 128}. Best is trial 10 with value: 0.508301404853129.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on Nonzero Predictions: 0.5045\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_39327/2555981884.py:13: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate',0.009, 0.01)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 5/10, Loss=0.6924\n",
      "[Train] Epoch 10/10, Loss=0.6912\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-11 12:22:18,473] Trial 15 finished with value: 0.5044699872286079 and parameters: {'learning_rate': 0.009788779602694095, 'batch_size': 128}. Best is trial 10 with value: 0.508301404853129.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on Nonzero Predictions: 0.5045\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_39327/2555981884.py:13: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate',0.009, 0.01)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 5/10, Loss=0.6925\n",
      "[Train] Epoch 10/10, Loss=0.6915\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-11 12:22:19,288] Trial 16 finished with value: 0.5006385696040868 and parameters: {'learning_rate': 0.009784285726829593, 'batch_size': 128}. Best is trial 10 with value: 0.508301404853129.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on Nonzero Predictions: 0.5006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_39327/2555981884.py:13: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate',0.009, 0.01)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 5/10, Loss=0.6923\n",
      "[Train] Epoch 10/10, Loss=0.6911\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-11 12:22:20,104] Trial 17 finished with value: 0.5095785440613027 and parameters: {'learning_rate': 0.009848611354388012, 'batch_size': 128}. Best is trial 17 with value: 0.5095785440613027.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on Nonzero Predictions: 0.5096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_39327/2555981884.py:13: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate',0.009, 0.01)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 5/10, Loss=0.6924\n",
      "[Train] Epoch 10/10, Loss=0.6916\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-11 12:22:20,913] Trial 18 finished with value: 0.5031928480204342 and parameters: {'learning_rate': 0.009911915712169237, 'batch_size': 128}. Best is trial 17 with value: 0.5095785440613027.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on Nonzero Predictions: 0.5032\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_39327/2555981884.py:13: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate',0.009, 0.01)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 5/10, Loss=0.6930\n",
      "[Train] Epoch 10/10, Loss=0.6920\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-11 12:22:22,014] Trial 19 finished with value: 0.4929757343550447 and parameters: {'learning_rate': 0.0096574347457179, 'batch_size': 64}. Best is trial 17 with value: 0.5095785440613027.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on Nonzero Predictions: 0.4930\n",
      "Number of finished trials:  20\n",
      "Best trial:\n",
      "  Value (Accuracy Nonzero): 0.5096\n",
      "  Best hyperparameters:\n",
      "    learning_rate: 0.009848611354388012\n",
      "    batch_size: 128\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------------------------------\n",
    "# 4) Crear y ejecutar el estudio de Optuna\n",
    "# ---------------------------------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    study = optuna.create_study(direction=\"maximize\")  # Queremos maximizar la accuracy\n",
    "    study.optimize(objective, n_trials=20)  # número de iteraciones de búsqueda\n",
    "\n",
    "    # Ver los mejores resultados\n",
    "    print(\"Number of finished trials: \", len(study.trials))\n",
    "    \n",
    "    best_trial = study.best_trial\n",
    "    print(\"Best trial:\")\n",
    "    print(f\"  Value (Accuracy Nonzero): {best_trial.value:.4f}\")\n",
    "    \n",
    "    print(\"  Best hyperparameters:\")\n",
    "    for key, value in best_trial.params.items():\n",
    "        print(f\"    {key}: {value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f74f78fd-d3b9-4a6a-8159-e2cd1c834579",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
