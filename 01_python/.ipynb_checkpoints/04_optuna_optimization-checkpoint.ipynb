{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "940889cf-06e0-4e8c-8efa-1eeacc97ca57",
   "metadata": {},
   "source": [
    "## **This notebook aims to find the best hyperparameters for AAPL, 10y**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e46c228-21f4-4e80-a4e8-b62981ab4799",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.metrics import accuracy_score\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a11a538-40d5-49c3-92e9-e26fdcefe083",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"  # Use GPU 0 in first notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b259658-5fed-4231-9f06-f9db567e1d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "project_dir = \"/home/jupyter-tfg2425paula/prediction_project_v3\"\n",
    "os.chdir(project_dir)\n",
    "\n",
    "clean_data_dir = os.path.join(project_dir, \"00_data/clean\")\n",
    "horizontal_data_dir = os.path.join(project_dir, \"00_data/horizontal_structure\")\n",
    "results_dir = os.path.join(project_dir, \"02_results\")\n",
    "plots_dir = os.path.join(project_dir, \"03_plots\")\n",
    "pca_data_dir = os.path.join(project_dir, \"00_data/pca\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7319029-ba4c-4c90-91b9-a0a02b5be9af",
   "metadata": {},
   "source": [
    "### **GRU Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a7a1485d-99bb-4358-a216-1c1c75e50ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRU3DClassifier(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers, dropout):\n",
    "        super(GRU3DClassifier, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "\n",
    "        out, _ = self.gru(x, h0)\n",
    "        out = self.fc(out[:, -1, :]) \n",
    "        # return self.sigmoid(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3061a6b4-dad3-4129-a2ed-0098e04440e4",
   "metadata": {},
   "source": [
    "### **LSTM Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "95ffa6e2-2c35-48de-8c95-b7ce86fe9dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StockPriceLSTM(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers=1, dropout=0.0):\n",
    "        super(StockPriceLSTM, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers=num_layers, \n",
    "                            batch_first=True, dropout=dropout)\n",
    "    \n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)  # Get the batch size dynamically\n",
    "\n",
    "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_dim).to(x.device)  # (num_layers, batch_size, hidden_dim)\n",
    "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_dim).to(x.device)  # (num_layers, batch_size, hidden_dim)\n",
    "        \n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "\n",
    "        out = self.fc(out[:, -1, :]) \n",
    "        # out = self.sigmoid(out)\n",
    "        return out\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df6d4b78-1492-45ea-a9af-854d8d69b8de",
   "metadata": {},
   "source": [
    "### **Choose data types**\n",
    "Okay, we know what suits better AAPL 10y data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e67b679f-204f-4e15-89fe-09879ed7246c",
   "metadata": {},
   "source": [
    "Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2b80ed82-98cd-453d-94e9-b7f42c61040e",
   "metadata": {},
   "outputs": [],
   "source": [
    "processing = \"clean\"\n",
    "stock = 'AAPL'\n",
    "security_type = \"single_name\"\n",
    "period = \"10y\"\n",
    "\n",
    "possible_train_size = 95\n",
    "batch_size = 32\n",
    "num_epochs = 100\n",
    "window_size = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f9fb2cc-d8de-4e4f-be9d-f8cb7aac8e16",
   "metadata": {},
   "source": [
    "Parameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "145e5ba2-35c7-4a71-b100-624ca6ed1ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds = [0.3, 0.35, 0.4, 0.45, 0.5]\n",
    "thresholds = [0.5]\n",
    "learning_rates = [0.005, 0.008, 0.009, 0.01]\n",
    "learning_rates = [0.01]\n",
    "num_epochs_list = [100, 200]\n",
    "num_epochs_list = [100]\n",
    "batch_sizes = [16, 32]\n",
    "batch_sizes = [16]\n",
    "prediction_thresholds = [0.35, 0.4, 0.45, 0.5]\n",
    "prediction_thresholds = [0.5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbbee35f-0147-48b0-93ef-d9687984bbd0",
   "metadata": {},
   "source": [
    "#### **Model and Hyperparameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "71861bc1-7067-47f8-b3dd-a5cef2bc0f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "hidden_size = 64  \n",
    "output_size = 2  \n",
    "num_layers = 2\n",
    "dropout = 0.2\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "60900dea-a72e-4b53-979b-aded1e725489",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_types = [\"lstm\", \"gru\"]   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d873f70-b583-4e84-9289-bc303ef0446f",
   "metadata": {},
   "source": [
    "#### **Last data modifications**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "85729e3c-f060-4dc2-a4fc-a80573fefd73",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def reshape_remove_characters(df):\n",
    "\n",
    "    X = np.array([np.stack(row) for row in df.drop(columns=['Target']).values])\n",
    "    y = df['Target'].values\n",
    "\n",
    "    smote = SMOTE(random_state=42)\n",
    "    n_samples, timesteps, n_features = X.shape\n",
    "    X_flat = X.reshape((n_samples, timesteps * n_features))\n",
    "    X_flat = np.where(X_flat == 'ç', 0, X_flat)\n",
    "\n",
    "    X_resampled = X_flat.reshape((-1, timesteps, n_features))\n",
    "    \n",
    "    return X_resampled, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88656719-d0c4-4bf2-aef1-31f00b167d2a",
   "metadata": {},
   "source": [
    "### **Evaluation function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "74c49327-1c0d-4706-9943-54b795cb0687",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evaluate_rolling_unchanged_model_threshold(\n",
    "    model, \n",
    "    X, \n",
    "    y, \n",
    "    criterion, \n",
    "    optimizer, \n",
    "    device, \n",
    "    train_size, \n",
    "    batch_size, \n",
    "    num_epochs, \n",
    "    lower_threshold\n",
    "):\n",
    "    \"\"\"\n",
    "    Evaluate a PyTorch model using a rolling prediction approach for time series,\n",
    "    training the model only once on the initial training set. For each time step\n",
    "    after train_size, the model makes a prediction without further parameter updates.\n",
    "    Only predicts +1 or -1 if the probability of class 1 is above/below given thresholds;\n",
    "    otherwise, predicts 0. Accuracy is computed only on nonzero predictions.\n",
    "\n",
    "    Args:\n",
    "        model:          PyTorch model to evaluate.\n",
    "        X:              Feature data (numpy array).\n",
    "        y:              Target data (numpy array).\n",
    "        criterion:      Loss function (e.g., CrossEntropyLoss).\n",
    "        optimizer:      Optimizer (e.g., Adam).\n",
    "        device:         Device for computation (CPU or GPU).\n",
    "        train_size:     Initial size of the training data (int or float).\n",
    "                        If < 1, treated as fraction of total length.\n",
    "        batch_size:     Batch size for training.\n",
    "        num_epochs:     Number of epochs for initial training only.\n",
    "        lower_threshold: Probability threshold below which model predicts -1.\n",
    "        upper_threshold: Probability threshold above which model predicts +1.\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary with the following keys:\n",
    "            - \"rolling_predictions\": All predictions (-1, 0, +1) across the test period.\n",
    "            - \"rolling_targets\": Corresponding true targets in [-1, +1].\n",
    "            - \"filtered_predictions\": Nonzero predictions only.\n",
    "            - \"filtered_targets\": Targets corresponding to nonzero predictions.\n",
    "            - \"accuracy_nonzero\": Accuracy computed only on nonzero predictions.\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert X, y to tensors\n",
    "    X = torch.tensor(X, dtype=torch.float32)\n",
    "    y = torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "    # Determine initial training set size\n",
    "    if train_size < 1.0:\n",
    "        lower_bound = int(train_size * len(X))\n",
    "    else:\n",
    "        lower_bound = train_size\n",
    "\n",
    "    # -------------------------\n",
    "    # 1) SINGLE TRAINING PHASE\n",
    "    # -------------------------\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "    \n",
    "    X_train = X[:lower_bound].to(device)\n",
    "    y_train = y[:lower_bound].to(device)\n",
    "\n",
    "    train_dataset = TensorDataset(X_train, y_train)\n",
    "    trainloader = DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size=batch_size, \n",
    "        shuffle=False,         # Keep False if order matters; True for better generalization\n",
    "        # num_workers=4,         # Adjust based on your CPU cores\n",
    "        # pin_memory=True,       # Speeds up transfer if using GPUs\n",
    "        drop_last=False        # Ensure the last batch is included\n",
    "    )\n",
    "\n",
    "    epoch_train_losses = []\n",
    "    for epoch in range(num_epochs):\n",
    "        # torch.cuda.empty_cache()\n",
    "        epoch_loss = 0.0\n",
    "        for X_batch, y_batch in trainloader:\n",
    "            X_batch = X_batch.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            pred_y = model(X_batch)   # [batch_size, num_classes]\n",
    "            loss = criterion(pred_y, y_batch)\n",
    "            loss.backward()\n",
    "\n",
    "            # Gradient clipping (optional)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "               \n",
    "        if (epoch + 1) % 5 == 0 or epoch == num_epochs - 1:\n",
    "            print(f\"[Train] Epoch {epoch+1}/{num_epochs}, Loss={epoch_loss/len(trainloader):.4f}\")\n",
    "\n",
    "        epoch_train_losses.append(epoch_loss/len(trainloader))\n",
    "        \n",
    "    loss_decrease_percentage = ((epoch_train_losses[-1] - epoch_train_losses[0]) / epoch_train_losses[0]) * 100\n",
    "    # ---------------------------------\n",
    "    # 2) ROLLING PREDICTIONS, NO UPDATE\n",
    "    # ---------------------------------\n",
    "    model.eval()\n",
    "\n",
    "    rolling_predictions = []\n",
    "    rolling_targets     = []\n",
    "\n",
    "    for i in range(lower_bound, len(X)):\n",
    "        # Single-step \"test\" sample\n",
    "        X_test = X[i:i+1].to(device)  # shape: (1, num_features)\n",
    "        y_test = y[i:i+1].to(device)  # shape: (1, )\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # Forward pass\n",
    "            pred_y = model(X_test)  # [1, num_classes]\n",
    "            probabilities = torch.softmax(pred_y, dim=1).cpu().numpy()  # shape: (1, 2)\n",
    "            prob_class_1  = probabilities[:, 1]  # shape: (1,)\n",
    "\n",
    "            # Threshold-based logic\n",
    "            # Initialize all predictions to 0\n",
    "            pred_classes = np.zeros_like(prob_class_1)\n",
    "            # Predict -1 if prob < lower_threshold\n",
    "            pred_classes[prob_class_1 < lower_threshold] = -1\n",
    "            # Predict +1 if prob > upper_threshold\n",
    "            pred_classes[prob_class_1 > 1-lower_threshold] = 1\n",
    "\n",
    "        rolling_predictions.append(pred_classes[0])  # scalar\n",
    "        rolling_targets.append(y_test.item())\n",
    "\n",
    "    rolling_predictions = np.array(rolling_predictions)\n",
    "    rolling_targets = np.array(rolling_targets).astype(int)\n",
    "\n",
    "    # Convert any 0-labeled targets to -1 if your original data is in [-1, +1]\n",
    "    # (Sometimes y might be {0,1} or {-1, +1}; adapt as needed.)\n",
    "    rolling_targets[rolling_targets == 0] = -1\n",
    "\n",
    "    # Filter out zero predictions\n",
    "    nonzero_mask = rolling_predictions != 0\n",
    "    filtered_preds = rolling_predictions[nonzero_mask]\n",
    "    filtered_targets = rolling_targets[nonzero_mask]\n",
    "\n",
    "    if len(filtered_preds) == 0:\n",
    "        accuracy_nonzero = None\n",
    "        print(\"No nonzero predictions, cannot compute thresholded accuracy.\")\n",
    "    else:\n",
    "        accuracy_nonzero = accuracy_score(filtered_targets, filtered_preds)\n",
    "        print(f\"Accuracy on Nonzero Predictions: {accuracy_nonzero:.4f}\")\n",
    "\n",
    "    return {\n",
    "        \"rolling_predictions\": rolling_predictions,\n",
    "        \"rolling_targets\": rolling_targets,\n",
    "        \"filtered_predictions\": filtered_preds,\n",
    "        \"filtered_targets\": filtered_targets,\n",
    "        \"accuracy_nonzero\": accuracy_nonzero,\n",
    "        \"loss_decrease_percentage\": loss_decrease_percentage,\n",
    "        \"final_train_loss\": epoch_train_losses[-1] \n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b794e699-83fb-4ca2-8c93-318858018a2e",
   "metadata": {},
   "source": [
    "### **4th Type of comparison:**\n",
    "\n",
    "Hyperparameters finetuning, for AAPL 10y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "836045d1-1ffb-4328-90d9-5ec333dfcc1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_data_dir = os.path.join(project_dir, f\"00_data/{processing}\") \n",
    "    \n",
    "# 1) Load original data (info only)\n",
    "filename = f\"{security_type}/{stock}/{period}_data.csv\"\n",
    "original_input_filepath = os.path.join(initial_data_dir, filename)\n",
    "original_data = pd.read_csv(original_input_filepath)\n",
    "\n",
    "# 2) Load the preprocessed data\n",
    "pkl_filename = f\"{processing}/{security_type}/{stock}/{period}_{window_size}_data.pkl\"\n",
    "input_filepath = os.path.join(horizontal_data_dir, pkl_filename)\n",
    "input_df = pd.read_pickle(input_filepath)\n",
    "\n",
    "# 3) Reshape\n",
    "X_resampled, y_resampled = reshape_remove_characters(input_df)\n",
    "\n",
    "input_size = X_resampled.shape[2]\n",
    "train_size = int(X_resampled.shape[0] * possible_train_size / 100)\n",
    "test_size = X_resampled.shape[0] - train_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "935b7698-c75e-496b-8850-8053044ce7f9",
   "metadata": {},
   "source": [
    "### **Iterate over hyperparameters using OPTUNA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8953eb19-f0c8-4c66-bdfa-9bd8a8a92b1a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evaluate_rolling_unchanged_model_threshold(\n",
    "    model, \n",
    "    X, \n",
    "    y, \n",
    "    criterion, \n",
    "    optimizer, \n",
    "    device, \n",
    "    train_size, \n",
    "    batch_size, \n",
    "    num_epochs, \n",
    "    lower_threshold\n",
    "):\n",
    "    \"\"\"\n",
    "    Evaluate a PyTorch model using a rolling prediction approach for time series,\n",
    "    training the model only once on the initial training set. For each time step\n",
    "    after train_size, the model makes a prediction without further parameter updates.\n",
    "    Only predicts +1 or -1 if the probability of class 1 is above/below given thresholds;\n",
    "    otherwise, predicts 0. Accuracy is computed only on nonzero predictions.\n",
    "\n",
    "    Args:\n",
    "        model:          PyTorch model to evaluate.\n",
    "        X:              Feature data (numpy array).\n",
    "        y:              Target data (numpy array).\n",
    "        criterion:      Loss function (e.g., CrossEntropyLoss).\n",
    "        optimizer:      Optimizer (e.g., Adam).\n",
    "        device:         Device for computation (CPU or GPU).\n",
    "        train_size:     Initial size of the training data (int or float).\n",
    "                        If < 1, treated as fraction of total length.\n",
    "        batch_size:     Batch size for training.\n",
    "        num_epochs:     Number of epochs for initial training only.\n",
    "        lower_threshold: Probability threshold below which model predicts -1.\n",
    "        upper_threshold: Probability threshold above which model predicts +1.\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary with the following keys:\n",
    "            - \"rolling_predictions\": All predictions (-1, 0, +1) across the test period.\n",
    "            - \"rolling_targets\": Corresponding true targets in [-1, +1].\n",
    "            - \"filtered_predictions\": Nonzero predictions only.\n",
    "            - \"filtered_targets\": Targets corresponding to nonzero predictions.\n",
    "            - \"accuracy_nonzero\": Accuracy computed only on nonzero predictions.\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert X, y to tensors\n",
    "    X = torch.tensor(X, dtype=torch.float32)\n",
    "    y = torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "    # Determine initial training set size\n",
    "    if train_size < 1.0:\n",
    "        lower_bound = int(train_size * len(X))\n",
    "    else:\n",
    "        lower_bound = train_size\n",
    "\n",
    "    # -------------------------\n",
    "    # 1) SINGLE TRAINING PHASE\n",
    "    # -------------------------\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "    \n",
    "    X_train = X[:lower_bound].to(device)\n",
    "    y_train = y[:lower_bound].to(device)\n",
    "\n",
    "    train_dataset = TensorDataset(X_train, y_train)\n",
    "    trainloader = DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size=batch_size, \n",
    "        shuffle=False,         # Keep False if order matters; True for better generalization\n",
    "        # num_workers=4,         # Adjust based on your CPU cores\n",
    "        # pin_memory=True,       # Speeds up transfer if using GPUs\n",
    "        drop_last=False        # Ensure the last batch is included\n",
    "    )\n",
    "\n",
    "    epoch_train_losses = []\n",
    "    for epoch in range(num_epochs):\n",
    "        # torch.cuda.empty_cache()\n",
    "        epoch_loss = 0.0\n",
    "        for X_batch, y_batch in trainloader:\n",
    "            X_batch = X_batch.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            pred_y = model(X_batch)   # [batch_size, num_classes]\n",
    "            loss = criterion(pred_y, y_batch)\n",
    "            loss.backward()\n",
    "\n",
    "            # Gradient clipping (optional)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "               \n",
    "        if (epoch + 1) % 5 == 0 or epoch == num_epochs - 1:\n",
    "            print(f\"[Train] Epoch {epoch+1}/{num_epochs}, Loss={epoch_loss/len(trainloader):.4f}\")\n",
    "\n",
    "        epoch_train_losses.append(epoch_loss/len(trainloader))\n",
    "        \n",
    "    loss_decrease_percentage = ((epoch_train_losses[-1] - epoch_train_losses[0]) / epoch_train_losses[0]) * 100\n",
    "    # ---------------------------------\n",
    "    # 2) ROLLING PREDICTIONS, NO UPDATE\n",
    "    # ---------------------------------\n",
    "    model.eval()\n",
    "\n",
    "    rolling_predictions = []\n",
    "    rolling_targets     = []\n",
    "\n",
    "    for i in range(lower_bound, len(X)):\n",
    "        # Single-step \"test\" sample\n",
    "        X_test = X[i:i+1].to(device)  # shape: (1, num_features)\n",
    "        y_test = y[i:i+1].to(device)  # shape: (1, )\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # Forward pass\n",
    "            pred_y = model(X_test)  # [1, num_classes]\n",
    "            probabilities = torch.softmax(pred_y, dim=1).cpu().numpy()  # shape: (1, 2)\n",
    "            prob_class_1  = probabilities[:, 1]  # shape: (1,)\n",
    "\n",
    "            # Threshold-based logic\n",
    "            # Initialize all predictions to 0\n",
    "            pred_classes = np.zeros_like(prob_class_1)\n",
    "            # Predict -1 if prob < lower_threshold\n",
    "            pred_classes[prob_class_1 < lower_threshold] = -1\n",
    "            # Predict +1 if prob > upper_threshold\n",
    "            pred_classes[prob_class_1 > 1-lower_threshold] = 1\n",
    "\n",
    "        rolling_predictions.append(pred_classes[0])  # scalar\n",
    "        rolling_targets.append(y_test.item())\n",
    "\n",
    "    rolling_predictions = np.array(rolling_predictions)\n",
    "    rolling_targets = np.array(rolling_targets).astype(int)\n",
    "\n",
    "    # Convert any 0-labeled targets to -1 if your original data is in [-1, +1]\n",
    "    # (Sometimes y might be {0,1} or {-1, +1}; adapt as needed.)\n",
    "    rolling_targets[rolling_targets == 0] = -1\n",
    "\n",
    "    # Filter out zero predictions\n",
    "    nonzero_mask = rolling_predictions != 0\n",
    "    filtered_preds = rolling_predictions[nonzero_mask]\n",
    "    filtered_targets = rolling_targets[nonzero_mask]\n",
    "\n",
    "    if len(filtered_preds) == 0:\n",
    "        accuracy_nonzero = None\n",
    "        print(\"No nonzero predictions, cannot compute thresholded accuracy.\")\n",
    "    else:\n",
    "        accuracy_nonzero = accuracy_score(filtered_targets, filtered_preds)\n",
    "        print(f\"Accuracy on Nonzero Predictions: {accuracy_nonzero:.4f}\")\n",
    "\n",
    "    return {\n",
    "        \"rolling_predictions\": rolling_predictions,\n",
    "        \"rolling_targets\": rolling_targets,\n",
    "        \"filtered_predictions\": filtered_preds,\n",
    "        \"filtered_targets\": filtered_targets,\n",
    "        \"accuracy_nonzero\": accuracy_nonzero,\n",
    "        \"loss_decrease_percentage\": loss_decrease_percentage\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "28c102c4-2e65-4e07-9d0d-d9f4ef3c55d0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import optuna\n",
    "from optuna.trial import Trial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c439a97e-d12f-4c6d-916e-57cab1c9ef41",
   "metadata": {},
   "source": [
    "#### **Define Optuna objective function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "79b2b1f4-5019-4bdb-aefd-29e0458affd1",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-09 23:27:01,350] A new study created in memory with name: no-name-66f60e61-2024-4489-ba23-910478195568\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 5/20, Loss=0.6943\n",
      "[Train] Epoch 10/20, Loss=0.6939\n",
      "[Train] Epoch 15/20, Loss=0.6932\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-09 23:27:04,536] Trial 0 finished with value: 0.5572519083969466 and parameters: {'model_type': 'lstm', 'learning_rate': 0.013061723975782447, 'batch_size': 32, 'num_epochs': 20}. Best is trial 0 with value: 0.5572519083969466.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 20/20, Loss=0.6930\n",
      "Accuracy on Nonzero Predictions: 0.5573\n",
      "[Train] Epoch 5/40, Loss=0.6944\n",
      "[Train] Epoch 10/40, Loss=0.6941\n",
      "[Train] Epoch 15/40, Loss=0.6934\n",
      "[Train] Epoch 20/40, Loss=0.6930\n",
      "[Train] Epoch 25/40, Loss=0.6926\n",
      "[Train] Epoch 30/40, Loss=0.6921\n",
      "[Train] Epoch 35/40, Loss=0.6914\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-09 23:27:10,780] Trial 1 finished with value: 0.5725190839694656 and parameters: {'model_type': 'lstm', 'learning_rate': 0.01330750081561962, 'batch_size': 32, 'num_epochs': 40}. Best is trial 1 with value: 0.5725190839694656.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 40/40, Loss=0.6909\n",
      "Accuracy on Nonzero Predictions: 0.5725\n",
      "[Train] Epoch 5/20, Loss=0.6946\n",
      "[Train] Epoch 10/20, Loss=0.6939\n",
      "[Train] Epoch 15/20, Loss=0.6938\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-09 23:27:15,483] Trial 2 finished with value: 0.4122137404580153 and parameters: {'model_type': 'lstm', 'learning_rate': 0.012102235876056055, 'batch_size': 16, 'num_epochs': 20}. Best is trial 1 with value: 0.5725190839694656.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 20/20, Loss=0.6937\n",
      "Accuracy on Nonzero Predictions: 0.4122\n",
      "[Train] Epoch 5/20, Loss=0.6946\n",
      "[Train] Epoch 10/20, Loss=0.6940\n",
      "[Train] Epoch 15/20, Loss=0.6939\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-09 23:27:20,299] Trial 3 finished with value: 0.4122137404580153 and parameters: {'model_type': 'gru', 'learning_rate': 0.017192310095623317, 'batch_size': 16, 'num_epochs': 20}. Best is trial 1 with value: 0.5725190839694656.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 20/20, Loss=0.6939\n",
      "Accuracy on Nonzero Predictions: 0.4122\n",
      "[Train] Epoch 5/70, Loss=0.6942\n",
      "[Train] Epoch 10/70, Loss=0.6938\n",
      "[Train] Epoch 15/70, Loss=0.6935\n",
      "[Train] Epoch 20/70, Loss=0.6932\n",
      "[Train] Epoch 25/70, Loss=0.6927\n",
      "[Train] Epoch 30/70, Loss=0.6924\n",
      "[Train] Epoch 35/70, Loss=0.6920\n",
      "[Train] Epoch 40/70, Loss=0.6916\n",
      "[Train] Epoch 45/70, Loss=0.6912\n",
      "[Train] Epoch 50/70, Loss=0.6908\n",
      "[Train] Epoch 55/70, Loss=0.6903\n",
      "[Train] Epoch 60/70, Loss=0.6893\n",
      "[Train] Epoch 65/70, Loss=0.6883\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-09 23:27:29,366] Trial 4 finished with value: 0.5877862595419847 and parameters: {'model_type': 'lstm', 'learning_rate': 0.010470105917277097, 'batch_size': 32, 'num_epochs': 70}. Best is trial 4 with value: 0.5877862595419847.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 70/70, Loss=0.6872\n",
      "Accuracy on Nonzero Predictions: 0.5878\n",
      "[Train] Epoch 5/40, Loss=0.6945\n",
      "[Train] Epoch 10/40, Loss=0.6940\n",
      "[Train] Epoch 15/40, Loss=0.6938\n",
      "[Train] Epoch 20/40, Loss=0.6938\n",
      "[Train] Epoch 25/40, Loss=0.6936\n",
      "[Train] Epoch 30/40, Loss=0.6935\n",
      "[Train] Epoch 35/40, Loss=0.6933\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-09 23:27:38,659] Trial 5 finished with value: 0.5190839694656488 and parameters: {'model_type': 'lstm', 'learning_rate': 0.014580355077930202, 'batch_size': 16, 'num_epochs': 40}. Best is trial 4 with value: 0.5877862595419847.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 40/40, Loss=0.6934\n",
      "Accuracy on Nonzero Predictions: 0.5191\n",
      "[Train] Epoch 5/40, Loss=0.6945\n",
      "[Train] Epoch 10/40, Loss=0.6940\n",
      "[Train] Epoch 15/40, Loss=0.6938\n",
      "[Train] Epoch 20/40, Loss=0.6936\n",
      "[Train] Epoch 25/40, Loss=0.6932\n",
      "[Train] Epoch 30/40, Loss=0.6930\n",
      "[Train] Epoch 35/40, Loss=0.6930\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-09 23:27:48,015] Trial 6 finished with value: 0.5419847328244275 and parameters: {'model_type': 'gru', 'learning_rate': 0.016010760706199414, 'batch_size': 16, 'num_epochs': 40}. Best is trial 4 with value: 0.5877862595419847.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 40/40, Loss=0.6924\n",
      "Accuracy on Nonzero Predictions: 0.5420\n",
      "[Train] Epoch 5/80, Loss=0.6938\n",
      "[Train] Epoch 10/80, Loss=0.6935\n",
      "[Train] Epoch 15/80, Loss=0.6936\n",
      "[Train] Epoch 20/80, Loss=0.6935\n",
      "[Train] Epoch 25/80, Loss=0.6931\n",
      "[Train] Epoch 30/80, Loss=0.6927\n",
      "[Train] Epoch 35/80, Loss=0.6922\n",
      "[Train] Epoch 40/80, Loss=0.6918\n",
      "[Train] Epoch 45/80, Loss=0.6913\n",
      "[Train] Epoch 50/80, Loss=0.6909\n",
      "[Train] Epoch 55/80, Loss=0.6906\n",
      "[Train] Epoch 60/80, Loss=0.6904\n",
      "[Train] Epoch 65/80, Loss=0.6901\n",
      "[Train] Epoch 70/80, Loss=0.6898\n",
      "[Train] Epoch 75/80, Loss=0.6896\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-09 23:27:53,029] Trial 7 finished with value: 0.549618320610687 and parameters: {'model_type': 'lstm', 'learning_rate': 0.007764281679780019, 'batch_size': 64, 'num_epochs': 80}. Best is trial 4 with value: 0.5877862595419847.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 80/80, Loss=0.6895\n",
      "Accuracy on Nonzero Predictions: 0.5496\n",
      "[Train] Epoch 5/80, Loss=0.6941\n",
      "[Train] Epoch 10/80, Loss=0.6939\n",
      "[Train] Epoch 15/80, Loss=0.6934\n",
      "[Train] Epoch 20/80, Loss=0.6931\n",
      "[Train] Epoch 25/80, Loss=0.6928\n",
      "[Train] Epoch 30/80, Loss=0.6925\n",
      "[Train] Epoch 35/80, Loss=0.6922\n",
      "[Train] Epoch 40/80, Loss=0.6919\n",
      "[Train] Epoch 45/80, Loss=0.6915\n",
      "[Train] Epoch 50/80, Loss=0.6911\n",
      "[Train] Epoch 55/80, Loss=0.6908\n",
      "[Train] Epoch 60/80, Loss=0.6904\n",
      "[Train] Epoch 65/80, Loss=0.6901\n",
      "[Train] Epoch 70/80, Loss=0.6897\n",
      "[Train] Epoch 75/80, Loss=0.6894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-09 23:28:09,181] Trial 8 finished with value: 0.5648854961832062 and parameters: {'model_type': 'lstm', 'learning_rate': 0.007087446586223474, 'batch_size': 16, 'num_epochs': 80}. Best is trial 4 with value: 0.5877862595419847.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 80/80, Loss=0.6891\n",
      "Accuracy on Nonzero Predictions: 0.5649\n",
      "[Train] Epoch 5/30, Loss=0.6944\n",
      "[Train] Epoch 10/30, Loss=0.6940\n",
      "[Train] Epoch 15/30, Loss=0.6939\n",
      "[Train] Epoch 20/30, Loss=0.6939\n",
      "[Train] Epoch 25/30, Loss=0.6938\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-09 23:28:15,237] Trial 9 finished with value: 0.46564885496183206 and parameters: {'model_type': 'gru', 'learning_rate': 0.01726417212997288, 'batch_size': 16, 'num_epochs': 30}. Best is trial 4 with value: 0.5877862595419847.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 30/30, Loss=0.6938\n",
      "Accuracy on Nonzero Predictions: 0.4656\n",
      "[Train] Epoch 5/100, Loss=0.6938\n",
      "[Train] Epoch 10/100, Loss=0.6934\n",
      "[Train] Epoch 15/100, Loss=0.6933\n",
      "[Train] Epoch 20/100, Loss=0.6931\n",
      "[Train] Epoch 25/100, Loss=0.6930\n",
      "[Train] Epoch 30/100, Loss=0.6929\n",
      "[Train] Epoch 35/100, Loss=0.6928\n",
      "[Train] Epoch 40/100, Loss=0.6926\n",
      "[Train] Epoch 45/100, Loss=0.6924\n",
      "[Train] Epoch 50/100, Loss=0.6923\n",
      "[Train] Epoch 55/100, Loss=0.6921\n",
      "[Train] Epoch 60/100, Loss=0.6920\n",
      "[Train] Epoch 65/100, Loss=0.6919\n",
      "[Train] Epoch 70/100, Loss=0.6917\n",
      "[Train] Epoch 75/100, Loss=0.6915\n",
      "[Train] Epoch 80/100, Loss=0.6914\n",
      "[Train] Epoch 85/100, Loss=0.6909\n",
      "[Train] Epoch 90/100, Loss=0.6903\n",
      "[Train] Epoch 95/100, Loss=0.6897\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-09 23:28:26,131] Trial 10 finished with value: 0.5801526717557252 and parameters: {'model_type': 'gru', 'learning_rate': 0.005166742643249309, 'batch_size': 32, 'num_epochs': 100}. Best is trial 4 with value: 0.5877862595419847.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 100/100, Loss=0.6890\n",
      "Accuracy on Nonzero Predictions: 0.5802\n",
      "[Train] Epoch 5/100, Loss=0.6939\n",
      "[Train] Epoch 10/100, Loss=0.6935\n",
      "[Train] Epoch 15/100, Loss=0.6933\n",
      "[Train] Epoch 20/100, Loss=0.6931\n",
      "[Train] Epoch 25/100, Loss=0.6930\n",
      "[Train] Epoch 30/100, Loss=0.6929\n",
      "[Train] Epoch 35/100, Loss=0.6928\n",
      "[Train] Epoch 40/100, Loss=0.6927\n",
      "[Train] Epoch 45/100, Loss=0.6924\n",
      "[Train] Epoch 50/100, Loss=0.6923\n",
      "[Train] Epoch 55/100, Loss=0.6918\n",
      "[Train] Epoch 60/100, Loss=0.6913\n",
      "[Train] Epoch 65/100, Loss=0.6907\n",
      "[Train] Epoch 70/100, Loss=0.6903\n",
      "[Train] Epoch 75/100, Loss=0.6900\n",
      "[Train] Epoch 80/100, Loss=0.6898\n",
      "[Train] Epoch 85/100, Loss=0.6895\n",
      "[Train] Epoch 90/100, Loss=0.6892\n",
      "[Train] Epoch 95/100, Loss=0.6890\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-09 23:28:37,150] Trial 11 finished with value: 0.5343511450381679 and parameters: {'model_type': 'gru', 'learning_rate': 0.005011462902976476, 'batch_size': 32, 'num_epochs': 100}. Best is trial 4 with value: 0.5877862595419847.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 100/100, Loss=0.6888\n",
      "Accuracy on Nonzero Predictions: 0.5344\n",
      "[Train] Epoch 5/100, Loss=0.6941\n",
      "[Train] Epoch 10/100, Loss=0.6937\n",
      "[Train] Epoch 15/100, Loss=0.6934\n",
      "[Train] Epoch 20/100, Loss=0.6931\n",
      "[Train] Epoch 25/100, Loss=0.6926\n",
      "[Train] Epoch 30/100, Loss=0.6922\n",
      "[Train] Epoch 35/100, Loss=0.6918\n",
      "[Train] Epoch 40/100, Loss=0.6914\n",
      "[Train] Epoch 45/100, Loss=0.6909\n",
      "[Train] Epoch 50/100, Loss=0.6904\n",
      "[Train] Epoch 55/100, Loss=0.6900\n",
      "[Train] Epoch 60/100, Loss=0.6894\n",
      "[Train] Epoch 65/100, Loss=0.6887\n",
      "[Train] Epoch 70/100, Loss=0.6875\n",
      "[Train] Epoch 75/100, Loss=0.6863\n",
      "[Train] Epoch 80/100, Loss=0.6853\n",
      "[Train] Epoch 85/100, Loss=0.6847\n",
      "[Train] Epoch 90/100, Loss=0.6840\n",
      "[Train] Epoch 95/100, Loss=0.6833\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-09 23:28:48,117] Trial 12 finished with value: 0.549618320610687 and parameters: {'model_type': 'gru', 'learning_rate': 0.00920884736356047, 'batch_size': 32, 'num_epochs': 100}. Best is trial 4 with value: 0.5877862595419847.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 100/100, Loss=0.6828\n",
      "Accuracy on Nonzero Predictions: 0.5496\n",
      "[Train] Epoch 5/70, Loss=0.6938\n",
      "[Train] Epoch 10/70, Loss=0.6935\n",
      "[Train] Epoch 15/70, Loss=0.6933\n",
      "[Train] Epoch 20/70, Loss=0.6932\n",
      "[Train] Epoch 25/70, Loss=0.6930\n",
      "[Train] Epoch 30/70, Loss=0.6928\n",
      "[Train] Epoch 35/70, Loss=0.6926\n",
      "[Train] Epoch 40/70, Loss=0.6923\n",
      "[Train] Epoch 45/70, Loss=0.6922\n",
      "[Train] Epoch 50/70, Loss=0.6920\n",
      "[Train] Epoch 55/70, Loss=0.6919\n",
      "[Train] Epoch 60/70, Loss=0.6918\n",
      "[Train] Epoch 65/70, Loss=0.6916\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-09 23:28:55,838] Trial 13 finished with value: 0.5419847328244275 and parameters: {'model_type': 'gru', 'learning_rate': 0.005486565063581714, 'batch_size': 32, 'num_epochs': 70}. Best is trial 4 with value: 0.5877862595419847.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 70/70, Loss=0.6915\n",
      "Accuracy on Nonzero Predictions: 0.5420\n",
      "[Train] Epoch 5/60, Loss=0.6938\n",
      "[Train] Epoch 10/60, Loss=0.6937\n",
      "[Train] Epoch 15/60, Loss=0.6938\n",
      "[Train] Epoch 20/60, Loss=0.6934\n",
      "[Train] Epoch 25/60, Loss=0.6927\n",
      "[Train] Epoch 30/60, Loss=0.6925\n",
      "[Train] Epoch 35/60, Loss=0.6924\n",
      "[Train] Epoch 40/60, Loss=0.6918\n",
      "[Train] Epoch 45/60, Loss=0.6910\n",
      "[Train] Epoch 50/60, Loss=0.6906\n",
      "[Train] Epoch 55/60, Loss=0.6901\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-09 23:28:59,624] Trial 14 finished with value: 0.5648854961832062 and parameters: {'model_type': 'lstm', 'learning_rate': 0.01023729604949784, 'batch_size': 64, 'num_epochs': 60}. Best is trial 4 with value: 0.5877862595419847.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 60/60, Loss=0.6897\n",
      "Accuracy on Nonzero Predictions: 0.5649\n",
      "[Train] Epoch 5/60, Loss=0.6938\n",
      "[Train] Epoch 10/60, Loss=0.6935\n",
      "[Train] Epoch 15/60, Loss=0.6933\n",
      "[Train] Epoch 20/60, Loss=0.6932\n",
      "[Train] Epoch 25/60, Loss=0.6930\n",
      "[Train] Epoch 30/60, Loss=0.6926\n",
      "[Train] Epoch 35/60, Loss=0.6923\n",
      "[Train] Epoch 40/60, Loss=0.6921\n",
      "[Train] Epoch 45/60, Loss=0.6918\n",
      "[Train] Epoch 50/60, Loss=0.6916\n",
      "[Train] Epoch 55/60, Loss=0.6913\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-09 23:29:06,298] Trial 15 finished with value: 0.5419847328244275 and parameters: {'model_type': 'gru', 'learning_rate': 0.006785014937558142, 'batch_size': 32, 'num_epochs': 60}. Best is trial 4 with value: 0.5877862595419847.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 60/60, Loss=0.6912\n",
      "Accuracy on Nonzero Predictions: 0.5420\n",
      "[Train] Epoch 5/90, Loss=0.6940\n",
      "[Train] Epoch 10/90, Loss=0.6938\n",
      "[Train] Epoch 15/90, Loss=0.6934\n",
      "[Train] Epoch 20/90, Loss=0.6929\n",
      "[Train] Epoch 25/90, Loss=0.6923\n",
      "[Train] Epoch 30/90, Loss=0.6918\n",
      "[Train] Epoch 35/90, Loss=0.6914\n",
      "[Train] Epoch 40/90, Loss=0.6912\n",
      "[Train] Epoch 45/90, Loss=0.6909\n",
      "[Train] Epoch 50/90, Loss=0.6907\n",
      "[Train] Epoch 55/90, Loss=0.6904\n",
      "[Train] Epoch 60/90, Loss=0.6901\n",
      "[Train] Epoch 65/90, Loss=0.6900\n",
      "[Train] Epoch 70/90, Loss=0.6899\n",
      "[Train] Epoch 75/90, Loss=0.6899\n",
      "[Train] Epoch 80/90, Loss=0.6896\n",
      "[Train] Epoch 85/90, Loss=0.6896\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-09 23:29:16,211] Trial 16 finished with value: 0.5648854961832062 and parameters: {'model_type': 'gru', 'learning_rate': 0.00979320893360582, 'batch_size': 32, 'num_epochs': 90}. Best is trial 4 with value: 0.5877862595419847.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 90/90, Loss=0.6893\n",
      "Accuracy on Nonzero Predictions: 0.5649\n",
      "[Train] Epoch 5/70, Loss=0.6941\n",
      "[Train] Epoch 10/70, Loss=0.6938\n",
      "[Train] Epoch 15/70, Loss=0.6934\n",
      "[Train] Epoch 20/70, Loss=0.6931\n",
      "[Train] Epoch 25/70, Loss=0.6927\n",
      "[Train] Epoch 30/70, Loss=0.6922\n",
      "[Train] Epoch 35/70, Loss=0.6919\n",
      "[Train] Epoch 40/70, Loss=0.6914\n",
      "[Train] Epoch 45/70, Loss=0.6910\n",
      "[Train] Epoch 50/70, Loss=0.6907\n",
      "[Train] Epoch 55/70, Loss=0.6905\n",
      "[Train] Epoch 60/70, Loss=0.6902\n",
      "[Train] Epoch 65/70, Loss=0.6899\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-09 23:29:23,899] Trial 17 finished with value: 0.6030534351145038 and parameters: {'model_type': 'lstm', 'learning_rate': 0.008633420813240979, 'batch_size': 32, 'num_epochs': 70}. Best is trial 17 with value: 0.6030534351145038.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 70/70, Loss=0.6889\n",
      "Accuracy on Nonzero Predictions: 0.6031\n",
      "[Train] Epoch 5/70, Loss=0.6937\n",
      "[Train] Epoch 10/70, Loss=0.6937\n",
      "[Train] Epoch 15/70, Loss=0.6937\n",
      "[Train] Epoch 20/70, Loss=0.6933\n",
      "[Train] Epoch 25/70, Loss=0.6932\n",
      "[Train] Epoch 30/70, Loss=0.6929\n",
      "[Train] Epoch 35/70, Loss=0.6928\n",
      "[Train] Epoch 40/70, Loss=0.6922\n",
      "[Train] Epoch 45/70, Loss=0.6916\n",
      "[Train] Epoch 50/70, Loss=0.6911\n",
      "[Train] Epoch 55/70, Loss=0.6908\n",
      "[Train] Epoch 60/70, Loss=0.6903\n",
      "[Train] Epoch 65/70, Loss=0.6897\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-09 23:29:28,305] Trial 18 finished with value: 0.549618320610687 and parameters: {'model_type': 'lstm', 'learning_rate': 0.008434460061834412, 'batch_size': 64, 'num_epochs': 70}. Best is trial 17 with value: 0.6030534351145038.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 70/70, Loss=0.6893\n",
      "Accuracy on Nonzero Predictions: 0.5496\n",
      "[Train] Epoch 5/50, Loss=0.6943\n",
      "[Train] Epoch 10/50, Loss=0.6939\n",
      "[Train] Epoch 15/50, Loss=0.6933\n",
      "[Train] Epoch 20/50, Loss=0.6930\n",
      "[Train] Epoch 25/50, Loss=0.6925\n",
      "[Train] Epoch 30/50, Loss=0.6921\n",
      "[Train] Epoch 35/50, Loss=0.6917\n",
      "[Train] Epoch 40/50, Loss=0.6914\n",
      "[Train] Epoch 45/50, Loss=0.6910\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-09 23:29:33,858] Trial 19 finished with value: 0.5877862595419847 and parameters: {'model_type': 'lstm', 'learning_rate': 0.011201888319495843, 'batch_size': 32, 'num_epochs': 50}. Best is trial 17 with value: 0.6030534351145038.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 50/50, Loss=0.6900\n",
      "Accuracy on Nonzero Predictions: 0.5878\n",
      "[Train] Epoch 5/70, Loss=0.6946\n",
      "[Train] Epoch 10/70, Loss=0.6942\n",
      "[Train] Epoch 15/70, Loss=0.6938\n",
      "[Train] Epoch 20/70, Loss=0.6937\n",
      "[Train] Epoch 25/70, Loss=0.6929\n",
      "[Train] Epoch 30/70, Loss=0.6919\n",
      "[Train] Epoch 35/70, Loss=0.6911\n",
      "[Train] Epoch 40/70, Loss=0.6901\n",
      "[Train] Epoch 45/70, Loss=0.6895\n",
      "[Train] Epoch 50/70, Loss=0.6885\n",
      "[Train] Epoch 55/70, Loss=0.6879\n",
      "[Train] Epoch 60/70, Loss=0.6873\n",
      "[Train] Epoch 65/70, Loss=0.6866\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-09 23:29:42,601] Trial 20 finished with value: 0.5419847328244275 and parameters: {'model_type': 'lstm', 'learning_rate': 0.0195705934093469, 'batch_size': 32, 'num_epochs': 70}. Best is trial 17 with value: 0.6030534351145038.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 70/70, Loss=0.6859\n",
      "Accuracy on Nonzero Predictions: 0.5420\n",
      "[Train] Epoch 5/50, Loss=0.6942\n",
      "[Train] Epoch 10/50, Loss=0.6939\n",
      "[Train] Epoch 15/50, Loss=0.6934\n",
      "[Train] Epoch 20/50, Loss=0.6931\n",
      "[Train] Epoch 25/50, Loss=0.6926\n",
      "[Train] Epoch 30/50, Loss=0.6921\n",
      "[Train] Epoch 35/50, Loss=0.6916\n",
      "[Train] Epoch 40/50, Loss=0.6912\n",
      "[Train] Epoch 45/50, Loss=0.6908\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-09 23:29:48,169] Trial 21 finished with value: 0.5877862595419847 and parameters: {'model_type': 'lstm', 'learning_rate': 0.011062681507814792, 'batch_size': 32, 'num_epochs': 50}. Best is trial 17 with value: 0.6030534351145038.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 50/50, Loss=0.6900\n",
      "Accuracy on Nonzero Predictions: 0.5878\n",
      "[Train] Epoch 5/50, Loss=0.6943\n",
      "[Train] Epoch 10/50, Loss=0.6941\n",
      "[Train] Epoch 15/50, Loss=0.6933\n",
      "[Train] Epoch 20/50, Loss=0.6930\n",
      "[Train] Epoch 25/50, Loss=0.6924\n",
      "[Train] Epoch 30/50, Loss=0.6919\n",
      "[Train] Epoch 35/50, Loss=0.6915\n",
      "[Train] Epoch 40/50, Loss=0.6909\n",
      "[Train] Epoch 45/50, Loss=0.6897\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-09 23:29:54,290] Trial 22 finished with value: 0.5801526717557252 and parameters: {'model_type': 'lstm', 'learning_rate': 0.011232626247408183, 'batch_size': 32, 'num_epochs': 50}. Best is trial 17 with value: 0.6030534351145038.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 50/50, Loss=0.6885\n",
      "Accuracy on Nonzero Predictions: 0.5802\n",
      "[Train] Epoch 5/80, Loss=0.6941\n",
      "[Train] Epoch 10/80, Loss=0.6937\n",
      "[Train] Epoch 15/80, Loss=0.6935\n",
      "[Train] Epoch 20/80, Loss=0.6933\n",
      "[Train] Epoch 25/80, Loss=0.6928\n",
      "[Train] Epoch 30/80, Loss=0.6923\n",
      "[Train] Epoch 35/80, Loss=0.6919\n",
      "[Train] Epoch 40/80, Loss=0.6915\n",
      "[Train] Epoch 45/80, Loss=0.6912\n",
      "[Train] Epoch 50/80, Loss=0.6906\n",
      "[Train] Epoch 55/80, Loss=0.6896\n",
      "[Train] Epoch 60/80, Loss=0.6888\n",
      "[Train] Epoch 65/80, Loss=0.6881\n",
      "[Train] Epoch 70/80, Loss=0.6873\n",
      "[Train] Epoch 75/80, Loss=0.6866\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-09 23:30:06,460] Trial 23 finished with value: 0.5801526717557252 and parameters: {'model_type': 'lstm', 'learning_rate': 0.008734877812008556, 'batch_size': 32, 'num_epochs': 80}. Best is trial 17 with value: 0.6030534351145038.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 80/80, Loss=0.6860\n",
      "Accuracy on Nonzero Predictions: 0.5802\n",
      "[Train] Epoch 5/50, Loss=0.6938\n",
      "[Train] Epoch 10/50, Loss=0.6936\n",
      "[Train] Epoch 15/50, Loss=0.6934\n",
      "[Train] Epoch 20/50, Loss=0.6933\n",
      "[Train] Epoch 25/50, Loss=0.6929\n",
      "[Train] Epoch 30/50, Loss=0.6927\n",
      "[Train] Epoch 35/50, Loss=0.6925\n",
      "[Train] Epoch 40/50, Loss=0.6924\n",
      "[Train] Epoch 45/50, Loss=0.6922\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-09 23:30:13,110] Trial 24 finished with value: 0.5343511450381679 and parameters: {'model_type': 'lstm', 'learning_rate': 0.006296098509923912, 'batch_size': 32, 'num_epochs': 50}. Best is trial 17 with value: 0.6030534351145038.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 50/50, Loss=0.6920\n",
      "Accuracy on Nonzero Predictions: 0.5344\n",
      "[Train] Epoch 5/60, Loss=0.6938\n",
      "[Train] Epoch 10/60, Loss=0.6936\n",
      "[Train] Epoch 15/60, Loss=0.6939\n",
      "[Train] Epoch 20/60, Loss=0.6934\n",
      "[Train] Epoch 25/60, Loss=0.6930\n",
      "[Train] Epoch 30/60, Loss=0.6928\n",
      "[Train] Epoch 35/60, Loss=0.6924\n",
      "[Train] Epoch 40/60, Loss=0.6921\n",
      "[Train] Epoch 45/60, Loss=0.6916\n",
      "[Train] Epoch 50/60, Loss=0.6911\n",
      "[Train] Epoch 55/60, Loss=0.6908\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-09 23:30:17,673] Trial 25 finished with value: 0.5343511450381679 and parameters: {'model_type': 'lstm', 'learning_rate': 0.007967557261131249, 'batch_size': 64, 'num_epochs': 60}. Best is trial 17 with value: 0.6030534351145038.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 60/60, Loss=0.6904\n",
      "Accuracy on Nonzero Predictions: 0.5344\n",
      "[Train] Epoch 5/70, Loss=0.6941\n",
      "[Train] Epoch 10/70, Loss=0.6938\n",
      "[Train] Epoch 15/70, Loss=0.6933\n",
      "[Train] Epoch 20/70, Loss=0.6930\n",
      "[Train] Epoch 25/70, Loss=0.6925\n",
      "[Train] Epoch 30/70, Loss=0.6920\n",
      "[Train] Epoch 35/70, Loss=0.6916\n",
      "[Train] Epoch 40/70, Loss=0.6911\n",
      "[Train] Epoch 45/70, Loss=0.6906\n",
      "[Train] Epoch 50/70, Loss=0.6899\n",
      "[Train] Epoch 55/70, Loss=0.6890\n",
      "[Train] Epoch 60/70, Loss=0.6882\n",
      "[Train] Epoch 65/70, Loss=0.6874\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-09 23:30:27,215] Trial 26 finished with value: 0.5954198473282443 and parameters: {'model_type': 'lstm', 'learning_rate': 0.01025211309951473, 'batch_size': 32, 'num_epochs': 70}. Best is trial 17 with value: 0.6030534351145038.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 70/70, Loss=0.6869\n",
      "Accuracy on Nonzero Predictions: 0.5954\n",
      "[Train] Epoch 5/90, Loss=0.6941\n",
      "[Train] Epoch 10/90, Loss=0.6939\n",
      "[Train] Epoch 15/90, Loss=0.6935\n",
      "[Train] Epoch 20/90, Loss=0.6932\n",
      "[Train] Epoch 25/90, Loss=0.6927\n",
      "[Train] Epoch 30/90, Loss=0.6922\n",
      "[Train] Epoch 35/90, Loss=0.6918\n",
      "[Train] Epoch 40/90, Loss=0.6914\n",
      "[Train] Epoch 45/90, Loss=0.6910\n",
      "[Train] Epoch 50/90, Loss=0.6904\n",
      "[Train] Epoch 55/90, Loss=0.6895\n",
      "[Train] Epoch 60/90, Loss=0.6884\n",
      "[Train] Epoch 65/90, Loss=0.6869\n",
      "[Train] Epoch 70/90, Loss=0.6858\n",
      "[Train] Epoch 75/90, Loss=0.6850\n",
      "[Train] Epoch 80/90, Loss=0.6844\n",
      "[Train] Epoch 85/90, Loss=0.6839\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-09 23:30:39,271] Trial 27 finished with value: 0.5725190839694656 and parameters: {'model_type': 'lstm', 'learning_rate': 0.009862417563560363, 'batch_size': 32, 'num_epochs': 90}. Best is trial 17 with value: 0.6030534351145038.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 90/90, Loss=0.6836\n",
      "Accuracy on Nonzero Predictions: 0.5725\n",
      "[Train] Epoch 5/70, Loss=0.6940\n",
      "[Train] Epoch 10/70, Loss=0.6938\n",
      "[Train] Epoch 15/70, Loss=0.6934\n",
      "[Train] Epoch 20/70, Loss=0.6931\n",
      "[Train] Epoch 25/70, Loss=0.6926\n",
      "[Train] Epoch 30/70, Loss=0.6921\n",
      "[Train] Epoch 35/70, Loss=0.6918\n",
      "[Train] Epoch 40/70, Loss=0.6915\n",
      "[Train] Epoch 45/70, Loss=0.6913\n",
      "[Train] Epoch 50/70, Loss=0.6911\n",
      "[Train] Epoch 55/70, Loss=0.6908\n",
      "[Train] Epoch 60/70, Loss=0.6906\n",
      "[Train] Epoch 65/70, Loss=0.6904\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-09 23:30:48,671] Trial 28 finished with value: 0.5801526717557252 and parameters: {'model_type': 'lstm', 'learning_rate': 0.007723317027654499, 'batch_size': 32, 'num_epochs': 70}. Best is trial 17 with value: 0.6030534351145038.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 70/70, Loss=0.6902\n",
      "Accuracy on Nonzero Predictions: 0.5802\n",
      "[Train] Epoch 5/90, Loss=0.6944\n",
      "[Train] Epoch 10/90, Loss=0.6938\n",
      "[Train] Epoch 15/90, Loss=0.6933\n",
      "[Train] Epoch 20/90, Loss=0.6929\n",
      "[Train] Epoch 25/90, Loss=0.6924\n",
      "[Train] Epoch 30/90, Loss=0.6919\n",
      "[Train] Epoch 35/90, Loss=0.6913\n",
      "[Train] Epoch 40/90, Loss=0.6908\n",
      "[Train] Epoch 45/90, Loss=0.6904\n",
      "[Train] Epoch 50/90, Loss=0.6900\n",
      "[Train] Epoch 55/90, Loss=0.6896\n",
      "[Train] Epoch 60/90, Loss=0.6886\n",
      "[Train] Epoch 65/90, Loss=0.6871\n",
      "[Train] Epoch 70/90, Loss=0.6855\n",
      "[Train] Epoch 75/90, Loss=0.6847\n",
      "[Train] Epoch 80/90, Loss=0.6837\n",
      "[Train] Epoch 85/90, Loss=0.6833\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-09 23:30:59,639] Trial 29 finished with value: 0.549618320610687 and parameters: {'model_type': 'lstm', 'learning_rate': 0.0132260578981384, 'batch_size': 32, 'num_epochs': 90}. Best is trial 17 with value: 0.6030534351145038.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 90/90, Loss=0.6827\n",
      "Accuracy on Nonzero Predictions: 0.5496\n",
      "[Train] Epoch 5/80, Loss=0.6942\n",
      "[Train] Epoch 10/80, Loss=0.6941\n",
      "[Train] Epoch 15/80, Loss=0.6937\n",
      "[Train] Epoch 20/80, Loss=0.6936\n",
      "[Train] Epoch 25/80, Loss=0.6932\n",
      "[Train] Epoch 30/80, Loss=0.6926\n",
      "[Train] Epoch 35/80, Loss=0.6922\n",
      "[Train] Epoch 40/80, Loss=0.6918\n",
      "[Train] Epoch 45/80, Loss=0.6915\n",
      "[Train] Epoch 50/80, Loss=0.6906\n",
      "[Train] Epoch 55/80, Loss=0.6900\n",
      "[Train] Epoch 60/80, Loss=0.6892\n",
      "[Train] Epoch 65/80, Loss=0.6885\n",
      "[Train] Epoch 70/80, Loss=0.6878\n",
      "[Train] Epoch 75/80, Loss=0.6870\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-09 23:31:06,915] Trial 30 finished with value: 0.549618320610687 and parameters: {'model_type': 'lstm', 'learning_rate': 0.012198444211351389, 'batch_size': 64, 'num_epochs': 80}. Best is trial 17 with value: 0.6030534351145038.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 80/80, Loss=0.6863\n",
      "Accuracy on Nonzero Predictions: 0.5496\n",
      "[Train] Epoch 5/60, Loss=0.6942\n",
      "[Train] Epoch 10/60, Loss=0.6940\n",
      "[Train] Epoch 15/60, Loss=0.6935\n",
      "[Train] Epoch 20/60, Loss=0.6932\n",
      "[Train] Epoch 25/60, Loss=0.6927\n",
      "[Train] Epoch 30/60, Loss=0.6921\n",
      "[Train] Epoch 35/60, Loss=0.6916\n",
      "[Train] Epoch 40/60, Loss=0.6910\n",
      "[Train] Epoch 45/60, Loss=0.6905\n",
      "[Train] Epoch 50/60, Loss=0.6898\n",
      "[Train] Epoch 55/60, Loss=0.6891\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-09 23:31:16,037] Trial 31 finished with value: 0.6030534351145038 and parameters: {'model_type': 'lstm', 'learning_rate': 0.010933888664969055, 'batch_size': 32, 'num_epochs': 60}. Best is trial 17 with value: 0.6030534351145038.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 60/60, Loss=0.6887\n",
      "Accuracy on Nonzero Predictions: 0.6031\n",
      "[Train] Epoch 5/70, Loss=0.6941\n",
      "[Train] Epoch 10/70, Loss=0.6939\n",
      "[Train] Epoch 15/70, Loss=0.6933\n",
      "[Train] Epoch 20/70, Loss=0.6931\n",
      "[Train] Epoch 25/70, Loss=0.6927\n",
      "[Train] Epoch 30/70, Loss=0.6922\n",
      "[Train] Epoch 35/70, Loss=0.6918\n",
      "[Train] Epoch 40/70, Loss=0.6915\n",
      "[Train] Epoch 45/70, Loss=0.6909\n",
      "[Train] Epoch 50/70, Loss=0.6903\n",
      "[Train] Epoch 55/70, Loss=0.6897\n",
      "[Train] Epoch 60/70, Loss=0.6886\n",
      "[Train] Epoch 65/70, Loss=0.6873\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-09 23:31:25,472] Trial 32 finished with value: 0.5648854961832062 and parameters: {'model_type': 'lstm', 'learning_rate': 0.009261875741480581, 'batch_size': 32, 'num_epochs': 70}. Best is trial 17 with value: 0.6030534351145038.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 70/70, Loss=0.6861\n",
      "Accuracy on Nonzero Predictions: 0.5649\n",
      "[Train] Epoch 5/60, Loss=0.6942\n",
      "[Train] Epoch 10/60, Loss=0.6940\n",
      "[Train] Epoch 15/60, Loss=0.6934\n",
      "[Train] Epoch 20/60, Loss=0.6931\n",
      "[Train] Epoch 25/60, Loss=0.6926\n",
      "[Train] Epoch 30/60, Loss=0.6919\n",
      "[Train] Epoch 35/60, Loss=0.6915\n",
      "[Train] Epoch 40/60, Loss=0.6911\n",
      "[Train] Epoch 45/60, Loss=0.6908\n",
      "[Train] Epoch 50/60, Loss=0.6904\n",
      "[Train] Epoch 55/60, Loss=0.6900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-09 23:31:33,024] Trial 33 finished with value: 0.5877862595419847 and parameters: {'model_type': 'lstm', 'learning_rate': 0.010524130385112344, 'batch_size': 32, 'num_epochs': 60}. Best is trial 17 with value: 0.6030534351145038.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 60/60, Loss=0.6894\n",
      "Accuracy on Nonzero Predictions: 0.5878\n",
      "[Train] Epoch 5/60, Loss=0.6943\n",
      "[Train] Epoch 10/60, Loss=0.6942\n",
      "[Train] Epoch 15/60, Loss=0.6939\n",
      "[Train] Epoch 20/60, Loss=0.6935\n",
      "[Train] Epoch 25/60, Loss=0.6933\n",
      "[Train] Epoch 30/60, Loss=0.6930\n",
      "[Train] Epoch 35/60, Loss=0.6925\n",
      "[Train] Epoch 40/60, Loss=0.6920\n",
      "[Train] Epoch 45/60, Loss=0.6915\n",
      "[Train] Epoch 50/60, Loss=0.6909\n",
      "[Train] Epoch 55/60, Loss=0.6902\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-09 23:31:40,022] Trial 34 finished with value: 0.5801526717557252 and parameters: {'model_type': 'lstm', 'learning_rate': 0.012465755383992859, 'batch_size': 32, 'num_epochs': 60}. Best is trial 17 with value: 0.6030534351145038.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 60/60, Loss=0.6896\n",
      "Accuracy on Nonzero Predictions: 0.5802\n",
      "[Train] Epoch 5/70, Loss=0.6943\n",
      "[Train] Epoch 10/70, Loss=0.6938\n",
      "[Train] Epoch 15/70, Loss=0.6931\n",
      "[Train] Epoch 20/70, Loss=0.6928\n",
      "[Train] Epoch 25/70, Loss=0.6923\n",
      "[Train] Epoch 30/70, Loss=0.6918\n",
      "[Train] Epoch 35/70, Loss=0.6913\n",
      "[Train] Epoch 40/70, Loss=0.6909\n",
      "[Train] Epoch 45/70, Loss=0.6905\n",
      "[Train] Epoch 50/70, Loss=0.6900\n",
      "[Train] Epoch 55/70, Loss=0.6889\n",
      "[Train] Epoch 60/70, Loss=0.6880\n",
      "[Train] Epoch 65/70, Loss=0.6874\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-09 23:31:49,212] Trial 35 finished with value: 0.5648854961832062 and parameters: {'model_type': 'lstm', 'learning_rate': 0.01400481533307976, 'batch_size': 32, 'num_epochs': 70}. Best is trial 17 with value: 0.6030534351145038.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 70/70, Loss=0.6868\n",
      "Accuracy on Nonzero Predictions: 0.5649\n",
      "[Train] Epoch 5/80, Loss=0.6940\n",
      "[Train] Epoch 10/80, Loss=0.6938\n",
      "[Train] Epoch 15/80, Loss=0.6934\n",
      "[Train] Epoch 20/80, Loss=0.6932\n",
      "[Train] Epoch 25/80, Loss=0.6927\n",
      "[Train] Epoch 30/80, Loss=0.6921\n",
      "[Train] Epoch 35/80, Loss=0.6917\n",
      "[Train] Epoch 40/80, Loss=0.6913\n",
      "[Train] Epoch 45/80, Loss=0.6910\n",
      "[Train] Epoch 50/80, Loss=0.6907\n",
      "[Train] Epoch 55/80, Loss=0.6901\n",
      "[Train] Epoch 60/80, Loss=0.6892\n",
      "[Train] Epoch 65/80, Loss=0.6881\n",
      "[Train] Epoch 70/80, Loss=0.6875\n",
      "[Train] Epoch 75/80, Loss=0.6871\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-09 23:31:58,051] Trial 36 finished with value: 0.5572519083969466 and parameters: {'model_type': 'lstm', 'learning_rate': 0.009019247677202684, 'batch_size': 32, 'num_epochs': 80}. Best is trial 17 with value: 0.6030534351145038.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 80/80, Loss=0.6868\n",
      "Accuracy on Nonzero Predictions: 0.5573\n",
      "[Train] Epoch 5/60, Loss=0.6947\n",
      "[Train] Epoch 10/60, Loss=0.6940\n",
      "[Train] Epoch 15/60, Loss=0.6933\n",
      "[Train] Epoch 20/60, Loss=0.6928\n",
      "[Train] Epoch 25/60, Loss=0.6924\n",
      "[Train] Epoch 30/60, Loss=0.6920\n",
      "[Train] Epoch 35/60, Loss=0.6917\n",
      "[Train] Epoch 40/60, Loss=0.6915\n",
      "[Train] Epoch 45/60, Loss=0.6914\n",
      "[Train] Epoch 50/60, Loss=0.6906\n",
      "[Train] Epoch 55/60, Loss=0.6900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-09 23:32:10,412] Trial 37 finished with value: 0.5114503816793893 and parameters: {'model_type': 'lstm', 'learning_rate': 0.011664031408254656, 'batch_size': 16, 'num_epochs': 60}. Best is trial 17 with value: 0.6030534351145038.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 60/60, Loss=0.6896\n",
      "Accuracy on Nonzero Predictions: 0.5115\n",
      "[Train] Epoch 5/70, Loss=0.6944\n",
      "[Train] Epoch 10/70, Loss=0.6941\n",
      "[Train] Epoch 15/70, Loss=0.6934\n",
      "[Train] Epoch 20/70, Loss=0.6928\n",
      "[Train] Epoch 25/70, Loss=0.6924\n",
      "[Train] Epoch 30/70, Loss=0.6920\n",
      "[Train] Epoch 35/70, Loss=0.6915\n",
      "[Train] Epoch 40/70, Loss=0.6903\n",
      "[Train] Epoch 45/70, Loss=0.6893\n",
      "[Train] Epoch 50/70, Loss=0.6886\n",
      "[Train] Epoch 55/70, Loss=0.6881\n",
      "[Train] Epoch 60/70, Loss=0.6877\n",
      "[Train] Epoch 65/70, Loss=0.6873\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-09 23:32:18,141] Trial 38 finished with value: 0.5267175572519084 and parameters: {'model_type': 'lstm', 'learning_rate': 0.014551493754186675, 'batch_size': 32, 'num_epochs': 70}. Best is trial 17 with value: 0.6030534351145038.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 70/70, Loss=0.6870\n",
      "Accuracy on Nonzero Predictions: 0.5267\n",
      "[Train] Epoch 5/80, Loss=0.6943\n",
      "[Train] Epoch 10/80, Loss=0.6939\n",
      "[Train] Epoch 15/80, Loss=0.6934\n",
      "[Train] Epoch 20/80, Loss=0.6929\n",
      "[Train] Epoch 25/80, Loss=0.6926\n",
      "[Train] Epoch 30/80, Loss=0.6923\n",
      "[Train] Epoch 35/80, Loss=0.6919\n",
      "[Train] Epoch 40/80, Loss=0.6916\n",
      "[Train] Epoch 45/80, Loss=0.6913\n",
      "[Train] Epoch 50/80, Loss=0.6911\n",
      "[Train] Epoch 55/80, Loss=0.6908\n",
      "[Train] Epoch 60/80, Loss=0.6905\n",
      "[Train] Epoch 65/80, Loss=0.6904\n",
      "[Train] Epoch 70/80, Loss=0.6903\n",
      "[Train] Epoch 75/80, Loss=0.6902\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-09 23:32:34,543] Trial 39 finished with value: 0.5648854961832062 and parameters: {'model_type': 'lstm', 'learning_rate': 0.008165946638656387, 'batch_size': 16, 'num_epochs': 80}. Best is trial 17 with value: 0.6030534351145038.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 80/80, Loss=0.6901\n",
      "Accuracy on Nonzero Predictions: 0.5649\n",
      "[Train] Epoch 5/40, Loss=0.6940\n",
      "[Train] Epoch 10/40, Loss=0.6936\n",
      "[Train] Epoch 15/40, Loss=0.6934\n",
      "[Train] Epoch 20/40, Loss=0.6933\n",
      "[Train] Epoch 25/40, Loss=0.6930\n",
      "[Train] Epoch 30/40, Loss=0.6927\n",
      "[Train] Epoch 35/40, Loss=0.6926\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-09 23:32:38,970] Trial 40 finished with value: 0.5343511450381679 and parameters: {'model_type': 'lstm', 'learning_rate': 0.007141784340635205, 'batch_size': 32, 'num_epochs': 40}. Best is trial 17 with value: 0.6030534351145038.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 40/40, Loss=0.6923\n",
      "Accuracy on Nonzero Predictions: 0.5344\n",
      "[Train] Epoch 5/50, Loss=0.6941\n",
      "[Train] Epoch 10/50, Loss=0.6939\n",
      "[Train] Epoch 15/50, Loss=0.6935\n",
      "[Train] Epoch 20/50, Loss=0.6932\n",
      "[Train] Epoch 25/50, Loss=0.6928\n",
      "[Train] Epoch 30/50, Loss=0.6923\n",
      "[Train] Epoch 35/50, Loss=0.6917\n",
      "[Train] Epoch 40/50, Loss=0.6912\n",
      "[Train] Epoch 45/50, Loss=0.6906\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-09 23:32:44,566] Trial 41 finished with value: 0.5954198473282443 and parameters: {'model_type': 'lstm', 'learning_rate': 0.010350662840067594, 'batch_size': 32, 'num_epochs': 50}. Best is trial 17 with value: 0.6030534351145038.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 50/50, Loss=0.6900\n",
      "Accuracy on Nonzero Predictions: 0.5954\n",
      "[Train] Epoch 5/50, Loss=0.6941\n",
      "[Train] Epoch 10/50, Loss=0.6937\n",
      "[Train] Epoch 15/50, Loss=0.6934\n",
      "[Train] Epoch 20/50, Loss=0.6931\n",
      "[Train] Epoch 25/50, Loss=0.6927\n",
      "[Train] Epoch 30/50, Loss=0.6922\n",
      "[Train] Epoch 35/50, Loss=0.6917\n",
      "[Train] Epoch 40/50, Loss=0.6912\n",
      "[Train] Epoch 45/50, Loss=0.6907\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-09 23:32:50,092] Trial 42 finished with value: 0.5877862595419847 and parameters: {'model_type': 'lstm', 'learning_rate': 0.009624113061220407, 'batch_size': 32, 'num_epochs': 50}. Best is trial 17 with value: 0.6030534351145038.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 50/50, Loss=0.6904\n",
      "Accuracy on Nonzero Predictions: 0.5878\n",
      "[Train] Epoch 5/40, Loss=0.6942\n",
      "[Train] Epoch 10/40, Loss=0.6938\n",
      "[Train] Epoch 15/40, Loss=0.6934\n",
      "[Train] Epoch 20/40, Loss=0.6929\n",
      "[Train] Epoch 25/40, Loss=0.6924\n",
      "[Train] Epoch 30/40, Loss=0.6920\n",
      "[Train] Epoch 35/40, Loss=0.6915\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-09 23:32:54,529] Trial 43 finished with value: 0.5648854961832062 and parameters: {'model_type': 'lstm', 'learning_rate': 0.010389149825998745, 'batch_size': 32, 'num_epochs': 40}. Best is trial 17 with value: 0.6030534351145038.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 40/40, Loss=0.6912\n",
      "Accuracy on Nonzero Predictions: 0.5649\n",
      "[Train] Epoch 5/30, Loss=0.6945\n",
      "[Train] Epoch 10/30, Loss=0.6941\n",
      "[Train] Epoch 15/30, Loss=0.6934\n",
      "[Train] Epoch 20/30, Loss=0.6930\n",
      "[Train] Epoch 25/30, Loss=0.6925\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-09 23:32:57,856] Trial 44 finished with value: 0.5801526717557252 and parameters: {'model_type': 'lstm', 'learning_rate': 0.012715133921068824, 'batch_size': 32, 'num_epochs': 30}. Best is trial 17 with value: 0.6030534351145038.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 30/30, Loss=0.6922\n",
      "Accuracy on Nonzero Predictions: 0.5802\n",
      "[Train] Epoch 5/50, Loss=0.6945\n",
      "[Train] Epoch 10/50, Loss=0.6941\n",
      "[Train] Epoch 15/50, Loss=0.6937\n",
      "[Train] Epoch 20/50, Loss=0.6935\n",
      "[Train] Epoch 25/50, Loss=0.6933\n",
      "[Train] Epoch 30/50, Loss=0.6930\n",
      "[Train] Epoch 35/50, Loss=0.6928\n",
      "[Train] Epoch 40/50, Loss=0.6925\n",
      "[Train] Epoch 45/50, Loss=0.6922\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-09 23:33:08,030] Trial 45 finished with value: 0.549618320610687 and parameters: {'model_type': 'lstm', 'learning_rate': 0.010695779227286846, 'batch_size': 16, 'num_epochs': 50}. Best is trial 17 with value: 0.6030534351145038.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 50/50, Loss=0.6917\n",
      "Accuracy on Nonzero Predictions: 0.5496\n",
      "[Train] Epoch 5/70, Loss=0.6941\n",
      "[Train] Epoch 10/70, Loss=0.6937\n",
      "[Train] Epoch 15/70, Loss=0.6932\n",
      "[Train] Epoch 20/70, Loss=0.6929\n",
      "[Train] Epoch 25/70, Loss=0.6924\n",
      "[Train] Epoch 30/70, Loss=0.6919\n",
      "[Train] Epoch 35/70, Loss=0.6914\n",
      "[Train] Epoch 40/70, Loss=0.6907\n",
      "[Train] Epoch 45/70, Loss=0.6898\n",
      "[Train] Epoch 50/70, Loss=0.6887\n",
      "[Train] Epoch 55/70, Loss=0.6872\n",
      "[Train] Epoch 60/70, Loss=0.6860\n",
      "[Train] Epoch 65/70, Loss=0.6852\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-09 23:33:15,814] Trial 46 finished with value: 0.5877862595419847 and parameters: {'model_type': 'lstm', 'learning_rate': 0.011669480316713226, 'batch_size': 32, 'num_epochs': 70}. Best is trial 17 with value: 0.6030534351145038.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 70/70, Loss=0.6847\n",
      "Accuracy on Nonzero Predictions: 0.5878\n",
      "[Train] Epoch 5/60, Loss=0.6936\n",
      "[Train] Epoch 10/60, Loss=0.6936\n",
      "[Train] Epoch 15/60, Loss=0.6939\n",
      "[Train] Epoch 20/60, Loss=0.6932\n",
      "[Train] Epoch 25/60, Loss=0.6930\n",
      "[Train] Epoch 30/60, Loss=0.6926\n",
      "[Train] Epoch 35/60, Loss=0.6919\n",
      "[Train] Epoch 40/60, Loss=0.6914\n",
      "[Train] Epoch 45/60, Loss=0.6909\n",
      "[Train] Epoch 50/60, Loss=0.6906\n",
      "[Train] Epoch 55/60, Loss=0.6903\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-09 23:33:19,641] Trial 47 finished with value: 0.5343511450381679 and parameters: {'model_type': 'lstm', 'learning_rate': 0.009529069700276468, 'batch_size': 64, 'num_epochs': 60}. Best is trial 17 with value: 0.6030534351145038.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 60/60, Loss=0.6902\n",
      "Accuracy on Nonzero Predictions: 0.5344\n",
      "[Train] Epoch 5/30, Loss=0.6944\n",
      "[Train] Epoch 10/30, Loss=0.6939\n",
      "[Train] Epoch 15/30, Loss=0.6932\n",
      "[Train] Epoch 20/30, Loss=0.6928\n",
      "[Train] Epoch 25/30, Loss=0.6924\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-09 23:33:22,979] Trial 48 finished with value: 0.5801526717557252 and parameters: {'model_type': 'gru', 'learning_rate': 0.015666123567270887, 'batch_size': 32, 'num_epochs': 30}. Best is trial 17 with value: 0.6030534351145038.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 30/30, Loss=0.6920\n",
      "Accuracy on Nonzero Predictions: 0.5802\n",
      "[Train] Epoch 5/60, Loss=0.6940\n",
      "[Train] Epoch 10/60, Loss=0.6937\n",
      "[Train] Epoch 15/60, Loss=0.6933\n",
      "[Train] Epoch 20/60, Loss=0.6931\n",
      "[Train] Epoch 25/60, Loss=0.6926\n",
      "[Train] Epoch 30/60, Loss=0.6922\n",
      "[Train] Epoch 35/60, Loss=0.6918\n",
      "[Train] Epoch 40/60, Loss=0.6914\n",
      "[Train] Epoch 45/60, Loss=0.6911\n",
      "[Train] Epoch 50/60, Loss=0.6908\n",
      "[Train] Epoch 55/60, Loss=0.6903\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-09 23:33:29,581] Trial 49 finished with value: 0.5801526717557252 and parameters: {'model_type': 'lstm', 'learning_rate': 0.008619589562057994, 'batch_size': 32, 'num_epochs': 60}. Best is trial 17 with value: 0.6030534351145038.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 60/60, Loss=0.6899\n",
      "Accuracy on Nonzero Predictions: 0.5802\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "\n",
    "def objective(trial, X, y, device=\"cpu\"):\n",
    "    \"\"\"\n",
    "    Returns accuracy (to be maximized), so we will call:\n",
    "        study = optuna.create_study(direction=\"maximize\")\n",
    "    \"\"\"\n",
    "    # -----------------------------------------------------------\n",
    "    # 1) Choose Model Type\n",
    "    #    Make sure we match the capitalization \"LSTM\" vs. \"GRU.\"\n",
    "    # -----------------------------------------------------------\n",
    "    model_type = trial.suggest_categorical(\"model_type\", [\"lstm\", \"gru\"])\n",
    "    \n",
    "    # -----------------------------------------------------------\n",
    "    # 2) Sample Hyperparameters around known best values\n",
    "    # -----------------------------------------------------------\n",
    "    # Best found was ~0.01053, so we widen around that a bit:\n",
    "    learning_rate = trial.suggest_float(\"learning_rate\", \n",
    "                                        0.005, 0.02, log=True)\n",
    "    \n",
    "    # Best was 64; we keep 16,32,64 in the search:\n",
    "    batch_size    = trial.suggest_categorical(\"batch_size\", [16, 32, 64])\n",
    "    \n",
    "    # Best was 80 epochs; we allow from 20 to 100:\n",
    "    num_epochs    = trial.suggest_int(\"num_epochs\", 20, 100, step=10)\n",
    "\n",
    "    # You can also tune hidden_size, num_layers, dropout, etc.\n",
    "    hidden_size   = 64\n",
    "    output_size   = 2\n",
    "    \n",
    "    # Some placeholders for whatever your model classes need:\n",
    "    input_size    = X.shape[2]  # e.g. the \"features\" dimension\n",
    "    num_layers    = 1\n",
    "    dropout       = 0.0\n",
    "\n",
    "    # -----------------------------------------------------------\n",
    "    # 3) Build Model\n",
    "    # -----------------------------------------------------------\n",
    "    if model_type == \"LSTM\":\n",
    "        model = StockPriceLSTM(input_size, hidden_size, output_size)\n",
    "    else:\n",
    "        model = GRU3DClassifier(input_size, hidden_size, output_size,\n",
    "                                num_layers, dropout)\n",
    "\n",
    "    model = model.to(device)\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # -----------------------------------------------------------\n",
    "    # 4) Train / Evaluate\n",
    "    # -----------------------------------------------------------\n",
    "    # Example of how you might slice training set:\n",
    "    train_size_percent = possible_train_size / 100\n",
    "    if isinstance(train_size_percent, float):\n",
    "        actual_train_size = int(train_size_percent * X.shape[0])\n",
    "    else:\n",
    "        actual_train_size = train_size_percent\n",
    "\n",
    "    result = evaluate_rolling_unchanged_model_threshold(\n",
    "        model=model,\n",
    "        X=X,\n",
    "        y=y,\n",
    "        criterion=criterion,\n",
    "        optimizer=optimizer,\n",
    "        device=device,\n",
    "        train_size=actual_train_size,\n",
    "        batch_size=batch_size,\n",
    "        num_epochs=num_epochs,\n",
    "        lower_threshold=0.5\n",
    "    )\n",
    "\n",
    "    # Extract the accuracy on nonzero predictions\n",
    "    accuracy = result[\"accuracy_nonzero\"]\n",
    "    \n",
    "    # If model never predicts nonzero => heavy penalty\n",
    "    if accuracy is None:\n",
    "        return 0.0  # or return float(\"-inf\")\n",
    "\n",
    "    # Now we just return accuracy so that Optuna will maximize it\n",
    "    return accuracy\n",
    "\n",
    "# Then create your study as follows:\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(lambda trial: objective(trial, X_resampled, y_resampled), n_trials=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "549c303b-01c1-46c1-a9c3-8107bf29d049",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best (1 - accuracy): 0.3740458015267175\n",
      "Best hyperparameters: {'model_type': 'LSTM', 'learning_rate': 0.010533170884024523, 'batch_size': 64, 'num_epochs': 80}\n"
     ]
    }
   ],
   "source": [
    "print(\"Best (1 - accuracy):\", study.best_value)\n",
    "print(\"Best hyperparameters:\", study.best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5075f582-67b2-4339-bf9b-0ff7d8a1f544",
   "metadata": {},
   "source": [
    "### **Choose Optuna hyperparameter** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3a60d375-1d6c-4840-9044-0d53a494d39c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 5/80, Loss=0.6940\n",
      "[Train] Epoch 10/80, Loss=0.6936\n",
      "[Train] Epoch 15/80, Loss=0.6937\n",
      "[Train] Epoch 20/80, Loss=0.6935\n",
      "[Train] Epoch 25/80, Loss=0.6932\n",
      "[Train] Epoch 30/80, Loss=0.6913\n",
      "[Train] Epoch 35/80, Loss=0.6917\n",
      "[Train] Epoch 40/80, Loss=0.6909\n",
      "[Train] Epoch 45/80, Loss=0.6899\n",
      "[Train] Epoch 50/80, Loss=0.6901\n",
      "[Train] Epoch 55/80, Loss=0.6893\n",
      "[Train] Epoch 60/80, Loss=0.6875\n",
      "[Train] Epoch 65/80, Loss=0.6865\n",
      "[Train] Epoch 70/80, Loss=0.6828\n",
      "[Train] Epoch 75/80, Loss=0.6784\n",
      "[Train] Epoch 80/80, Loss=0.6792\n",
      "Accuracy on Nonzero Predictions: 0.5802\n"
     ]
    }
   ],
   "source": [
    "batch_size= 32\n",
    "num_epochs= 80\n",
    "gru_model = GRU3DClassifier(input_size, hidden_size, output_size, num_layers, dropout)\n",
    "optimizer = optim.Adam(gru_model.parameters(), lr=0.010533170)\n",
    "train_size_percent = possible_train_size/100\n",
    "if isinstance(train_size_percent, float):\n",
    "    actual_train_size = int(train_size_percent * X_resampled.shape[0])\n",
    "else:\n",
    "    actual_train_size = train_size_percent\n",
    "    \n",
    "result = evaluate_rolling_unchanged_model_threshold(\n",
    "    model=gru_model,\n",
    "    X=X_resampled,\n",
    "    y=y_resampled,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    device=device,\n",
    "    train_size=actual_train_size,\n",
    "    batch_size=batch_size,\n",
    "    num_epochs=num_epochs,\n",
    "    lower_threshold=0.5,   # as requested\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c35b2b7-2157-46c5-9b6f-efe2874f27d0",
   "metadata": {},
   "source": [
    "### **Increase Optuna accuracy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b652af5-13e0-4c21-89cc-f289e3ce1951",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
